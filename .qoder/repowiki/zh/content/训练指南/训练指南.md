# è®­ç»ƒæŒ‡å—

<cite>
**æœ¬æ–‡æ¡£ä¸­å¼•ç”¨çš„æ–‡ä»¶**
- [trainer.py](file://src/transformers/trainer.py)
- [training_args.py](file://src/transformers/training_args.py)
- [trainer_callback.py](file://src/transformers/trainer_callback.py)
- [trainer_utils.py](file://src/transformers/trainer_utils.py)
- [data_collator.py](file://src/transformers/data/data_collator.py)
- [optimization.py](file://src/transformers/optimization.py)
- [run_classification.py](file://examples/pytorch/text-classification/run_classification.py)
- [run_clm.py](file://examples/pytorch/language-modeling/run_clm.py)
- [distributed_training.py](file://examples/training/distributed_training.py)
- [3D_parallel.py](file://examples/3D_parallel.py)
- [peft.py](file://src/transformers/integrations/peft.py)
- [quantization_config.py](file://src/transformers/utils/quantization_config.py)
</cite>

## ç›®å½•
1. [ç®€ä»‹](#ç®€ä»‹)
2. [Trainer API æ ¸å¿ƒæ¦‚å¿µ](#trainer-api-æ ¸å¿ƒæ¦‚å¿µ)
3. [è®­ç»ƒå‚æ•°é…ç½®](#è®­ç»ƒå‚æ•°é…ç½®)
4. [ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨](#ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨)
5. [å›è°ƒå‡½æ•°ç³»ç»Ÿ](#å›è°ƒå‡½æ•°ç³»ç»Ÿ)
6. [è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯](#è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯)
7. [åˆ†å¸ƒå¼è®­ç»ƒ](#åˆ†å¸ƒå¼è®­ç»ƒ)
8. [æ··åˆç²¾åº¦è®­ç»ƒ](#æ··åˆç²¾åº¦è®­ç»ƒ)
9. [æ•°æ®å¤„ç†ä¸é¢„å¤„ç†](#æ•°æ®å¤„ç†ä¸é¢„å¤„ç†)
10. [PEFT ä¸é€‚é…å™¨è®­ç»ƒ](#peft-ä¸é€‚é…å™¨è®­ç»ƒ)
11. [é‡åŒ–è®­ç»ƒ](#é‡åŒ–è®­ç»ƒ)
12. [å®Œæ•´è®­ç»ƒè„šæœ¬ç¤ºä¾‹](#å®Œæ•´è®­ç»ƒè„šæœ¬ç¤ºä¾‹)
13. [æ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µ](#æ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µ)
14. [å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)

## ç®€ä»‹

Transformers åº“æä¾›äº†å¼ºå¤§çš„ Trainer APIï¼Œç®€åŒ–äº†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚æœ¬æŒ‡å—å°†å…¨é¢ä»‹ç»å¦‚ä½•ä½¿ç”¨ Trainer API è¿›è¡Œå„ç§ç±»å‹çš„æ¨¡å‹è®­ç»ƒï¼Œä»åŸºç¡€é…ç½®åˆ°é«˜çº§ä¼˜åŒ–æŠ€æœ¯ã€‚

## Trainer API æ ¸å¿ƒæ¦‚å¿µ

### Trainer ç±»æ¶æ„

Trainer æ˜¯ ğŸ¤— Transformers çš„æ ¸å¿ƒè®­ç»ƒç±»ï¼Œæä¾›äº†å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯ã€‚

```mermaid
classDiagram
class Trainer {
+model : PreTrainedModel
+args : TrainingArguments
+data_collator : DataCollator
+train_dataset : Dataset
+eval_dataset : Dataset
+compute_metrics : Callable
+callbacks : List[TrainerCallback]
+optimizer : Optimizer
+lr_scheduler : LRScheduler
+train() TrainOutput
+evaluate() dict
+predict() PredictionOutput
+save_model() void
+create_optimizer_and_scheduler() void
}
class TrainingArguments {
+output_dir : str
+learning_rate : float
+per_device_train_batch_size : int
+per_device_eval_batch_size : int
+num_train_epochs : float
+max_steps : int
+warmup_steps : int
+weight_decay : float
+adam_beta1 : float
+adam_beta2 : float
+adam_epsilon : float
+max_grad_norm : float
+gradient_accumulation_steps : int
+logging_steps : int
+eval_steps : int
+save_steps : int
+save_strategy : SaveStrategy
+eval_strategy : IntervalStrategy
+fp16 : bool
+bf16 : bool
+deepspeed : str
+fsdp : list
+report_to : list
}
class TrainerCallback {
+on_init_end() TrainerControl
+on_train_begin() TrainerControl
+on_epoch_begin() TrainerControl
+on_step_begin() TrainerControl
+on_step_end() TrainerControl
+on_evaluate() TrainerControl
+on_save() TrainerControl
+on_log() TrainerControl
+on_train_end() TrainerControl
}
Trainer --> TrainingArguments : ä½¿ç”¨
Trainer --> TrainerCallback : åŒ…å«å¤šä¸ª
```

**å›¾è¡¨æ¥æº**
- [trainer.py](file://src/transformers/trainer.py#L200-L400)
- [training_args.py](file://src/transformers/training_args.py#L200-L400)
- [trainer_callback.py](file://src/transformers/trainer_callback.py#L200-L400)

### æ ¸å¿ƒç»„ä»¶è¯´æ˜

1. **æ¨¡å‹ç®¡ç†**: è‡ªåŠ¨å¤„ç†æ¨¡å‹è®¾å¤‡æ”¾ç½®å’Œå¹¶è¡ŒåŒ–
2. **æ•°æ®åŠ è½½**: æ”¯æŒå¤šç§æ•°æ®æ ¼å¼å’ŒåŠ¨æ€å¡«å……
3. **è®­ç»ƒå¾ªç¯**: å®Œæ•´çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œä¼˜åŒ–å™¨æ›´æ–°
4. **è¯„ä¼°æœºåˆ¶**: å®šæœŸè¯„ä¼°å’ŒæŒ‡æ ‡è®°å½•
5. **æ£€æŸ¥ç‚¹ä¿å­˜**: æ™ºèƒ½çš„æ¨¡å‹ä¿å­˜ç­–ç•¥
6. **æ—¥å¿—è®°å½•**: å¤šç§æ—¥å¿—è¾“å‡ºæ ¼å¼æ”¯æŒ

**ç« èŠ‚æ¥æº**
- [trainer.py](file://src/transformers/trainer.py#L200-L600)

## è®­ç»ƒå‚æ•°é…ç½®

### åŸºç¡€è®­ç»ƒå‚æ•°

TrainingArguments æä¾›äº†ä¸°å¯Œçš„é…ç½®é€‰é¡¹ï¼š

```mermaid
flowchart TD
A[TrainingArguments] --> B[åŸºæœ¬å‚æ•°]
A --> C[ä¼˜åŒ–å™¨å‚æ•°]
A --> D[è°ƒåº¦å™¨å‚æ•°]
A --> E[æ•°æ®åŠ è½½å‚æ•°]
A --> F[ä¿å­˜ä¸è¯„ä¼°å‚æ•°]
A --> G[åˆ†å¸ƒå¼å‚æ•°]
A --> H[æ··åˆç²¾åº¦å‚æ•°]
B --> B1[output_dir]
B --> B2[do_train/do_eval]
B --> B3[num_train_epochs/max_steps]
C --> C1[learning_rate]
C --> C2[weight_decay]
C --> C3[adam_beta1/beta2]
D --> D1[lr_scheduler_type]
D --> D2[warmup_steps]
D --> D3[gradient_accumulation_steps]
E --> E1[per_device_train_batch_size]
E --> E2[dataloader_num_workers]
E --> E3[dataloader_pin_memory]
F --> F1[save_strategy]
F --> F2[eval_strategy]
F --> F3[logging_steps]
G --> G1[deepspeed]
G --> G2[fsdp]
G --> G3[n_gpu]
H --> H1[fp16/bf16]
H --> H2[tf32]
H --> H3[torch_compile]
```

**å›¾è¡¨æ¥æº**
- [training_args.py](file://src/transformers/training_args.py#L200-L800)

### å…³é”®é…ç½®å‚æ•°è¯¦è§£

| å‚æ•°ç±»åˆ« | ä¸»è¦å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|---------|---------|--------|------|
| åŸºæœ¬è®­ç»ƒ | `learning_rate` | 5e-5 | åˆå§‹å­¦ä¹ ç‡ |
| åŸºæœ¬è®­ç»ƒ | `num_train_epochs` | 3.0 | è®­ç»ƒè½®æ•° |
| æ‰¹å¤„ç† | `per_device_train_batch_size` | 8 | æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å° |
| æ‰¹å¤„ç† | `gradient_accumulation_steps` | 1 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| å­¦ä¹ ç‡è°ƒåº¦ | `warmup_steps` | 0 | é¢„çƒ­æ­¥æ•° |
| å­¦ä¹ ç‡è°ƒåº¦ | `lr_scheduler_type` | "linear" | è°ƒåº¦å™¨ç±»å‹ |
| æƒé‡è¡°å‡ | `weight_decay` | 0.0 | æƒé‡è¡°å‡ç³»æ•° |
| æ¢¯åº¦è£å‰ª | `max_grad_norm` | 1.0 | æ¢¯åº¦æœ€å¤§èŒƒæ•° |

**ç« èŠ‚æ¥æº**
- [training_args.py](file://src/transformers/training_args.py#L200-L800)

## ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨

### æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹

Transformers æ”¯æŒå¤šç§ä¼˜åŒ–å™¨ï¼š

```mermaid
graph TD
A[ä¼˜åŒ–å™¨é€‰æ‹©] --> B[Adam ç³»åˆ—]
A --> C[SGD ç³»åˆ—]
A --> D[ä¸“ç”¨ä¼˜åŒ–å™¨]
B --> B1[AdamW]
B --> B2[AdamW_Torch]
B --> B3[AdamW_Torch_Fused]
B --> B4[Adafactor]
C --> C1[SGD]
C --> C2[AdamW_8Bit]
C --> C3[Lion]
D --> D1[GaLore]
D --> D2[APOLLO]
D --> D3[ScheduleFree]
```

**å›¾è¡¨æ¥æº**
- [training_args.py](file://src/transformers/training_args.py#L150-L250)
- [optimization.py](file://src/transformers/optimization.py#L50-L150)

### å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹

| è°ƒåº¦å™¨ç±»å‹ | æè¿° | é€‚ç”¨åœºæ™¯ |
|-----------|------|----------|
| `linear` | çº¿æ€§é¢„çƒ­åçº¿æ€§è¡°å‡ | æ ‡å‡†è®­ç»ƒä»»åŠ¡ |
| `cosine` | ä½™å¼¦é¢„çƒ­åä½™å¼¦è¡°å‡ | éœ€è¦å¹³æ»‘å­¦ä¹ ç‡å˜åŒ– |
| `cosine_with_restarts` | ä½™å¼¦é¢„çƒ­åé‡å¯è¡°å‡ | é¿å…å±€éƒ¨æœ€ä¼˜ |
| `polynomial` | å¤šé¡¹å¼é¢„çƒ­åå¤šé¡¹å¼è¡°å‡ | ç‰¹æ®Šéœ€æ±‚ |
| `constant` | æ’å®šå­¦ä¹ ç‡ | ç¨³å®šè®­ç»ƒ |
| `constant_with_warmup` | é¢„çƒ­åæ’å®šå­¦ä¹ ç‡ | æ¸©å’Œå¼€å§‹è®­ç»ƒ |
| `reduce_lr_on_plateau` | åŸºäºéªŒè¯æŒ‡æ ‡è¡°å‡ | éªŒè¯é›†ç›‘æ§ |

**ç« èŠ‚æ¥æº**
- [optimization.py](file://src/transformers/optimization.py#L578-L601)

## å›è°ƒå‡½æ•°ç³»ç»Ÿ

### å›è°ƒå‡½æ•°æ¶æ„

```mermaid
classDiagram
class TrainerCallback {
<<abstract>>
+on_init_end(args, state, control) TrainerControl
+on_train_begin(args, state, control) TrainerControl
+on_epoch_begin(args, state, control) TrainerControl
+on_step_begin(args, state, control) TrainerControl
+on_step_end(args, state, control) TrainerControl
+on_evaluate(args, state, control, metrics) TrainerControl
+on_save(args, state, control) TrainerControl
+on_log(args, state, control, logs) TrainerControl
+on_train_end(args, state, control) TrainerControl
}
class DefaultFlowCallback {
+on_step_end(args, state, control) TrainerControl
+on_epoch_end(args, state, control) TrainerControl
}
class ProgressCallback {
+training_bar : tqdm
+prediction_bar : tqdm
+on_train_begin(args, state, control) TrainerControl
+on_step_end(args, state, control) TrainerControl
+on_log(args, state, control, logs) TrainerControl
}
class EarlyStoppingCallback {
+early_stopping_patience : int
+early_stopping_threshold : float
+early_stopping_patience_counter : int
+on_evaluate(args, state, control, metrics) TrainerControl
}
class PrinterCallback {
+on_log(args, state, control, logs) TrainerControl
}
TrainerCallback <|-- DefaultFlowCallback
TrainerCallback <|-- ProgressCallback
TrainerCallback <|-- EarlyStoppingCallback
TrainerCallback <|-- PrinterCallback
```

**å›¾è¡¨æ¥æº**
- [trainer_callback.py](file://src/transformers/trainer_callback.py#L200-L600)

### å†…ç½®å›è°ƒå‡½æ•°

1. **DefaultFlowCallback**: å¤„ç†é»˜è®¤çš„è®­ç»ƒæµç¨‹æ§åˆ¶
2. **ProgressCallback**: æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
3. **EarlyStoppingCallback**: å®ç°æ—©åœæœºåˆ¶
4. **PrinterCallback**: ç®€å•çš„æ—¥å¿—æ‰“å°

**ç« èŠ‚æ¥æº**
- [trainer_callback.py](file://src/transformers/trainer_callback.py#L200-L768)

## è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

### åˆ›å»ºè‡ªå®šä¹‰ Trainer ç±»

å¯¹äºéœ€è¦æ›´ç²¾ç»†æ§åˆ¶çš„åœºæ™¯ï¼Œå¯ä»¥ç»§æ‰¿ Trainer ç±»ï¼š

```mermaid
sequenceDiagram
participant User as ç”¨æˆ·ä»£ç 
participant CustomTrainer as è‡ªå®šä¹‰Trainer
participant Model as æ¨¡å‹
participant Optimizer as ä¼˜åŒ–å™¨
participant Scheduler as è°ƒåº¦å™¨
User->>CustomTrainer : åˆå§‹åŒ–
CustomTrainer->>Model : åŠ è½½æ¨¡å‹
CustomTrainer->>Optimizer : åˆ›å»ºä¼˜åŒ–å™¨
CustomTrainer->>Scheduler : åˆ›å»ºè°ƒåº¦å™¨
loop è®­ç»ƒå¾ªç¯
User->>CustomTrainer : train()
CustomTrainer->>Model : forward()
Model-->>CustomTrainer : loss
CustomTrainer->>Model : backward()
CustomTrainer->>Optimizer : step()
CustomTrainer->>Scheduler : step()
CustomTrainer->>CustomTrainer : è‡ªå®šä¹‰é€»è¾‘
end
```

**å›¾è¡¨æ¥æº**
- [trainer.py](file://src/transformers/trainer.py#L2500-L2600)

### è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤

å¯ä»¥é€šè¿‡é‡å†™ä»¥ä¸‹æ–¹æ³•å®ç°è‡ªå®šä¹‰è®­ç»ƒé€»è¾‘ï¼š

1. **`create_optimizer_and_scheduler`**: è‡ªå®šä¹‰ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»º
2. **`training_step`**: è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤
3. **`prediction_step`**: è‡ªå®šä¹‰é¢„æµ‹æ­¥éª¤
4. **`compute_loss`**: è‡ªå®šä¹‰æŸå¤±è®¡ç®—

**ç« èŠ‚æ¥æº**
- [trainer.py](file://src/transformers/trainer.py#L2500-L2700)

## åˆ†å¸ƒå¼è®­ç»ƒ

### æ”¯æŒçš„åˆ†å¸ƒå¼ç­–ç•¥

```mermaid
graph TD
A[åˆ†å¸ƒå¼è®­ç»ƒ] --> B[æ•°æ®å¹¶è¡Œ]
A --> C[æ¨¡å‹å¹¶è¡Œ]
A --> D[æµæ°´çº¿å¹¶è¡Œ]
B --> B1[DistributedDataParallel]
B --> B2[FSDP]
B --> B3[DeepSpeed ZeRO]
C --> C1[Tensor Parallelism]
C --> C2[Pipeline Parallelism]
C --> C3[3D Parallelism]
D --> D1[åºåˆ—å¹¶è¡Œ]
D --> D2[ä¸“å®¶å¹¶è¡Œ]
D --> D3[æ¿€æ´»æ£€æŸ¥ç‚¹]
```

**å›¾è¡¨æ¥æº**
- [distributed_training.py](file://examples/training/distributed_training.py#L1-L50)
- [3D_parallel.py](file://examples/3D_parallel.py#L1-L50)

### FSDP é…ç½®

FSDP (Fully Sharded Data Parallel) é…ç½®é€‰é¡¹ï¼š

| é…ç½®é¡¹ | å¯é€‰å€¼ | è¯´æ˜ |
|-------|--------|------|
| `fsdp` | `full_shard`, `shard_grad_op`, `hybrid_shard` | åˆ†ç‰‡ç­–ç•¥ |
| `fsdp_config` | `{"activation_checkpointing": true}` | æ¿€æ´»æ£€æŸ¥ç‚¹ |
| `cpu_ram_efficient_loading` | `true/false` | CPUå†…å­˜ä¼˜åŒ–åŠ è½½ |
| `backward_prefetch` | `backward_pre`, `backward_post` | åå‘é¢„å–ç­–ç•¥ |

### DeepSpeed é…ç½®

DeepSpeed ZeRO é˜¶æ®µé…ç½®ï¼š

```mermaid
flowchart LR
A[ZeRO é˜¶æ®µ] --> B[ZeRO-1]
A --> C[ZeRO-2]
A --> D[ZeRO-3]
B --> B1[ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡]
C --> C1[æ¢¯åº¦åˆ†ç‰‡]
D --> D1[å‚æ•°åˆ†ç‰‡]
B --> B2[å†…å­˜èŠ‚çœ: ~2x]
C --> C2[å†…å­˜èŠ‚çœ: ~4x]
D --> D3[å†…å­˜èŠ‚çœ: ~8x]
```

**å›¾è¡¨æ¥æº**
- [training_args.py](file://src/transformers/training_args.py#L600-L700)

**ç« èŠ‚æ¥æº**
- [distributed_training.py](file://examples/training/distributed_training.py#L1-L114)
- [3D_parallel.py](file://examples/3D_parallel.py#L1-L23)

## æ··åˆç²¾åº¦è®­ç»ƒ

### æ”¯æŒçš„ç²¾åº¦ç±»å‹

```mermaid
graph TD
A[æ··åˆç²¾åº¦è®­ç»ƒ] --> B[FP16]
A --> C[BF16]
A --> D[TF32]
B --> B1[åŠç²¾åº¦æµ®ç‚¹]
B --> B2[è‡ªåŠ¨ç¼©æ”¾]
B --> B3[æ¢¯åº¦ç¼©æ”¾]
C --> C1[è„‘æµ®ç‚¹]
C --> C2[æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§]
C --> C3[ç°ä»£GPUæ”¯æŒ]
D --> D1[å¼ é‡æ ¸å¿ƒ]
D --> D2[NVIDIA Ampereæ¶æ„]
D --> D3[é«˜æ€§èƒ½è®¡ç®—]
```

### ç²¾åº¦é…ç½®é€‰é¡¹

| å‚æ•° | ç±»å‹ | è¯´æ˜ | æ€§èƒ½å½±å“ |
|------|------|------|----------|
| `fp16` | `bool` | å¯ç”¨FP16è®­ç»ƒ | å‡å°‘50%å†…å­˜ï¼Œå¯èƒ½å½±å“ç²¾åº¦ |
| `bf16` | `bool` | å¯ç”¨BF16è®­ç»ƒ | å‡å°‘50%å†…å­˜ï¼Œæ›´å¥½ç¨³å®šæ€§ |
| `tf32` | `bool` | å¯ç”¨TF32 | æå‡è®¡ç®—é€Ÿåº¦ |
| `torch_compile` | `bool` | å¯ç”¨ç¼–è¯‘ä¼˜åŒ– | æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ |

**ç« èŠ‚æ¥æº**
- [training_args.py](file://src/transformers/training_args.py#L400-L500)

## æ•°æ®å¤„ç†ä¸é¢„å¤„ç†

### æ•°æ®æ•´ç†å™¨ (Data Collator)

```mermaid
classDiagram
class DataCollator {
<<interface>>
+__call__(features) dict
}
class DefaultDataCollator {
+return_tensors : str
+__call__(features) dict
}
class DataCollatorWithPadding {
+tokenizer : PreTrainedTokenizer
+padding : Union[bool, str]
+max_length : Optional[int]
+pad_to_multiple_of : Optional[int]
+__call__(features) dict
}
class DataCollatorForLanguageModeling {
+tokenizer : PreTrainedTokenizer
+mlm : bool
+mlm_probability : float
+__call__(features) dict
}
DataCollator <|-- DefaultDataCollator
DataCollator <|-- DataCollatorWithPadding
DataCollator <|-- DataCollatorForLanguageModeling
```

**å›¾è¡¨æ¥æº**
- [data_collator.py](file://src/transformers/data/data_collator.py#L50-L200)

### æ•°æ®å¤„ç†æµç¨‹

```mermaid
flowchart TD
A[åŸå§‹æ•°æ®] --> B[æ•°æ®é¢„å¤„ç†]
B --> C[Tokenization]
C --> D[é•¿åº¦å¯¹é½]
D --> E[æ‰¹é‡æ‰“åŒ…]
E --> F[æ•°æ®æ•´ç†å™¨]
F --> G[æ¨¡å‹è¾“å…¥]
B --> B1[æ–‡æœ¬æ¸…ç†]
B --> B2[ç‰¹æ®Šæ ‡è®°å¤„ç†]
C --> C1[åˆ†è¯]
C --> C2[ç‰¹æ®Šæ ‡è®°æ·»åŠ ]
D --> D1[å¡«å……]
D --> D2[æˆªæ–­]
E --> E1[åŠ¨æ€æ‰¹å¤„ç†]
E --> E2[æ¢¯åº¦ç´¯ç§¯å‹å¥½]
```

**å›¾è¡¨æ¥æº**
- [data_collator.py](file://src/transformers/data/data_collator.py#L100-L400)

**ç« èŠ‚æ¥æº**
- [data_collator.py](file://src/transformers/data/data_collator.py#L1-L800)

## PEFT ä¸é€‚é…å™¨è®­ç»ƒ

### LoRA (Low-Rank Adaptation)

```mermaid
graph TD
A[åŸºç¡€æ¨¡å‹] --> B[æ·»åŠ LoRAå±‚]
B --> C[æƒé‡åˆ†è§£]
C --> D[ä½ç§©çŸ©é˜µ]
D --> E[å¾®è°ƒå‚æ•°]
B --> B1[å¯è®­ç»ƒçš„LoRAçŸ©é˜µ]
B --> B2[å†»ç»“åŸå§‹æƒé‡]
C --> C1[A = B Ã— C]
C --> C2[rank << original_dim]
E --> E1[ä»…æ›´æ–°LoRAå‚æ•°]
E --> E2[å¤§å¹…å‡å°‘å†…å­˜ä½¿ç”¨]
```

**å›¾è¡¨æ¥æº**
- [peft.py](file://src/transformers/integrations/peft.py#L300-L400)

### PEFT é…ç½®ç¤ºä¾‹

| é…ç½®å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|---------|------|--------|
| `r` | LoRAç§© | 8-64 |
| `lora_alpha` | LoRAç¼©æ”¾å› å­ | rçš„å€æ•° |
| `target_modules` | ç›®æ ‡æ¨¡å—åˆ—è¡¨ | ["q_proj", "v_proj"] |
| `lora_dropout` | Dropoutæ¦‚ç‡ | 0.1 |
| `bias` | æ˜¯å¦è®­ç»ƒåç½® | "none" |

**ç« èŠ‚æ¥æº**
- [peft.py](file://src/transformers/integrations/peft.py#L100-L400)

## é‡åŒ–è®­ç»ƒ

### é‡åŒ–æ–¹æ³•å¯¹æ¯”

```mermaid
graph TD
A[é‡åŒ–æ–¹æ³•] --> B[Post-Training Quantization]
A --> C[Quantization-Aware Training]
B --> B1[é™æ€é‡åŒ–]
B --> B2[åŠ¨æ€é‡åŒ–]
C --> C1[è®­ç»ƒæ—¶é‡åŒ–]
C --> C2[æ¸è¿›å¼é‡åŒ–]
B1 --> B11[æ— éœ€é‡æ–°è®­ç»ƒ]
B1 --> B12[å¿«é€Ÿéƒ¨ç½²]
B2 --> B21[è¿è¡Œæ—¶é‡åŒ–]
B2 --> B22[çµæ´»ç²¾åº¦]
C1 --> C11[ä¿æŒç²¾åº¦]
C1 --> C12[æ›´å¥½çš„æ•ˆæœ]
C2 --> C21[é€æ­¥é™ä½ç²¾åº¦]
C2 --> C22[ç¨³å®šæ”¶æ•›]
```

**å›¾è¡¨æ¥æº**
- [quantization_config.py](file://src/transformers/utils/quantization_config.py#L1500-L1600)

### é‡åŒ–é…ç½®é€‰é¡¹

| é‡åŒ–ç±»å‹ | æ–¹æ³• | ç²¾åº¦ | å†…å­˜èŠ‚çœ | é€Ÿåº¦æå‡ |
|---------|------|------|----------|----------|
| INT8 | åŠ¨æ€é‡åŒ– | 8ä½ | ~50% | 1.5-2x |
| INT4 | é™æ€é‡åŒ– | 4ä½ | ~75% | 2-3x |
| FP16 | åŠç²¾åº¦ | 16ä½ | ~50% | 1.5-2x |
| BF16 | è„‘æµ®ç‚¹ | 16ä½ | ~50% | 1.5-2x |

**ç« èŠ‚æ¥æº**
- [quantization_config.py](file://src/transformers/utils/quantization_config.py#L1500-L1600)

## å®Œæ•´è®­ç»ƒè„šæœ¬ç¤ºä¾‹

### æ–‡æœ¬åˆ†ç±»è®­ç»ƒç¤ºä¾‹

ä»¥ä¸‹æ˜¯åŸºäº run_classification.py çš„å®Œæ•´è®­ç»ƒè„šæœ¬ç»“æ„ï¼š

```mermaid
flowchart TD
A[åˆå§‹åŒ–] --> B[åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨]
B --> C[æ•°æ®é¢„å¤„ç†]
C --> D[åˆ›å»ºTrainer]
D --> E[è®­ç»ƒæ¨¡å‹]
E --> F[è¯„ä¼°æ¨¡å‹]
F --> G[ä¿å­˜ç»“æœ]
B --> B1[AutoModelForSequenceClassification]
B --> B2[AutoTokenizer]
C --> C1[æ•°æ®åŠ è½½]
C --> C2[æ–‡æœ¬é¢„å¤„ç†]
C --> C3[æ ‡ç­¾ç¼–ç ]
D --> D1[TrainingArguments]
D --> D2[DataCollator]
D --> D3[ComputeMetrics]
E --> E1[trainer.train]
E --> E2[è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹]
F --> F1[trainer.evaluate]
F --> F2[è®¡ç®—æŒ‡æ ‡]
G --> G1[model.save]
G --> G2[metrics.log]
```

**å›¾è¡¨æ¥æº**
- [run_classification.py](file://examples/pytorch/text-classification/run_classification.py#L400-L600)

### è¯­è¨€æ¨¡å‹è®­ç»ƒç¤ºä¾‹

è¯­è¨€æ¨¡å‹è®­ç»ƒçš„å…³é”®ç‰¹ç‚¹ï¼š

1. **å› æœè¯­è¨€å»ºæ¨¡**: ä½¿ç”¨ `AutoModelForCausalLM`
2. **æ©ç è¯­è¨€å»ºæ¨¡**: ä½¿ç”¨ `DataCollatorForLanguageModeling`
3. **æµå¼æ•°æ®å¤„ç†**: æ”¯æŒå¤§å‹æ•°æ®é›†
4. **ä¸Šä¸‹æ–‡çª—å£ç®¡ç†**: å¤„ç†é•¿åºåˆ—

**ç« èŠ‚æ¥æº**
- [run_classification.py](file://examples/pytorch/text-classification/run_classification.py#L1-L746)
- [run_clm.py](file://examples/pytorch/language-modeling/run_clm.py#L1-L200)

## æ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µ

### å†…å­˜ä¼˜åŒ–ç­–ç•¥

```mermaid
flowchart TD
A[å†…å­˜ä¼˜åŒ–] --> B[æ¢¯åº¦æ£€æŸ¥ç‚¹]
A --> C[æ··åˆç²¾åº¦]
A --> D[æ¨¡å‹å¹¶è¡Œ]
A --> E[æ•°æ®å¹¶è¡Œ]
B --> B1[æ¿€æ´»é‡è®¡ç®—]
B --> B2[å‡å°‘å†…å­˜å³°å€¼]
C --> C1[FP16/BF16]
C --> C2[æ¢¯åº¦ç¼©æ”¾]
D --> D1[FSDP]
D --> D2[DeepSpeed]
E --> E1[DDP]
E --> E2[å¤šGPUåä½œ]
```

### æ€§èƒ½ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡ç±»åˆ« | å…³é”®æŒ‡æ ‡ | ç›‘æ§æ–¹æ³• |
|---------|---------|----------|
| è®­ç»ƒæ•ˆç‡ | æ ·æœ¬/ç§’ | `speed_metrics` |
| å†…å­˜ä½¿ç”¨ | GPUå†…å­˜å ç”¨ | `TrainerMemoryTracker` |
| æŸå¤±æ›²çº¿ | è®­ç»ƒ/éªŒè¯æŸå¤± | æ—¥å¿—è®°å½• |
| å­¦ä¹ ç‡ | å½“å‰å­¦ä¹ ç‡ | `on_log`å›è°ƒ |

### æœ€ä½³å®è·µå»ºè®®

1. **æ‰¹å¤„ç†å¤§å°**: ä»è¾ƒå°å€¼å¼€å§‹ï¼Œé€æ­¥å¢åŠ 
2. **å­¦ä¹ ç‡æœç´¢**: ä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­å’Œè¡°å‡
3. **æ—©åœæœºåˆ¶**: åŸºäºéªŒè¯æŒ‡æ ‡è®¾ç½®æ—©åœ
4. **æ£€æŸ¥ç‚¹ç­–ç•¥**: å¹³è¡¡å­˜å‚¨ç©ºé—´å’Œæ¢å¤èƒ½åŠ›
5. **æ—¥å¿—è®°å½•**: è¯¦ç»†è®°å½•è®­ç»ƒè¿‡ç¨‹

## å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### å†…å­˜ç›¸å…³é—®é¢˜

```mermaid
flowchart TD
A[å†…å­˜é—®é¢˜] --> B[OOMé”™è¯¯]
A --> C[å†…å­˜æ³„æ¼]
A --> D[å†…å­˜ç¢ç‰‡]
B --> B1[å‡å°‘æ‰¹å¤„ç†å¤§å°]
B --> B2[å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹]
B --> B3[ä½¿ç”¨æ··åˆç²¾åº¦]
C --> C1[é‡Šæ”¾æœªä½¿ç”¨ç¼“å­˜]
C --> C2[æ­£ç¡®å…³é—­æ•°æ®åŠ è½½å™¨]
D --> D1[å®šæœŸæ¸…ç†å†…å­˜]
D --> D2[ä½¿ç”¨å†…å­˜ä¼˜åŒ–å·¥å…·]
```

### è®­ç»ƒæ”¶æ•›é—®é¢˜

| é—®é¢˜ç±»å‹ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|---------|----------|
| æŸå¤±ä¸ä¸‹é™ | å­¦ä¹ ç‡è¿‡é«˜/è¿‡ä½ | è°ƒæ•´å­¦ä¹ ç‡æˆ–ä½¿ç”¨è°ƒåº¦å™¨ |
| éªŒè¯æŸå¤±ä¸Šå‡ | è¿‡æ‹Ÿåˆ | å¢åŠ æ­£åˆ™åŒ–æˆ–æ—©åœ |
| è®­ç»ƒé€Ÿåº¦æ…¢ | æ•°æ®åŠ è½½ç“¶é¢ˆ | ä¼˜åŒ–æ•°æ®ç®¡é“ |
| å†…å­˜ä¸è¶³ | æ‰¹å¤„ç†è¿‡å¤§ | å‡å°æ‰¹å¤„ç†å¤§å°æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ |

### åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜

1. **åŒæ­¥é—®é¢˜**: ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹æ—¶é—´åŒæ­¥
2. **ç½‘ç»œå»¶è¿Ÿ**: ä¼˜åŒ–é€šä¿¡æ‹“æ‰‘
3. **è®¾å¤‡ä¸åŒ¹é…**: ç»Ÿä¸€ç¡¬ä»¶è§„æ ¼
4. **æ£€æŸ¥ç‚¹å†²çª**: ä½¿ç”¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ

**ç« èŠ‚æ¥æº**
- [trainer_utils.py](file://src/transformers/trainer_utils.py#L600-L800)

## ç»“è®º

Transformers çš„ Trainer API æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒä»åŸºç¡€æ–‡æœ¬åˆ†ç±»åˆ°å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å„ç§è®­ç»ƒä»»åŠ¡ã€‚é€šè¿‡åˆç†é…ç½®è®­ç»ƒå‚æ•°ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œæ•°æ®å¤„ç†æµç¨‹ï¼Œå¯ä»¥å®ç°é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚

å…³é”®è¦ç‚¹ï¼š
- é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
- åˆ©ç”¨åˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦æå‡æ•ˆç‡
- ä½¿ç”¨ PEFT æŠ€æœ¯è¿›è¡Œé«˜æ•ˆå¾®è°ƒ
- å®æ–½é€‚å½“çš„ç›‘æ§å’Œè°ƒè¯•ç­–ç•¥
- éµå¾ªæœ€ä½³å®è·µä»¥è·å¾—æœ€ä½³æ€§èƒ½

é€šè¿‡æœ¬æŒ‡å—çš„å­¦ä¹ ï¼Œç”¨æˆ·åº”è¯¥èƒ½å¤ŸæŒæ¡ Transformers è®­ç»ƒçš„æ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®å…·ä½“ä»»åŠ¡éœ€æ±‚è®¾è®¡å’Œå®æ–½æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆã€‚