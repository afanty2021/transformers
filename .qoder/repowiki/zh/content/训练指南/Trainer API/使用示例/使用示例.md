# 使用示例

<cite>
**本文档中引用的文件**  
- [run_glue.py](file://examples/pytorch/text-classification/run_glue.py)
- [run_clm.py](file://examples/pytorch/language-modeling/run_clm.py)
- [run_mlm.py](file://examples/pytorch/language-modeling/run_mlm.py)
- [run_summarization.py](file://examples/pytorch/summarization/run_summarization.py)
- [run_translation.py](file://examples/pytorch/translation/run_translation.py)
- [trainer.py](file://src/transformers/trainer.py)
- [training_args.py](file://src/transformers/training_args.py)
- [data_collator.py](file://src/transformers/data/data_collator.py)
- [peft.py](file://src/transformers/integrations/peft.py)
</cite>

## 目录
1. [简介](#简介)
2. [文本分类任务示例](#文本分类任务示例)
3. [语言建模任务示例](#语言建模任务示例)
4. [文本生成任务示例](#文本生成任务示例)
5. [自定义训练流程](#自定义训练流程)
6. [高级用法](#高级用法)

## 简介
本示例文档展示了如何使用Transformers库中的Trainer API进行各种自然语言处理任务的训练。文档涵盖了文本分类、语言建模和文本生成等典型应用场景，提供了完整的代码流程，包括数据集准备、模型初始化、分词器配置、数据整理器选择、TrainingArguments设置和Trainer实例化等关键步骤。

**Section sources**
- [trainer.py](file://src/transformers/trainer.py#L1-L100)
- [training_args.py](file://src/transformers/training_args.py#L1-L100)

## 文本分类任务示例
本示例展示了如何使用Trainer API在GLUE基准数据集上进行文本分类任务的微调。代码流程包括数据集加载、预处理、模型初始化和训练配置。

```python
# 数据集准备和加载
from datasets import load_dataset
dataset = load_dataset("glue", "mrpc")

# 模型和分词器初始化
from transformers import AutoModelForSequenceClassification, AutoTokenizer
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# 数据预处理
def preprocess_function(examples):
    return tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, padding=True)

encoded_dataset = dataset.map(preprocess_function, batched=True)

# 数据整理器配置
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 训练参数设置
from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
)

# 训练器实例化和训练
from transformers import Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()
```

**Section sources**
- [run_glue.py](file://examples/pytorch/text-classification/run_glue.py#L1-L200)
- [data_collator.py](file://src/transformers/data/data_collator.py#L1-L50)

## 语言建模任务示例
本示例展示了如何使用Trainer API进行因果语言建模（CLM）和掩码语言建模（MLM）任务的训练。

### 因果语言建模（CLM）
```python
# 模型初始化
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("gpt2")

# 数据预处理
def group_texts(examples):
    # 将文本连接并分块
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    total_length = (total_length // block_size) * block_size
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

# 数据整理器配置
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)

# 训练参数设置
training_args = TrainingArguments(
    output_dir="./clm_results",
    overwrite_output_dir=True,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    save_steps=10_000,
    save_total_limit=2,
)

# 训练器配置
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

### 掩码语言建模（MLM）
```python
# 模型初始化
from transformers import AutoModelForMaskedLM
model = AutoModelForMaskedLM.from_pretrained("bert-base-uncased")

# 数据整理器配置（启用MLM）
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)

# 其他配置与CLM类似
training_args = TrainingArguments(
    output_dir="./mlm_results",
    # ... 其他参数
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

**Section sources**
- [run_clm.py](file://examples/pytorch/language-modeling/run_clm.py#L1-L200)
- [run_mlm.py](file://examples/pytorch/language-modeling/run_mlm.py#L1-L200)
- [data_collator.py](file://src/transformers/data/data_collator.py#L500-L600)

## 文本生成任务示例
本示例展示了如何使用Trainer API进行摘要生成和机器翻译等文本生成任务。

### 摘要生成
```python
# 模型初始化
from transformers import AutoModelForSeq2SeqLM
model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

# 数据预处理
def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["document"]]
    targets = examples["summary"]
    model_inputs = tokenizer(inputs, max_length=max_source_length, padding="max_length", truncation=True)
    
    # 设置标签
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, padding="max_length", truncation=True)
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# 数据整理器配置
from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# 训练参数设置
training_args = Seq2SeqTrainingArguments(
    output_dir="./summarization_results",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    predict_with_generate=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_steps=100,
)

# 训练器配置
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
```

### 机器翻译
```python
# 数据预处理
def preprocess_function(examples):
    inputs = [example[source_lang] for example in examples["translation"]]
    targets = [example[target_lang] for example in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_source_length, padding="max_length", truncation=True)
    
    # 设置标签
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, padding="max_length", truncation=True)
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# 其他配置与摘要生成类似
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
```

**Section sources**
- [run_summarization.py](file://examples/pytorch/summarization/run_summarization.py#L1-L200)
- [run_translation.py](file://examples/pytorch/translation/run_translation.py#L1-L200)
- [data_collator.py](file://src/transformers/data/data_collator.py#L700-L800)

## 自定义训练流程
本示例展示了如何自定义Trainer的训练流程，包括添加自定义回调、修改训练循环和集成PEFT（参数高效微调）。

### 自定义回调
```python
from transformers import TrainerCallback

class CustomCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        print(f"Epoch {state.epoch} completed")
        # 可以在这里添加自定义逻辑，如保存特定检查点
    
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 100 == 0:
            print(f"Step {state.global_step} completed")
    
    def on_train_begin(self, args, state, control, **kwargs):
        print("Training started")

# 在训练器中使用自定义回调
trainer = Trainer(
    # ... 其他参数
    callbacks=[CustomCallback()],
)
```

### 修改训练循环
```python
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        # 自定义损失计算逻辑
        outputs = model(**inputs)
        logits = outputs.get("logits")
        labels = inputs.get("labels")
        
        # 自定义损失函数
        loss_fct = torch.nn.CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss
    
    def training_step(self, model, inputs):
        # 自定义训练步骤
        model.train()
        inputs = self._prepare_inputs(inputs)
        
        with self.autocast_smart_context_manager():
            loss = self.compute_loss(model, inputs)
        
        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps
        
        loss.backward()
        
        return loss.detach()
```

**Section sources**
- [trainer.py](file://src/transformers/trainer.py#L1000-L2000)
- [trainer_callback.py](file://src/transformers/trainer_callback.py#L1-L100)

## 高级用法
本示例展示了如何使用PEFT（参数高效微调）进行高效训练，以及如何集成其他高级功能。

### PEFT集成
```python
from peft import LoraConfig, get_peft_model
from transformers import Trainer

# 配置LoRA参数
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# 将LoRA适配器应用到模型
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
model = get_peft_model(model, peft_config)

# 训练参数设置（注意：PEFT训练通常需要更大的学习率）
training_args = TrainingArguments(
    output_dir="./peft_results",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=3e-4,  # PEFT通常使用更高的学习率
    num_train_epochs=3,
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    report_to="wandb",  # 集成W&B进行实验跟踪
)

# 训练器配置
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)

trainer.train()

# 保存适配器权重
model.save_pretrained("peft_adapter")
```

### NEFTune噪声嵌入
```python
# 在训练参数中启用NEFTune
training_args = TrainingArguments(
    # ... 其他参数
    neftune_noise_alpha=5.0,  # 噪声强度，通常在5.0-15.0之间
)

# Trainer会自动处理NEFTune的激活
trainer = Trainer(
    model=model,
    args=training_args,
    # ... 其他参数
)
```

### DeepSpeed集成
```python
# 创建DeepSpeed配置文件（ds_config.json）
{
  "fp16": {
    "enabled": true
  },
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 2e-5
    }
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
      "device": "cpu"
    }
  }
}

# 在训练参数中指定DeepSpeed配置
training_args = TrainingArguments(
    # ... 其他参数
    deepspeed="ds_config.json",
    per_device_train_batch_size=1,  # DeepSpeed通常使用较小的批大小
    gradient_accumulation_steps=32,  # 通过梯度累积补偿
)
```

**Section sources**
- [peft.py](file://src/transformers/integrations/peft.py#L1-L200)
- [trainer.py](file://src/transformers/trainer.py#L2000-L3000)
- [training_args.py](file://src/transformers/training_args.py#L500-L1000)