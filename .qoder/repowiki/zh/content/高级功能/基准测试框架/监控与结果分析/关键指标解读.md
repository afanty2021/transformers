# 关键指标解读

<cite>
**本文档引用的文件**  
- [grafana_datasource.yaml](file://benchmark/grafana_datasource.yaml)
- [grafana_dashboard.json](file://benchmark/grafana_dashboard.json)
- [prometheus.yml](file://examples/metrics-monitoring/prometheus.yml)
- [docker-compose.yml](file://examples/metrics-monitoring/docker-compose.yml)
- [grafana-datasources.yaml](file://examples/metrics-monitoring/grafana-datasources.yaml)
- [grafana-dashboard.yaml](file://examples/metrics-monitoring/grafana-dashboard.yaml)
- [benchmark.py](file://benchmark/benchmark.py)
- [llama.py](file://benchmark/benches/llama.py)
- [benchmarks_entrypoint.py](file://benchmark/benchmarks_entrypoint.py)
- [benchmark_runner.py](file://benchmark_v2/framework/benchmark_runner.py)
- [hardware_metrics.py](file://benchmark_v2/framework/hardware_metrics.py)
</cite>

## 目录
1. [引言](#引言)
2. [数据源配置与Prometheus集成](#数据源配置与prometheus集成)
3. [核心性能指标详解](#核心性能指标详解)
4. [指标采集频率与存储策略](#指标采集频率与存储策略)
5. [系统瓶颈识别与诊断](#系统瓶颈识别与诊断)
6. [异常诊断流程与解决方案](#异常诊断流程与解决方案)

## 引言
本文档旨在深入解读transformers基准测试中监控系统收集的核心性能指标。通过分析grafana_datasource.yaml中的数据源配置，阐述其与Prometheus的集成方式。详细解释监控系统采集的关键指标，包括请求吞吐量、端到端延迟、内存占用、GPU利用率和批处理效率等。说明每个指标的计算方法、正常范围和性能意义，并指导用户如何通过指标变化趋势识别系统瓶颈。结合prometheus.yml中的监控配置，说明指标采集的频率和存储策略，提供指标异常的诊断流程和常见问题解决方案。

## 数据源配置与Prometheus集成
监控系统的数据源配置在grafana_datasource.yaml文件中定义，该文件配置了Grafana与PostgreSQL数据库的连接，用于存储和查询基准测试的性能指标数据。数据源名称为"grafana-postgresql-datasource"，类型为PostgreSQL，连接URL、用户名和密码通过环境变量注入，确保了配置的安全性和灵活性。数据库名称为"metrics"，连接池配置最大打开连接数为100，最大空闲连接数为100，连接最大生命周期为14400秒。

在examples/metrics-monitoring目录下，grafana-datasources.yaml文件定义了Grafana与Prometheus的集成。Prometheus数据源的名称为"Prometheus"，类型为prometheus，访问方式为代理，URL指向本地的Prometheus服务（http://prometheus:9090），并设置为默认数据源。同时，还配置了Tempo数据源，用于分布式追踪。这种配置使得Grafana能够从Prometheus拉取实时监控指标，并从PostgreSQL查询历史基准测试数据，实现监控与分析的统一视图。

docker-compose.yml文件定义了监控系统的容器化部署，包括Prometheus、Grafana、Tempo和Memcached服务。Prometheus容器挂载了本地的prometheus.yml配置文件，Grafana容器则挂载了仪表板和数据源配置文件，实现了配置的外部化和版本控制。

**Section sources**
- [grafana_datasource.yaml](file://benchmark/grafana_datasource.yaml)
- [grafana-datasources.yaml](file://examples/metrics-monitoring/grafana-datasources.yaml)
- [docker-compose.yml](file://examples/metrics-monitoring/docker-compose.yml)

## 核心性能指标详解
监控系统采集的关键性能指标分为模型性能指标和系统资源指标两大类。

### 模型性能指标
模型性能指标主要反映模型推理的效率和质量，包括：
- **请求吞吐量（Throughput）**：单位时间内成功处理的请求数量或生成的token数量，通常以"tokens/s"为单位。高吞吐量表示系统处理能力强。在benchmark.py中，通过`per_token.throughput.value`等指标进行衡量。
- **端到端延迟（End-to-End Latency）**：从接收到请求到返回完整响应的总时间，包括预填充（prefill）和解码（decode）阶段的延迟。在llama.py中，通过`time_to_first_token_secs`、`time_to_second_token_secs`等指标测量首token和后续token的生成延迟。
- **批处理效率（Batch Processing Efficiency）**：衡量批处理大小对性能的影响，通过比较不同批处理大小下的吞吐量和延迟来评估。在benchmark_runner.py中，通过配置不同的`batch_size`参数进行测试。

### 系统资源指标
系统资源指标反映硬件资源的使用情况，用于识别系统瓶颈：
- **内存占用（Memory Usage）**：包括CPU内存和GPU显存的使用量。在llama.py中，通过psutil库收集CPU内存使用量（`mem_megabytes`），通过gpustat库收集GPU显存使用量（`gpu_mem_megabytes`）。
- **GPU利用率（GPU Utilization）**：GPU计算核心的使用率，反映GPU的繁忙程度。在hardware_metrics.py中，通过`get_nvidia_gpu_stats`或`get_amd_gpu_stats`函数获取NVIDIA或AMD GPU的利用率（`gpu_util`）。
- **CPU利用率（CPU Utilization）**：CPU的使用率，反映CPU的负载情况。在llama.py中，通过psutil库的`cpu_percent()`方法获取。

这些指标的正常范围取决于具体的硬件配置和模型大小。一般来说，GPU利用率持续高于80%可能表示GPU瓶颈，CPU利用率持续高于70%可能表示CPU瓶颈，内存使用接近物理限制可能表示内存瓶颈。

**Section sources**
- [llama.py](file://benchmark/benches/llama.py#L0-L353)
- [hardware_metrics.py](file://benchmark_v2/framework/hardware_metrics.py#L43-L73)
- [benchmark_runner.py](file://benchmark_v2/framework/benchmark_runner.py#L0-L456)

## 指标采集频率与存储策略
指标采集的频率由Prometheus的配置文件prometheus.yml决定。该文件中的`scrape_interval`设置为15秒，表示Prometheus每15秒从目标服务拉取一次指标数据。这个间隔在保证监控实时性的同时，也避免了过于频繁的采集对系统性能造成影响。

存储策略方面，实时监控指标由Prometheus存储，其本地存储支持高效的时间序列数据查询。历史基准测试数据则存储在PostgreSQL数据库中，通过grafana_datasource.yaml配置的连接进行访问。在benchmarks_entrypoint.py中，`MetricsRecorder`类负责将设备测量数据（如CPU/GPU利用率）和模型测量数据（如延迟、吞吐量）分别插入到`device_measurements`和`model_measurements`表中。这种分离存储的策略使得实时监控和历史分析可以独立进行，互不影响。

**Section sources**
- [prometheus.yml](file://examples/metrics-monitoring/prometheus.yml#L0-L3)
- [benchmarks_entrypoint.py](file://benchmark/benchmarks_entrypoint.py#L166-L187)

## 系统瓶颈识别与诊断
通过分析关键指标的变化趋势，可以有效识别系统瓶颈：

### CPU瓶颈
当CPU利用率持续接近100%，而GPU利用率较低时，可能存在CPU瓶颈。这通常发生在数据预处理或后处理阶段，CPU无法及时为GPU提供数据。解决方案包括优化数据加载管道、使用更高效的数据格式或增加CPU资源。

### GPU瓶颈
当GPU利用率持续高于80%，且增加批处理大小无法显著提高吞吐量时，可能存在GPU瓶颈。这表示GPU计算能力已成为限制因素。解决方案包括使用更高效的注意力机制（如Flash Attention）、模型量化或升级到更高性能的GPU。

### 内存泄漏
如果内存或显存使用量随时间持续增长，且不随请求结束而释放，可能存在内存泄漏。在监控图表中表现为内存使用曲线呈上升趋势。需要检查代码中是否有未释放的张量或缓存。

### I/O等待
当CPU和GPU利用率都较低，但系统响应延迟较高时，可能存在I/O等待问题。这可能是由于磁盘读写或网络传输速度慢导致。需要检查存储系统性能或网络带宽。

**Section sources**
- [llama.py](file://benchmark/benches/llama.py#L51-L60)
- [hardware_metrics.py](file://benchmark_v2/framework/hardware_metrics.py#L75-L108)

## 异常诊断流程与解决方案
当发现性能指标异常时，应遵循以下诊断流程：

1. **确认问题**：通过Grafana仪表板确认异常指标，检查是单个指标异常还是多个指标同时异常。
2. **隔离范围**：确定问题是全局性的还是特定于某个模型、配置或硬件。
3. **检查资源**：查看CPU、GPU、内存和I/O的使用情况，识别资源瓶颈。
4. **分析日志**：检查应用日志和系统日志，寻找错误信息或警告。
5. **复现问题**：尝试在测试环境中复现问题，便于调试。
6. **应用解决方案**：根据诊断结果应用相应的解决方案。

常见问题及解决方案：
- **高延迟低吞吐量**：检查是否启用了高效的注意力机制，尝试使用Flash Attention或SDPA。
- **GPU利用率低**：检查批处理大小是否过小，适当增加批处理大小以提高GPU利用率。
- **内存溢出**：检查模型是否过大，尝试使用模型量化或梯度检查点技术减少内存使用。
- **性能波动大**：检查系统是否有其他高负载任务干扰，确保测试环境的纯净性。

**Section sources**
- [benchmark_runner.py](file://benchmark_v2/framework/benchmark_runner.py#L0-L456)
- [benchmark_config.py](file://benchmark_v2/framework/benchmark_config.py#L0-L214)