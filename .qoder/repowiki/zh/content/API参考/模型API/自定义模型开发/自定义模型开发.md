# 自定义模型开发

<cite>
**本文档中引用的文件**  
- [configuration_my_new_model2.py](file://examples/modular-transformers/configuration_my_new_model2.py)
- [modeling_my_new_model2.py](file://examples/modular-transformers/modeling_my_new_model2.py)
- [modeling_auto.py](file://src/transformers/models/auto/modeling_auto.py)
- [auto_factory.py](file://src/transformers/models/auto/auto_factory.py)
- [configuration_utils.py](file://src/transformers/configuration_utils.py)
- [modeling_utils.py](file://src/transformers/modeling_utils.py)
- [test_modeling_common.py](file://tests/test_modeling_common.py)
</cite>

## 目录
1. [简介](#简介)
2. [配置类定义](#配置类定义)
3. [模型架构实现](#模型架构实现)
4. [前向传播逻辑](#前向传播逻辑)
5. [AutoModel系统集成](#automodel系统集成)
6. [权重初始化与参数绑定](#权重初始化与参数绑定)
7. [模型测试与验证](#模型测试与验证)
8. [案例研究：创建变体模型](#案例研究创建变体模型)
9. [结论](#结论)

## 简介
本文档全面介绍了在transformers库中开发自定义模型的完整流程。文档涵盖了从定义配置类到实现前向传播逻辑的各个方面，详细说明了如何将新模型类型注册到AutoModel系统以确保工厂模式的兼容性。同时，文档提供了模型权重初始化、参数绑定和共享机制的最佳实践，并详细说明了模型测试要求和验证流程，以确保新模型符合库的标准。通过实际案例，展示了如何基于现有架构创建变体模型以及如何实现全新的神经网络架构。

## 配置类定义
在transformers库中，每个模型都需要一个对应的配置类，该类继承自`PreTrainedConfig`。配置类定义了模型的超参数和架构设置，是模型实例化的基础。

配置类的主要职责包括：
- 定义模型类型标识符（`model_type`）
- 设置模型的超参数（如隐藏层大小、注意力头数等）
- 提供序列化和反序列化功能
- 管理模型的元数据

配置类通常包含以下关键属性：
- `model_type`: 模型类型的字符串标识符
- `vocab_size`: 词汇表大小
- `hidden_size`: 隐藏层大小
- `num_hidden_layers`: 隐藏层数量
- `num_attention_heads`: 注意力头数量
- `intermediate_size`: 中间层大小
- `hidden_act`: 隐藏层激活函数
- `max_position_embeddings`: 最大位置嵌入
- `initializer_range`: 初始化范围
- `rms_norm_eps`: RMS归一化epsilon值

```python
class MyNewModel2Config(PreTrainedConfig):
    model_type = "my_new_model2"
    
    def __init__(
        self,
        vocab_size: Optional[int] = 32000,
        hidden_size: Optional[int] = 4096,
        intermediate_size: Optional[int] = 11008,
        num_hidden_layers: Optional[int] = 32,
        num_attention_heads: Optional[int] = 32,
        num_key_value_heads: Optional[int] = None,
        hidden_act: Optional[str] = "silu",
        max_position_embeddings: Optional[int] = 2048,
        initializer_range: Optional[float] = 0.02,
        rms_norm_eps: Optional[int] = 1e-6,
        use_cache: Optional[bool] = True,
        pad_token_id: Optional[int] = None,
        bos_token_id: Optional[int] = 1,
        eos_token_id: Optional[int] = 2,
        pretraining_tp: Optional[int] = 1,
        tie_word_embeddings: Optional[bool] = False,
        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,
        attention_bias: Optional[bool] = False,
        attention_dropout: Optional[float] = 0.0,
        mlp_bias: Optional[bool] = False,
        head_dim: Optional[int] = None,
        **kwargs,
    ):
        # 初始化配置参数
        super().__init__(...)
```

配置类还支持高级功能，如：
- `base_model_tp_plan`: 基础模型的张量并行计划
- `base_model_pp_plan`: 基础模型的流水线并行计划
- `keys_to_ignore_at_inference`: 推理时忽略的键

**Section sources**
- [configuration_my_new_model2.py](file://examples/modular-transformers/configuration_my_new_model2.py#L1-L114)

## 模型架构实现
模型架构的实现遵循transformers库的标准模式，通常包括以下几个核心组件：

### 基础模型类
基础模型类继承自`PreTrainedModel`，并定义了模型的核心架构。每个模型类必须指定其对应的配置类：

```python
class MyNewModel2PreTrainedModel(PreTrainedModel):
    config_class = MyNewModel2Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["MyNewModel2DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
```

### 模型组件
模型通常由多个可重用的组件构成，这些组件包括：

#### RMS归一化层
```python
class MyNewModel2RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.zeros(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float())
        output = output * (1.0 + self.weight.float())
        return output.type_as(x)
```

#### 多层感知机（MLP）
```python
class MyNewModel2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj
```

#### 注意力机制
```python
class MyNewModel2Attention(nn.Module):
    def __init__(self, config: MyNewModel2Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = not getattr(config, "use_bidirectional_attention", False)

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
```

#### 解码器层
```python
class MyNewModel2DecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: MyNewModel2Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = MyNewModel2Attention(config=config, layer_idx=layer_idx)
        self.mlp = MyNewModel2MLP(config)
        self.input_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
```

**Section sources**
- [modeling_my_new_model2.py](file://examples/modular-transformers/modeling_my_new_model2.py#L1-L272)

## 前向传播逻辑
前向传播逻辑是模型的核心，它定义了输入数据如何通过模型的各个层并产生输出。在transformers库中，前向传播通常遵循以下模式：

### 注意力前向传播
```python
def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights
```

### 解码器层前向传播
```python
def forward(
    self,
    hidden_states: torch.Tensor,
    attention_mask: Optional[torch.Tensor] = None,
    position_ids: Optional[torch.LongTensor] = None,
    past_key_values: Optional[Cache] = None,
    use_cache: Optional[bool] = False,
    cache_position: Optional[torch.LongTensor] = None,
    position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
    **kwargs: Unpack[TransformersKwargs],
) -> torch.Tensor:
    residual = hidden_states
    hidden_states = self.input_layernorm(hidden_states)
    # Self Attention
    hidden_states, _ = self.self_attn(
        hidden_states=hidden_states,
        attention_mask=attention_mask,
        position_ids=position_ids,
        past_key_values=past_key_values,
        use_cache=use_cache,
        cache_position=cache_position,
        position_embeddings=position_embeddings,
        **kwargs,
    )
    hidden_states = residual + hidden_states

    # Fully Connected
    residual = hidden_states
    hidden_states = self.post_attention_layernorm(hidden_states)
    hidden_states = self.mlp(hidden_states)
    hidden_states = residual + hidden_states
    return hidden_states
```

### 旋转位置嵌入
```python
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

**Section sources**
- [modeling_my_new_model2.py](file://examples/modular-transformers/modeling_my_new_model2.py#L1-L272)

## AutoModel系统集成
AutoModel系统是transformers库的核心功能之一，它允许用户通过工厂模式动态创建模型实例。要将新模型集成到AutoModel系统中，需要执行以下步骤：

### 模型注册
新模型需要在AutoModel的映射中注册，以便系统能够识别和实例化它。这通常通过`register`方法完成：

```python
# 在测试中演示模型注册
def test_from_pretrained_dynamic_model_local(self):
    try:
        AutoConfig.register("custom", CustomConfig)
        AutoModel.register(CustomConfig, CustomModel)

        config = CustomConfig(hidden_size=32)
        model = CustomModel(config)

        with tempfile.TemporaryDirectory() as tmp_dir:
            model.save_pretrained(tmp_dir)
            model = AutoModel.from_pretrained(tmp_dir)
            self.assertIsInstance(model, CustomModel)
```

### 映射配置
AutoModel系统使用多个映射来管理不同类型的模型：

```python
# MODEL_MAPPING_NAMES定义了基础模型的映射
MODEL_MAPPING_NAMES = OrderedDict([
    ("my_new_model2", "MyNewModel2Model"),
    # ... 其他模型
])

# MODEL_FOR_CAUSAL_LM_MAPPING_NAMES定义了因果语言模型的映射
MODEL_FOR_CAUSAL_LM_MAPPING_NAMES = OrderedDict([
    ("my_new_model2", "MyNewModel2ForCausalLM"),
    # ... 其他模型
])
```

### 工厂模式实现
AutoModel系统基于工厂模式实现，其核心逻辑在`auto_factory.py`中：

```python
class _BaseAutoModelClass:
    _model_mapping = None

    def __init__(self, *args, **kwargs) -> None:
        raise OSError(
            f"{self.__class__.__name__} is designed to be instantiated "
            f"using the `{self.__class__.__name__}.from_pretrained(pretrained_model_name_or_path)` or "
            f"`{self.__class__.__name__}.from_config(config)` methods."
        )

    @classmethod
    def from_config(cls, config, **kwargs):
        # 根据配置创建模型实例
        pass

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[str]], *model_args, **kwargs):
        # 从预训练模型创建实例
        pass

    @classmethod
    def register(cls, config_class, model_class, exist_ok=False) -> None:
        """
        Register a new model for this class.
        
        Args:
            config_class ([`PreTrainedConfig`]):
                The configuration corresponding to the model to register.
            model_class ([`PreTrainedModel`]):
                The model to register.
        """
        if hasattr(model_class, "config_class") and model_class.config_class.__name__ != config_class.__name__:
            raise ValueError(
                "The model class you are passing has a `config_class` attribute that is not consistent with the "
                f"config class you passed (model has {model_class.config_class} and you passed {config_class}. Fix "
                "one of those so they match!"
            )
        cls._model_mapping.register(config_class, model_class, exist_ok=exist_ok)
```

**Section sources**
- [modeling_auto.py](file://src/transformers/models/auto/modeling_auto.py#L1-L2390)
- [auto_factory.py](file://src/transformers/models/auto/auto_factory.py#L1-L645)

## 权重初始化与参数绑定
权重初始化和参数绑定是确保模型正确训练和推理的关键环节。transformers库提供了标准化的机制来处理这些任务。

### 权重初始化
模型通过`_init_weights`方法进行权重初始化：

```python
class MyNewModel2PreTrainedModel(PreTrainedModel):
    def _init_weights(self, module):
        super()._init_weights(module)

        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)
        if "RMSNorm" in module.__class__.__name__:
            module.weight.data.zero_()
```

标准的初始化策略包括：
- 线性层：使用正态分布初始化
- 嵌入层：使用正态分布初始化
- 归一化层：权重初始化为零或一
- 偏置：初始化为零

### 参数绑定
参数绑定机制用于共享模型中的参数，减少内存使用并提高训练效率：

```python
def _get_tied_weight_keys(module: nn.Module, prefix=""):
    tied_weight_keys = []
    if getattr(module, "_tied_weights_keys", None) is not None:
        names = [f"{prefix}.{k}" if prefix else k for k in module._tied_weights_keys]
        tied_weight_keys.extend(names)
    if getattr(module, "_dynamic_tied_weights_keys", None) is not None:
        names = [f"{prefix}.{k}" if prefix else k for k in module._dynamic_tied_weights_keys]
        tied_weight_keys.extend(names)
    for name, submodule in module.named_children():
        local_prefix = f"{prefix}.{name}" if prefix else name
        tied_weight_keys.extend(_get_tied_weight_keys(submodule, prefix=local_prefix))
    return tied_weight_keys
```

常见的参数绑定场景包括：
- 词嵌入和输出层权重共享
- 编码器-解码器架构中的参数共享
- 多任务学习中的共享层

**Section sources**
- [modeling_my_new_model2.py](file://examples/modular-transformers/modeling_my_new_model2.py#L1-L272)
- [modeling_utils.py](file://src/transformers/modeling_utils.py#L1-L5420)

## 模型测试与验证
模型测试和验证是确保新模型符合库标准的关键步骤。transformers库提供了一套完整的测试框架来验证模型的正确性。

### 测试框架
核心测试类`ModelTesterMixin`提供了标准化的测试方法：

```python
class ModelTesterMixin:
    model_tester_class = None
    all_model_classes = ()
    fx_compatible = False

    def test_save_load(self):
        # 测试模型保存和加载
        pass

    def test_save_load_keys_to_ignore_on_save(self):
        # 测试保存时忽略的键
        pass

    def test_gradient_checkpointing_enable_disable(self):
        # 测试梯度检查点
        pass

    def test_save_load_fast_init_from_base(self):
        # 测试快速初始化
        pass

    def test_save_load_fast_init_to_base(self):
        # 测试快速初始化到基础模型
        pass
```

### 验证流程
验证流程包括以下关键步骤：

1. **配置验证**：确保配置类的参数正确无误
2. **模型实例化**：验证模型可以正确创建
3. **前向传播**：验证前向传播逻辑的正确性
4. **保存加载**：验证模型可以正确保存和加载
5. **梯度检查**：验证梯度计算的正确性
6. **设备兼容性**：验证模型在不同设备上的兼容性

### 测试要求
新模型必须满足以下测试要求：
- 通过所有核心测试用例
- 支持CPU和GPU设备
- 支持混合精度训练
- 支持梯度检查点
- 支持模型并行
- 文档完整且准确

**Section sources**
- [test_modeling_common.py](file://tests/test_modeling_common.py#L1-L3942)

## 案例研究：创建变体模型
本节通过实际案例展示如何基于现有架构创建变体模型。

### 创建新模型变体
使用`add_new_model_like.py`脚本可以快速创建基于现有模型的新变体：

```python
class ModelInfos:
    """
    Retrieve the basic information about an existing model classes.
    """

    def __init__(self, lowercase_name: str):
        # Just to make sure it's indeed lowercase
        self.lowercase_name = lowercase_name.lower().replace(" ", "_").replace("-", "_")
        if self.lowercase_name not in CONFIG_MAPPING_NAMES:
            self.lowercase_name.replace("_", "-")
        if self.lowercase_name not in CONFIG_MAPPING_NAMES:
            raise ValueError(f"{lowercase_name} is not a valid model name")

        self.paper_name = MODEL_NAMES_MAPPING[self.lowercase_name]
        self.config_class = CONFIG_MAPPING_NAMES[self.lowercase_name]
        self.camelcase_name = self.config_class.replace("Config", "")
```

### 模块化文件创建
创建模块化文件以支持新模型：

```python
def create_modular_file(
    repo_path: Path,
    old_model_infos: ModelInfos,
    new_lowercase_name: str,
    filenames_to_add: list[tuple[str, bool]],
) -> str:
    """
    Create a new modular file which will copy the old model, based on the new name and the different filenames
    (modules) to add.
    """
    # Implementation details
    pass
```

### 测试文件创建
为新模型创建相应的测试文件：

```python
def create_test_files(
    repo_path: Path, old_model_infos: ModelInfos, new_lowercase_name, filenames_to_add: list[tuple[str, bool]]
):
    """
    Create the test files for the new model. It basically copies over the old test files and adjust the class names.
    """
    # Implementation details
    pass
```

**Section sources**
- [add_new_model_like.py](file://src/transformers/cli/add_new_model_like.py#L1-L464)

## 结论
本文档全面介绍了在transformers库中开发自定义模型的完整流程。从定义配置类到实现前向传播逻辑，再到将新模型集成到AutoModel系统，文档提供了详细的指导和最佳实践。通过遵循这些指南，开发者可以创建符合库标准的高质量模型，确保其与现有生态系统无缝集成。模型测试和验证流程确保了新模型的可靠性和性能，而案例研究展示了如何基于现有架构创建变体模型。这些实践不仅提高了开发效率，还保证了代码质量和可维护性。