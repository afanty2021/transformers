# æ¨ç†æŒ‡å—

<cite>
**æœ¬æ–‡æ¡£ä¸­å¼•ç”¨çš„æ–‡ä»¶**
- [src/transformers/pipelines/__init__.py](file://src/transformers/pipelines/__init__.py)
- [src/transformers/pipelines/base.py](file://src/transformers/pipelines/base.py)
- [src/transformers/quantizers/__init__.py](file://src/transformers/quantizers/__init__.py)
- [src/transformers/models/auto/modeling_auto.py](file://src/transformers/models/auto/modeling_auto.py)
- [src/transformers/generation/continuous_batching/continuous_api.py](file://src/transformers/generation/continuous_batching/continuous_api.py)
- [examples/pytorch/text-generation/run_generation.py](file://examples/pytorch/text-generation/run_generation.py)
- [examples/pytorch/text-generation/README.md](file://examples/pytorch/text-generation/README.md)
- [src/transformers/utils/logging.py](file://src/transformers/utils/logging.py)
- [src/transformers/integrations/accelerate.py](file://src/transformers/integrations/accelerate.py)
</cite>

## ç›®å½•
1. [ç®€ä»‹](#ç®€ä»‹)
2. [é¡¹ç›®ç»“æ„æ¦‚è§ˆ](#é¡¹ç›®ç»“æ„æ¦‚è§ˆ)
3. [Pipeline APIåŸºç¡€](#pipeline-apiåŸºç¡€)
4. [ä½çº§APIè¯¦è§£](#ä½çº§apiè¯¦è§£)
5. [æ‰¹å¤„ç†ä¸åºåˆ—ç®¡ç†](#æ‰¹å¤„ç†ä¸åºåˆ—ç®¡ç†)
6. [å®æ—¶æ¨ç†ä¸è¿ç»­æ‰¹å¤„ç†](#å®æ—¶æ¨ç†ä¸è¿ç»­æ‰¹å¤„ç†)
7. [æ¨¡å‹é‡åŒ–ä¸ä¼˜åŒ–](#æ¨¡å‹é‡åŒ–ä¸ä¼˜åŒ–)
8. [è®¾å¤‡æ˜ å°„ä¸åˆ†å¸ƒå¼æ¨ç†](#è®¾å¤‡æ˜ å°„ä¸åˆ†å¸ƒå¼æ¨ç†)
9. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
10. [æ•…éšœæ’é™¤æŒ‡å—](#æ•…éšœæ’é™¤æŒ‡å—)
11. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
12. [æ€»ç»“](#æ€»ç»“)

## ç®€ä»‹

æœ¬æŒ‡å—å…¨é¢ä»‹ç»äº†ä½¿ç”¨Hugging Face Transformersåº“è¿›è¡Œæ¨¡å‹æ¨ç†çš„å„ç§æ–¹æ³•å’ŒæŠ€æœ¯ã€‚ä»ç®€å•çš„Pipeline APIåˆ°å¤æ‚çš„ä½çº§æ¨¡å‹è°ƒç”¨ï¼Œå†åˆ°é«˜çº§çš„é‡åŒ–å’Œä¼˜åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æ¯ç§æ¨ç†æ–¹å¼çš„ç‰¹ç‚¹ã€é€‚ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µã€‚

æ¨ç†æ˜¯å°†è®­ç»ƒå¥½çš„æ¨¡å‹åº”ç”¨äºæ–°æ•°æ®çš„è¿‡ç¨‹ï¼Œæ˜¯æœºå™¨å­¦ä¹ åº”ç”¨çš„æ ¸å¿ƒç¯èŠ‚ã€‚é€šè¿‡æœ¬æŒ‡å—ï¼Œæ‚¨å°†å­¦ä¼šå¦‚ä½•é«˜æ•ˆåœ°éƒ¨ç½²å’Œä¼˜åŒ–å„ç§ç±»å‹çš„æ¨¡å‹æ¨ç†ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€é—®ç­”ç­‰å„ç§ä»»åŠ¡ã€‚

## é¡¹ç›®ç»“æ„æ¦‚è§ˆ

Transformersåº“çš„æ¨ç†åŠŸèƒ½ä¸»è¦åˆ†å¸ƒåœ¨ä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒæ¨¡å—ä¸­ï¼š

```mermaid
graph TB
subgraph "æ¨ç†æ ¸å¿ƒæ¨¡å—"
A[Pipelines API] --> B[è‡ªåŠ¨å¤„ç†å™¨]
C[ä½çº§æ¨¡å‹API] --> D[ç›´æ¥æ¨¡å‹è°ƒç”¨]
E[ç”Ÿæˆç³»ç»Ÿ] --> F[æ–‡æœ¬ç”Ÿæˆ]
G[é‡åŒ–ç³»ç»Ÿ] --> H[æ¨¡å‹ä¼˜åŒ–]
I[è¿ç»­æ‰¹å¤„ç†] --> J[å®æ—¶æ¨ç†]
end
subgraph "æ”¯æŒç»„ä»¶"
K[è®¾å¤‡ç®¡ç†] --> L[GPU/CPUåˆ†é…]
M[å†…å­˜ä¼˜åŒ–] --> N[ç¼“å­˜ç®¡ç†]
O[æ—¥å¿—ç³»ç»Ÿ] --> P[è°ƒè¯•ä¿¡æ¯]
end
A --> K
C --> M
E --> O
```

**å›¾è¡¨æ¥æº**
- [src/transformers/pipelines/__init__.py](file://src/transformers/pipelines/__init__.py#L1-L50)
- [src/transformers/pipelines/base.py](file://src/transformers/pipelines/base.py#L1-L50)

**ç« èŠ‚æ¥æº**
- [src/transformers/pipelines/__init__.py](file://src/transformers/pipelines/__init__.py#L1-L100)
- [src/transformers/pipelines/base.py](file://src/transformers/pipelines/base.py#L1-L100)

## Pipeline APIåŸºç¡€

Pipeline APIæ˜¯ä½¿ç”¨Transformersåº“è¿›è¡Œæ¨ç†æœ€ç®€å•çš„æ–¹å¼ï¼Œå®ƒå°è£…äº†å®Œæ•´çš„é¢„å¤„ç†ã€æ¨¡å‹æ¨ç†å’Œåå¤„ç†æµç¨‹ã€‚

### åŸºç¡€ä½¿ç”¨æ–¹æ³•

```mermaid
sequenceDiagram
participant User as ç”¨æˆ·ä»£ç 
participant Pipeline as Pipelineå®ä¾‹
participant Tokenizer as åˆ†è¯å™¨
participant Model as æ¨¡å‹
participant Processor as åå¤„ç†å™¨
User->>Pipeline : pipeline(task, model)
Pipeline->>Tokenizer : åŠ è½½åˆ†è¯å™¨
Pipeline->>Model : åŠ è½½æ¨¡å‹
User->>Pipeline : __call__(inputs)
Pipeline->>Tokenizer : ç¼–ç è¾“å…¥
Pipeline->>Model : å‰å‘ä¼ æ’­
Model-->>Pipeline : æ¨¡å‹è¾“å‡º
Pipeline->>Processor : åå¤„ç†
Processor-->>User : æœ€ç»ˆç»“æœ
```

**å›¾è¡¨æ¥æº**
- [src/transformers/pipelines/base.py](file://src/transformers/pipelines/base.py#L1141-L1164)
- [src/transformers/pipelines/__init__.py](file://src/transformers/pipelines/__init__.py#L600-L700)

### æ”¯æŒçš„ä»»åŠ¡ç±»å‹

Transformersåº“æ”¯æŒå¤šç§ä»»åŠ¡ç±»å‹çš„Pipelineï¼š

| ä»»åŠ¡ç±»å‹ | æè¿° | ç¤ºä¾‹ |
|---------|------|------|
| æ–‡æœ¬åˆ†ç±» | å°†æ–‡æœ¬åˆ†ç±»åˆ°é¢„å®šä¹‰ç±»åˆ« | sentiment-analysis, text-classification |
| é—®ç­”ç³»ç»Ÿ | ä»ä¸Šä¸‹æ–‡ä¸­æå–ç­”æ¡ˆ | question-answering |
| æ–‡æœ¬ç”Ÿæˆ | ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬åºåˆ— | text-generation, summarization |
| å¡«å……æ©ç  | é¢„æµ‹è¢«æ©ç çš„token | fill-mask |
| å‘½åå®ä½“è¯†åˆ« | æ ‡æ³¨æ–‡æœ¬ä¸­çš„å®ä½“ | token-classification, ner |
| å›¾åƒåˆ†ç±» | å¯¹å›¾åƒè¿›è¡Œåˆ†ç±» | image-classification |

### å¸¸è§ä»»åŠ¡ç¤ºä¾‹

#### æ–‡æœ¬åˆ†ç±»
```python
from transformers import pipeline

# æƒ…æ„Ÿåˆ†æ
classifier = pipeline("sentiment-analysis")
result = classifier("æˆ‘å¾ˆå–œæ¬¢è¿™ä¸ªäº§å“ï¼")
# è¾“å‡º: [{'label': 'POSITIVE', 'score': 0.9997}]
```

#### é—®ç­”ç³»ç»Ÿ
```python
# é—®é¢˜å›ç­”
qa_pipeline = pipeline("question-answering")
result = qa_pipeline({
    "question": "Transformersåº“çš„ä¸»è¦ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ",
    "context": "Transformersåº“æ˜¯ä¸€ä¸ªå¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…..."
})
# è¾“å‡º: {'score': 0.95, 'start': 10, 'end': 30, 'answer': 'è‡ªç„¶è¯­è¨€å¤„ç†'}
```

#### æ–‡æœ¬ç”Ÿæˆ
```python
# æ–‡æœ¬ç”Ÿæˆ
generator = pipeline("text-generation")
result = generator("æœªæ¥çš„äººå·¥æ™ºèƒ½ä¼šå¦‚ä½•å‘å±•ï¼Ÿ", max_length=50)
# è¾“å‡º: ['æœªæ¥çš„äººå·¥æ™ºèƒ½å°†åœ¨å¤šä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨...']
```

**ç« èŠ‚æ¥æº**
- [src/transformers/pipelines/__init__.py](file://src/transformers/pipelines/__init__.py#L364-L396)
- [src/transformers/pipelines/text_classification.py](file://src/transformers/pipelines/text_classification.py#L1-L50)

## ä½çº§APIè¯¦è§£

ä½çº§APIå…è®¸ç”¨æˆ·ç›´æ¥æ§åˆ¶æ¨¡å‹çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼Œæä¾›æ›´å¤§çš„çµæ´»æ€§å’Œç²¾ç¡®æ§åˆ¶ã€‚

### ç›´æ¥æ¨¡å‹è°ƒç”¨

```mermaid
flowchart TD
A[è¾“å…¥æ•°æ®] --> B[æ‰‹åŠ¨é¢„å¤„ç†]
B --> C[æ¨¡å‹å‰å‘ä¼ æ’­]
C --> D[åå¤„ç†]
D --> E[æœ€ç»ˆè¾“å‡º]
F[è‡ªå®šä¹‰å‚æ•°] --> C
G[è®¾å¤‡é…ç½®] --> C
H[æ‰¹å¤„ç†è®¾ç½®] --> C
```

**å›¾è¡¨æ¥æº**
- [src/transformers/pipelines/base.py](file://src/transformers/pipelines/base.py#L1141-L1164)

### æ¨¡å‹åŠ è½½ä¸é…ç½®

ä½çº§APIçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå¯¹æ¨¡å‹åŠ è½½è¿‡ç¨‹çš„å®Œå…¨æ§åˆ¶ï¼š

```python
# è‡ªå®šä¹‰æ¨¡å‹åŠ è½½
from transformers import AutoModel, AutoTokenizer, AutoConfig

# åŠ è½½é…ç½®
config = AutoConfig.from_pretrained("bert-base-uncased")
config.output_attentions = True

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModel.from_pretrained(
    "bert-base-uncased",
    config=config,
    device_map="auto",
    torch_dtype=torch.float16
)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

### è¾“å…¥è¾“å‡ºå¤„ç†

ä½çº§APIéœ€è¦æ‰‹åŠ¨å¤„ç†è¾“å…¥ç¼–ç å’Œè¾“å‡ºè§£ç ï¼š

```python
# æ‰‹åŠ¨å¤„ç†è¾“å…¥
inputs = tokenizer("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­", return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

# å¤„ç†è¾“å‡º
last_hidden_state = outputs.last_hidden_state
pooler_output = outputs.pooler_output
```

**ç« èŠ‚æ¥æº**
- [src/transformers/models/auto/modeling_auto.py](file://src/transformers/models/auto/modeling_auto.py#L1-L200)
- [examples/pytorch/text-generation/run_generation.py](file://examples/pytorch/text-generation/run_generation.py#L1-L100)

## æ‰¹å¤„ç†ä¸åºåˆ—ç®¡ç†

é«˜æ•ˆçš„æ‰¹å¤„ç†æ˜¯æå‡æ¨ç†æ€§èƒ½çš„å…³é”®æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶ã€‚

### æ‰¹å¤„ç†ç­–ç•¥

```mermaid
graph LR
subgraph "æ‰¹å¤„ç†æµç¨‹"
A[è¾“å…¥é˜Ÿåˆ—] --> B[åºåˆ—é•¿åº¦æ’åº]
B --> C[å¡«å……å¯¹é½]
C --> D[æ‰¹é‡æ¨ç†]
D --> E[ç»“æœè§£ç ]
end
subgraph "ä¼˜åŒ–æŠ€æœ¯"
F[åŠ¨æ€æ‰¹å¤„ç†] --> G[æ³¨æ„åŠ›ä¼˜åŒ–]
G --> H[å†…å­˜ç®¡ç†]
end
D --> F
```

**å›¾è¡¨æ¥æº**
- [src/transformers/pipelines/base.py](file://src/transformers/pipelines/base.py#L80-L150)

### åºåˆ—é•¿åº¦ç®¡ç†

æœ‰æ•ˆçš„åºåˆ—é•¿åº¦ç®¡ç†å¯ä»¥æ˜¾è‘—æå‡å†…å­˜åˆ©ç”¨ç‡å’Œæ¨ç†é€Ÿåº¦ï¼š

```python
# åŠ¨æ€æ‰¹å¤„ç†ç¤ºä¾‹
def dynamic_batching(inputs, max_tokens=2048):
    # æŒ‰åºåˆ—é•¿åº¦æ’åº
    sorted_inputs = sorted(inputs, key=lambda x: len(x), reverse=True)
    
    batches = []
    current_batch = []
    current_tokens = 0
    
    for input_text in sorted_inputs:
        input_tokens = len(tokenizer(input_text)['input_ids'])
        
        if current_tokens + input_tokens > max_tokens:
            batches.append(current_batch)
            current_batch = [input_text]
            current_tokens = input_tokens
        else:
            current_batch.append(input_text)
            current_tokens += input_tokens
    
    if current_batch:
        batches.append(current_batch)
    
    return batches
```

### å†…å­˜ä¼˜åŒ–æŠ€æœ¯

| æŠ€æœ¯ | æè¿° | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| æ¢¯åº¦æ£€æŸ¥ç‚¹ | é‡æ–°è®¡ç®—ä¸­é—´æ¿€æ´»å€¼ | å†…å­˜å—é™ç¯å¢ƒ |
| æ··åˆç²¾åº¦ | ä½¿ç”¨FP16å‡å°‘å†…å­˜å ç”¨ | GPUæ¨ç† |
| æ¨¡å‹åˆ†ç‰‡ | å°†å¤§æ¨¡å‹åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ | è¶…å¤§æ¨¡å‹ |
| KVç¼“å­˜ä¼˜åŒ– | ä¼˜åŒ–é”®å€¼å¯¹å­˜å‚¨ | ç”Ÿæˆä»»åŠ¡ |

**ç« èŠ‚æ¥æº**
- [src/transformers/pipelines/base.py](file://src/transformers/pipelines/base.py#L80-L200)

## å®æ—¶æ¨ç†ä¸è¿ç»­æ‰¹å¤„ç†

è¿ç»­æ‰¹å¤„ç†æ˜¯å¤„ç†å®æ—¶è¯·æ±‚çš„é«˜çº§æŠ€æœ¯ï¼Œç‰¹åˆ«é€‚ç”¨äºèŠå¤©æœºå™¨äººå’Œäº¤äº’å¼åº”ç”¨ã€‚

### è¿ç»­æ‰¹å¤„ç†æ¶æ„

```mermaid
sequenceDiagram
participant Client as å®¢æˆ·ç«¯
participant Manager as æ‰¹å¤„ç†ç®¡ç†å™¨
participant Scheduler as è°ƒåº¦å™¨
participant Model as æ¨¡å‹
Client->>Manager : æäº¤è¯·æ±‚
Manager->>Scheduler : æ·»åŠ åˆ°é˜Ÿåˆ—
Scheduler->>Manager : è°ƒåº¦æ‰¹æ¬¡
Manager->>Model : æ‰¹é‡æ¨ç†
Model-->>Manager : è¿”å›ç»“æœ
Manager-->>Client : æµå¼è¿”å›
```

**å›¾è¡¨æ¥æº**
- [src/transformers/generation/continuous_batching/continuous_api.py](file://src/transformers/generation/continuous_batching/continuous_api.py#L697-L729)

### å®æ—¶ç”Ÿæˆå®ç°

è¿ç»­æ‰¹å¤„ç†çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ï¼š

```python
# è¿ç»­æ‰¹å¤„ç†ç¤ºä¾‹
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.continuous_batching import ContinuousBatchingManager

# åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForCausalLM.from_pretrained("gpt2", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# åˆ›å»ºè¿ç»­æ‰¹å¤„ç†ç®¡ç†å™¨
manager = model.init_continuous_batching()
manager.start()

# å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚
requests = [
    "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†ã€‚",
    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ã€‚",
    "å¦‚ä½•åˆ¶ä½œæŠ«è¨ï¼Ÿ"
]

request_ids = []
for prompt in requests:
    inputs = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
    request_id = manager.add_request(inputs, max_new_tokens=100, streaming=True)
    request_ids.append(request_id)

# è·å–ç»“æœ
results = []
for request_id in request_ids:
    for chunk in manager.request_id_iter(request_id):
        results.append(chunk)
```

### æµå¼ç”Ÿæˆ

æµå¼ç”Ÿæˆå…è®¸åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¿”å›ç»“æœï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼š

```python
# æµå¼ç”Ÿæˆç¤ºä¾‹
def stream_generation(model, tokenizer, prompt, max_tokens=100):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    for token in model.generate(
        inputs.input_ids,
        max_new_tokens=max_tokens,
        pad_token_id=tokenizer.eos_token_id,
        streamer=TextStreamer(tokenizer)
    ):
        yield tokenizer.decode(token, skip_special_tokens=True)
```

**ç« èŠ‚æ¥æº**
- [src/transformers/generation/continuous_batching/continuous_api.py](file://src/transformers/generation/continuous_batching/continuous_api.py#L408-L436)
- [examples/pytorch/continuous_batching.py](file://examples/pytorch/continuous_batching.py#L38-L63)

## æ¨¡å‹é‡åŒ–ä¸ä¼˜åŒ–

æ¨¡å‹é‡åŒ–æ˜¯å‡å°‘æ¨¡å‹å¤§å°å’Œæå‡æ¨ç†é€Ÿåº¦çš„é‡è¦æŠ€æœ¯ï¼Œç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒã€‚

### é‡åŒ–æŠ€æœ¯æ¦‚è§ˆ

```mermaid
graph TD
A[åŸå§‹æ¨¡å‹] --> B{é‡åŒ–æ–¹æ³•}
B --> C[INT8é‡åŒ–]
B --> D[INT4é‡åŒ–]
B --> E[FP16é‡åŒ–]
B --> F[æ··åˆç²¾åº¦]
C --> G[BitsAndBytes]
D --> H[AWQ/QLoRA]
E --> I[Optimum]
F --> J[åŠ¨æ€é‡åŒ–]
G --> K[æ¨ç†åŠ é€Ÿ]
H --> K
I --> K
J --> K
```

**å›¾è¡¨æ¥æº**
- [src/transformers/quantizers/__init__.py](file://src/transformers/quantizers/__init__.py#L1-L17)

### å¸¸ç”¨é‡åŒ–æ–¹æ³•

| æ–¹æ³• | ç²¾åº¦ | å†…å­˜èŠ‚çœ | æ€§èƒ½å½±å“ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|----------|----------|
| INT8 | 8ä½æ•´æ•° | 50% | è½»å¾® | ç§»åŠ¨è®¾å¤‡ |
| INT4 | 4ä½æ•´æ•° | 75% | ä¸­ç­‰ | è¾¹ç¼˜è®¡ç®— |
| FP16 | åŠç²¾åº¦æµ®ç‚¹ | 50% | æå° | GPUæ¨ç† |
| åŠ¨æ€é‡åŒ– | å˜åŒ– | 25-50% | å¾ˆå° | å®æ—¶åº”ç”¨ |

### é‡åŒ–å®ç°ç¤ºä¾‹

```python
# INT8é‡åŒ–
from transformers import BitsAndBytesConfig, AutoModelForCausalLM

# é…ç½®INT8é‡åŒ–
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_skip_modules=["lm_head"]
)

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/DialoGPT-medium",
    quantization_config=quantization_config,
    device_map="auto"
)

# INT4é‡åŒ–ï¼ˆæ›´æ¿€è¿›ï¼‰
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "microsoft/DialoGPT-medium",
    quantization_config=quantization_config,
    device_map="auto"
)
```

### è‡ªå®šä¹‰é‡åŒ–

å¯¹äºç‰¹å®šéœ€æ±‚ï¼Œå¯ä»¥å®ç°è‡ªå®šä¹‰é‡åŒ–æ–¹æ¡ˆï¼š

```python
# è‡ªå®šä¹‰é‡åŒ–ç¤ºä¾‹
class CustomQuantizer:
    def __init__(self, bits=4):
        self.bits = bits
        self.scale = 2 ** (bits - 1)
    
    def quantize(self, tensor):
        # é‡åŒ–é€»è¾‘
        quantized = torch.round(tensor * self.scale).clamp(-self.scale, self.scale - 1)
        return quantized
    
    def dequantize(self, quantized_tensor):
        # åé‡åŒ–
        return quantized_tensor / self.scale
```

**ç« èŠ‚æ¥æº**
- [src/transformers/quantizers/__init__.py](file://src/transformers/quantizers/__init__.py#L1-L17)
- [examples/quantization/custom_quantization_int8_example.py](file://examples/quantization/custom_quantization_int8_example.py#L171-L197)

## è®¾å¤‡æ˜ å°„ä¸åˆ†å¸ƒå¼æ¨ç†

å¤§å‹æ¨¡å‹çš„æ¨ç†é€šå¸¸éœ€è¦è·¨å¤šä¸ªè®¾å¤‡è¿›è¡Œåˆ†å¸ƒå¼çš„è®¡ç®—å’Œå­˜å‚¨ç®¡ç†ã€‚

### è®¾å¤‡æ˜ å°„ç­–ç•¥

```mermaid
graph TB
subgraph "è®¾å¤‡æ˜ å°„æ¶æ„"
A[ä¸»CPU] --> B[GPU 0]
A --> C[GPU 1]
A --> D[GPU 2]
A --> E[GPU 3]
B --> F[æ¨¡å‹å±‚1-4]
C --> G[æ¨¡å‹å±‚5-8]
D --> H[æ¨¡å‹å±‚9-12]
E --> I[æ¨¡å‹å±‚13-16]
end
subgraph "å­˜å‚¨é€‰é¡¹"
J[å†…å­˜] --> K[GPUæ˜¾å­˜]
L[ç¡¬ç›˜] --> M[ç£ç›˜å­˜å‚¨]
N[CPU] --> O[ä¸»æœºå†…å­˜]
end
F --> J
G --> L
H --> N
I --> L
```

**å›¾è¡¨æ¥æº**
- [src/transformers/integrations/accelerate.py](file://src/transformers/integrations/accelerate.py#L437-L481)

### è‡ªåŠ¨è®¾å¤‡æ˜ å°„

```python
# è‡ªåŠ¨è®¾å¤‡æ˜ å°„
from transformers import AutoModelForCausalLM

# ä½¿ç”¨device_map="auto"è®©Accelerateè‡ªåŠ¨åˆ†é…
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/DialoGPT-large",
    device_map="auto",
    torch_dtype=torch.float16,
    # max_memoryå‚æ•°å¯æŒ‡å®šæ¯ä¸ªè®¾å¤‡çš„æœ€å¤§å†…å­˜
    max_memory={
        0: "10GB",  # GPU 0
        1: "10GB",  # GPU 1
        "cpu": "20GB"  # CPU
    }
)
```

### æ‰‹åŠ¨è®¾å¤‡æ˜ å°„

å¯¹äºæ›´ç²¾ç»†çš„æ§åˆ¶ï¼Œå¯ä»¥æ‰‹åŠ¨æŒ‡å®šè®¾å¤‡æ˜ å°„ï¼š

```python
# æ‰‹åŠ¨è®¾å¤‡æ˜ å°„
device_map = {
    "transformer.wte": 0,  # è¯åµŒå…¥å±‚æ”¾åœ¨GPU 0
    "transformer.wpe": 0,  # ä½ç½®åµŒå…¥å±‚æ”¾åœ¨GPU 0
    "transformer.h.0": 0,  # ç¬¬ä¸€å±‚æ”¾åœ¨GPU 0
    "transformer.h.1": 0,
    "transformer.h.2": 1,  # ç¬¬ä¸‰å±‚æ”¾åœ¨GPU 1
    "transformer.h.3": 1,
    "transformer.ln_f": 1,  # æœ€åä¸€å±‚æ”¾åœ¨GPU 1
    "lm_head": 1,          # è¯­è¨€æ¨¡å‹å¤´æ”¾åœ¨GPU 1
}

model = AutoModelForCausalLM.from_pretrained(
    "gpt2-large",
    device_map=device_map,
    torch_dtype=torch.float16
)
```

### åˆ†å¸ƒå¼æ¨ç†ä¼˜åŒ–

| ä¼˜åŒ–æŠ€æœ¯ | æè¿° | æ•ˆæœ |
|----------|------|------|
| æ¢¯åº¦ç´¯ç§¯ | åœ¨å¤šä¸ªè®¾å¤‡ä¸Šç´¯ç§¯æ¢¯åº¦ | æ”¯æŒæ›´å¤§æ‰¹æ¬¡ |
| å¼ é‡å¹¶è¡Œ | å°†å¼ é‡åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ | å‡å°‘å•è®¾å¤‡å†…å­˜ |
| æµæ°´çº¿å¹¶è¡Œ | å°†æ¨¡å‹å±‚åˆ†å¸ƒåˆ°ä¸åŒè®¾å¤‡ | æå‡ååé‡ |
| æ¿€æ´»æ£€æŸ¥ç‚¹ | é‡è®¡ç®—æ¿€æ´»å€¼ | å‡å°‘å†…å­˜ä½¿ç”¨ |

**ç« èŠ‚æ¥æº**
- [src/transformers/integrations/accelerate.py](file://src/transformers/integrations/accelerate.py#L325-L343)
- [src/transformers/integrations/peft.py](file://src/transformers/integrations/peft.py#L519-L536)

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

é«˜æ•ˆçš„æ¨ç†ä¸ä»…ä¾èµ–äºç¡¬ä»¶å’Œç®—æ³•ï¼Œè¿˜éœ€è¦åˆç†çš„è½¯ä»¶ä¼˜åŒ–ç­–ç•¥ã€‚

### æ¨ç†æ€§èƒ½ä¼˜åŒ–å±‚æ¬¡

```mermaid
graph TD
A[åº”ç”¨å±‚ä¼˜åŒ–] --> B[æ¡†æ¶å±‚ä¼˜åŒ–]
B --> C[æ¨¡å‹å±‚ä¼˜åŒ–]
C --> D[ç¡¬ä»¶å±‚ä¼˜åŒ–]
A --> A1[æ‰¹å¤„ç†ç­–ç•¥]
A --> A2[ç¼“å­˜æœºåˆ¶]
A --> A3[å¼‚æ­¥å¤„ç†]
B --> B1[ç®—å­èåˆ]
B --> B2[å†…å­˜æ± ]
B --> B3[å›¾ä¼˜åŒ–]
C --> C1[æ¨¡å‹å‰ªæ]
C --> C2[çŸ¥è¯†è’¸é¦]
C --> C3[æƒé‡å…±äº«]
D --> D1[GPUåŠ é€Ÿ]
D --> D2[ä¸“ç”¨èŠ¯ç‰‡]
D --> D3[å†…å­˜å¸¦å®½]
```

### å…³é”®ä¼˜åŒ–æŠ€æœ¯

#### 1. æ‰¹å¤„ç†ä¼˜åŒ–
```python
# é«˜æ•ˆæ‰¹å¤„ç†å®ç°
def efficient_batch_inference(model, tokenizer, texts, batch_size=8):
    model.eval()
    all_outputs = []
    
    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            
            # æ‰¹é‡ç¼–ç 
            inputs = tokenizer(
                batch_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            # å‰å‘ä¼ æ’­
            outputs = model(**inputs)
            all_outputs.extend(outputs.logits)
    
    return all_outputs
```

#### 2. ç¼“å­˜ä¼˜åŒ–
```python
# KVç¼“å­˜ä¼˜åŒ–
class OptimizedGeneration:
    def __init__(self, model):
        self.model = model
        self.cache = {}
    
    def generate_with_cache(self, input_ids, **kwargs):
        # æ£€æŸ¥ç¼“å­˜
        cache_key = tuple(input_ids.flatten().tolist())
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # ç”Ÿæˆæ–°ç»“æœ
        output = self.model.generate(input_ids, **kwargs)
        
        # å­˜å‚¨åˆ°ç¼“å­˜
        self.cache[cache_key] = output
        
        return output
```

#### 3. å†…å­˜ç®¡ç†
```python
# å†…å­˜ä¼˜åŒ–ç­–ç•¥
import gc
import torch

def memory_efficient_inference(model, dataloader, max_memory=0.8):
    model.eval()
    torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
    
    with torch.no_grad():
        for batch in dataloader:
            # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.memory_allocated() > torch.cuda.max_memory_allocated() * max_memory:
                torch.cuda.empty_cache()
                gc.collect()
            
            # å¤„ç†æ‰¹æ¬¡
            outputs = model(**batch)
            
            # å¤„ç†è¾“å‡º
            yield outputs
```

### æ€§èƒ½ç›‘æ§

```python
# æ€§èƒ½ç›‘æ§å·¥å…·
import time
import psutil
import torch

class InferenceProfiler:
    def __init__(self):
        self.metrics = {
            'inference_time': [],
            'memory_usage': [],
            'throughput': []
        }
    
    def profile_inference(self, model, inputs):
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # è®°å½•å¼€å§‹å†…å­˜
        start_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else psutil.virtual_memory().used
        
        # æ‰§è¡Œæ¨ç†
        with torch.no_grad():
            outputs = model(**inputs)
        
        # è®¡ç®—è€—æ—¶
        inference_time = time.time() - start_time
        
        # è®°å½•ç»“æŸå†…å­˜
        end_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else psutil.virtual_memory().used
        
        # è®°å½•æŒ‡æ ‡
        self.metrics['inference_time'].append(inference_time)
        self.metrics['memory_usage'].append(end_memory - start_memory)
        
        return outputs
```

**ç« èŠ‚æ¥æº**
- [src/transformers/utils/logging.py](file://src/transformers/utils/logging.py#L1-L100)

## æ•…éšœæ’é™¤æŒ‡å—

åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œæ¨ç†ç³»ç»Ÿå¯èƒ½é‡åˆ°å„ç§é—®é¢˜ã€‚æœ¬èŠ‚æä¾›å¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆã€‚

### å¸¸è§é”™è¯¯ç±»å‹

```mermaid
graph TB
A[æ¨ç†é”™è¯¯] --> B[å†…å­˜ç›¸å…³]
A --> C[è®¾å¤‡ç›¸å…³]
A --> D[æ¨¡å‹ç›¸å…³]
A --> E[é…ç½®ç›¸å…³]
B --> B1[OOMé”™è¯¯]
B --> B2[å†…å­˜æ³„æ¼]
B --> B3[å†…å­˜ç¢ç‰‡]
C --> C1[GPUä¸å¯ç”¨]
C --> C2[è®¾å¤‡ä¸åŒ¹é…]
C --> C3[é©±åŠ¨é—®é¢˜]
D --> D1[æ¨¡å‹åŠ è½½å¤±è´¥]
D --> D2[æƒé‡ä¸åŒ¹é…]
D --> D3[æ¶æ„é”™è¯¯]
E --> E1[é…ç½®æ— æ•ˆ]
E --> E2[å‚æ•°å†²çª]
E --> E3[ç‰ˆæœ¬ä¸å…¼å®¹]
```

### é”™è¯¯è¯Šæ–­æµç¨‹

#### 1. å†…å­˜é—®é¢˜è¯Šæ–­
```python
# å†…å­˜é—®é¢˜è¯Šæ–­è„šæœ¬
def diagnose_memory_issues():
    print("=== å†…å­˜è¯Šæ–­æŠ¥å‘Š ===")
    
    # æ£€æŸ¥GPUå†…å­˜
    if torch.cuda.is_available():
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
        
        # æ£€æŸ¥å½“å‰GPUå†…å­˜
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        print(f"å·²åˆ†é…å†…å­˜: {allocated:.2f} GB")
        print(f"ç¼“å­˜å†…å­˜: {cached:.2f} GB")
        
        # æ£€æŸ¥æœ€å¤§å†…å­˜ä½¿ç”¨
        max_allocated = torch.cuda.max_memory_allocated() / 1024**3
        print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {max_allocated:.2f} GB")
    
    # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
    if hasattr(psutil, 'virtual_memory'):
        vm = psutil.virtual_memory()
        print(f"ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡: {vm.percent}%")
        print(f"å¯ç”¨å†…å­˜: {vm.available / 1024**3:.2f} GB")
```

#### 2. è®¾å¤‡æ˜ å°„é—®é¢˜
```python
# è®¾å¤‡æ˜ å°„è¯Šæ–­
def diagnose_device_map(model, device_map):
    print("=== è®¾å¤‡æ˜ å°„è¯Šæ–­ ===")
    
    # æ£€æŸ¥è®¾å¤‡æ˜ å°„å®Œæ•´æ€§
    model_layers = [name for name, _ in model.named_modules()]
    mapped_layers = []
    
    for device, layers in device_map.items():
        if isinstance(layers, str):
            mapped_layers.append(layers)
        else:
            mapped_layers.extend(layers)
    
    unmapped_layers = set(model_layers) - set(mapped_layers)
    if unmapped_layers:
        print(f"è­¦å‘Š: ä»¥ä¸‹å±‚æœªæ˜ å°„åˆ°ä»»ä½•è®¾å¤‡: {unmapped_layers}")
    
    # æ£€æŸ¥è®¾å¤‡å…¼å®¹æ€§
    for device, layers in device_map.items():
        if device != "cpu" and not torch.cuda.is_available():
            print(f"é”™è¯¯: è®¾å¤‡ {device} ä¸å¯ç”¨")
```

#### 3. æ¨¡å‹åŠ è½½é—®é¢˜
```python
# æ¨¡å‹åŠ è½½è¯Šæ–­
def diagnose_model_loading(model_name, device_map=None):
    print(f"=== æ¨¡å‹åŠ è½½è¯Šæ–­: {model_name} ===")
    
    try:
        # å°è¯•åŠ è½½é…ç½®
        config = AutoConfig.from_pretrained(model_name)
        print(f"âœ“ é…ç½®åŠ è½½æˆåŠŸ: {config.model_type}")
        
        # æ£€æŸ¥æ¨¡å‹æ¶æ„
        if hasattr(config, 'architectures') and config.architectures:
            print(f"âœ“ æ¨¡å‹æ¶æ„: {config.architectures}")
        
        # æ£€æŸ¥è®¾å¤‡æ˜ å°„
        if device_map:
            print(f"âœ“ è®¾å¤‡æ˜ å°„: {device_map}")
        
        # å°è¯•åŠ è½½æ¨¡å‹
        model = AutoModel.from_pretrained(model_name, device_map=device_map)
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False
```

### æ€§èƒ½é—®é¢˜è§£å†³

#### 1. æ¨ç†é€Ÿåº¦æ…¢
```python
# æ¨ç†é€Ÿåº¦ä¼˜åŒ–æ£€æŸ¥æ¸…å•
def optimize_inference_speed():
    checks = [
        "æ˜¯å¦ä½¿ç”¨äº†åˆé€‚çš„è®¾å¤‡ç±»å‹ (GPU vs CPU)",
        "æ˜¯å¦å¯ç”¨äº†æ··åˆç²¾åº¦ (FP16/BF16)",
        "æ˜¯å¦ä½¿ç”¨äº†é€‚å½“çš„æ‰¹å¤„ç†å¤§å°",
        "æ˜¯å¦å¯ç”¨äº†æ¨¡å‹é‡åŒ–",
        "æ˜¯å¦ä½¿ç”¨äº†ä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶",
        "æ˜¯å¦ç¦ç”¨äº†ä¸å¿…è¦çš„æ¢¯åº¦è®¡ç®—",
        "æ˜¯å¦ä½¿ç”¨äº†ç¼–è¯‘ä¼˜åŒ– (TorchScript/JIT)"
    ]
    
    for check in checks:
        print(f"âœ“ {check}")
```

#### 2. å†…å­˜ä½¿ç”¨è¿‡é«˜
```python
# å†…å­˜ä¼˜åŒ–ç­–ç•¥
def memory_optimization_strategy():
    strategies = {
        "é‡åŒ–": "ä½¿ç”¨INT8/INT4é‡åŒ–å‡å°‘å†…å­˜å ç”¨",
        "åˆ†ç‰‡": "ä½¿ç”¨device_map='auto'è‡ªåŠ¨åˆ†ç‰‡",
        "å¸è½½": "å¯ç”¨CPUå¸è½½å’Œç£ç›˜å¸è½½",
        "æ‰¹å¤„ç†": "è°ƒæ•´æ‰¹å¤„ç†å¤§å°å¹³è¡¡é€Ÿåº¦å’Œå†…å­˜",
        "ç¼“å­˜": "ä¼˜åŒ–KVç¼“å­˜ç­–ç•¥"
    }
    
    for technique, description in strategies.items():
        print(f"ğŸ’¡ {technique}: {description}")
```

### æ—¥å¿—å’Œç›‘æ§

```python
# æ¨ç†ç³»ç»Ÿç›‘æ§
import logging
from datetime import datetime

class InferenceMonitor:
    def __init__(self):
        self.logger = logging.getLogger('inference_monitor')
        self.metrics = {}
    
    def log_inference(self, model_name, input_size, inference_time, memory_used):
        timestamp = datetime.now().isoformat()
        self.logger.info(f"{timestamp} | {model_name} | "
                        f"Size: {input_size} | "
                        f"Time: {inference_time:.3f}s | "
                        f"Memory: {memory_used:.2f}GB")
    
    def get_performance_summary(self):
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        return {
            'average_inference_time': sum(self.metrics['times']) / len(self.metrics['times']),
            'peak_memory_usage': max(self.metrics['memory']),
            'total_requests': len(self.metrics['times'])
        }
```

**ç« èŠ‚æ¥æº**
- [src/transformers/utils/logging.py](file://src/transformers/utils/logging.py#L300-L408)

## æœ€ä½³å®è·µ

åŸºäºå®é™…éƒ¨ç½²ç»éªŒï¼Œä»¥ä¸‹æ˜¯ä½¿ç”¨Transformersåº“è¿›è¡Œæ¨ç†çš„æœ€ä½³å®è·µå»ºè®®ã€‚

### å¼€å‘é˜¶æ®µæœ€ä½³å®è·µ

#### 1. æ¨¡å‹é€‰æ‹©å’ŒéªŒè¯
```python
# æ¨¡å‹è¯„ä¼°æ¨¡æ¿
def evaluate_model_performance(model, test_data):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„ç»¼åˆæ¨¡æ¿
    """
    results = {
        'accuracy': [],
        'inference_time': [],
        'memory_usage': [],
        'throughput': []
    }
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    for batch in test_data:
        # æ—¶é—´æµ‹é‡
        start_time = time.time()
        
        # å†…å­˜ç›‘æ§
        start_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        
        # æ¨ç†æ‰§è¡Œ
        with torch.no_grad():
            outputs = model(**batch)
        
        # æ€§èƒ½è®°å½•
        inference_time = time.time() - start_time
        end_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        
        results['inference_time'].append(inference_time)
        results['memory_usage'].append(end_memory - start_memory)
    
    return results
```

#### 2. é…ç½®ç®¡ç†
```python
# æ¨ç†é…ç½®ç®¡ç†
class InferenceConfig:
    def __init__(self):
        self.defaults = {
            'batch_size': 8,
            'max_length': 512,
            'temperature': 0.7,
            'top_p': 0.9,
            'device': 'auto',
            'dtype': 'auto',
            'quantization': None,
            'cache_dir': './cache'
        }
    
    def load_config(self, config_path):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                custom_config = json.load(f)
            self.defaults.update(custom_config)
    
    def get_optimal_settings(self, model_size, hardware_specs):
        """æ ¹æ®ç¡¬ä»¶è§„æ ¼ä¼˜åŒ–é…ç½®"""
        settings = self.defaults.copy()
        
        # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´æ‰¹å¤„ç†
        if model_size > 10:  # 10B+ å‚æ•°
            settings['batch_size'] = 1
        elif model_size > 1:  # 1B-10B å‚æ•°
            settings['batch_size'] = 4
        else:  # å°æ¨¡å‹
            settings['batch_size'] = 16
        
        # æ ¹æ®GPUå†…å­˜è°ƒæ•´
        if hardware_specs.get('gpu_memory_gb', 0) < 16:
            settings['quantization'] = 'int8'
            settings['batch_size'] = max(1, settings['batch_size'] // 2)
        
        return settings
```

### ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ

#### 1. å®¹é”™å’Œæ¢å¤
```python
# å®¹é”™æœºåˆ¶
class RobustInferenceSystem:
    def __init__(self, primary_model, backup_model=None):
        self.primary_model = primary_model
        self.backup_model = backup_model
        self.fallback_count = 0
        self.max_fallbacks = 3
    
    def robust_inference(self, inputs, **kwargs):
        try:
            # ä¸»æ¨¡å‹æ¨ç†
            return self.primary_model(**inputs, **kwargs)
        except torch.cuda.OutOfMemoryError:
            logger.warning("ä¸»æ¨¡å‹å†…å­˜ä¸è¶³ï¼Œå°è¯•é™çº§é…ç½®")
            return self._fallback_inference(inputs, **kwargs)
        except Exception as e:
            logger.error(f"ä¸»æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
            return self._fallback_inference(inputs, **kwargs)
    
    def _fallback_inference(self, inputs, **kwargs):
        if self.backup_model and self.fallback_count < self.max_fallbacks:
            self.fallback_count += 1
            logger.info(f"åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ï¼Œé™çº§æ¬¡æ•°: {self.fallback_count}")
            return self.backup_model(**inputs, **kwargs)
        raise RuntimeError("æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨")
```

#### 2. è´Ÿè½½å‡è¡¡
```python
# è´Ÿè½½å‡è¡¡å™¨
class LoadBalancer:
    def __init__(self, models):
        self.models = models
        self.current_index = 0
    
    def get_next_model(self):
        model = self.models[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.models)
        return model
    
    def batch_inference(self, inputs, batch_size=8):
        """è´Ÿè½½å‡è¡¡çš„æ‰¹å¤„ç†æ¨ç†"""
        results = []
        
        for i in range(0, len(inputs), batch_size):
            batch = inputs[i:i+batch_size]
            model = self.get_next_model()
            
            # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
            with ThreadPoolExecutor() as executor:
                future = executor.submit(model, batch)
                batch_result = future.result()
                results.extend(batch_result)
        
        return results
```

### ç›‘æ§å’Œç»´æŠ¤

#### 1. æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿
```python
# å®æ—¶ç›‘æ§
class InferenceDashboard:
    def __init__(self):
        self.metrics = defaultdict(list)
    
    def track_metric(self, metric_name, value):
        self.metrics[metric_name].append({
            'timestamp': time.time(),
            'value': value
        })
    
    def get_performance_stats(self):
        stats = {}
        for metric, values in self.metrics.items():
            if values:
                values = [v['value'] for v in values]
                stats[metric] = {
                    'avg': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values),
                    'count': len(values)
                }
        return stats
    
    def export_metrics(self, filepath):
        """å¯¼å‡ºæŒ‡æ ‡åˆ°æ–‡ä»¶"""
        stats = self.get_performance_stats()
        with open(filepath, 'w') as f:
            json.dump(stats, f, indent=2)
```

#### 2. è‡ªåŠ¨åŒ–ç»´æŠ¤
```python
# è‡ªåŠ¨ç»´æŠ¤è„šæœ¬
def automated_maintenance():
    """è‡ªåŠ¨åŒ–ç»´æŠ¤ä»»åŠ¡"""
    maintenance_tasks = [
        cleanup_cache(),
        update_model_weights(),
        monitor_gpu_health(),
        optimize_memory_usage(),
        backup_configurations()
    ]
    
    for task in maintenance_tasks:
        try:
            task.execute()
        except Exception as e:
            logger.error(f"ç»´æŠ¤ä»»åŠ¡å¤±è´¥: {e}")
```

## æ€»ç»“

æœ¬æŒ‡å—å…¨é¢ä»‹ç»äº†ä½¿ç”¨Transformersåº“è¿›è¡Œæ¨¡å‹æ¨ç†çš„å„ç§æ–¹æ³•å’ŒæŠ€æœ¯ã€‚ä»ç®€å•çš„Pipeline APIåˆ°å¤æ‚çš„è¿ç»­æ‰¹å¤„ç†ç³»ç»Ÿï¼Œä»åŸºæœ¬çš„é‡åŒ–æŠ€æœ¯åˆ°é«˜çº§çš„åˆ†å¸ƒå¼æ¨ç†ï¼Œæˆ‘ä»¬æ¶µç›–äº†æ¨ç†ç³»ç»Ÿçš„å„ä¸ªæ–¹é¢ã€‚

### å…³é”®è¦ç‚¹å›é¡¾

1. **Pipeline API**ï¼šæœ€é€‚åˆå¿«é€ŸåŸå‹å¼€å‘å’Œç®€å•åº”ç”¨åœºæ™¯
2. **ä½çº§API**ï¼šæä¾›æœ€å¤§çš„çµæ´»æ€§å’Œæ§åˆ¶èƒ½åŠ›
3. **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼šæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡
4. **é‡åŒ–æŠ€æœ¯**ï¼šæœ‰æ•ˆå‡å°‘æ¨¡å‹å¤§å°å’Œæå‡æ¨ç†é€Ÿåº¦
5. **è®¾å¤‡æ˜ å°„**ï¼šæ”¯æŒå¤§è§„æ¨¡æ¨¡å‹çš„åˆ†å¸ƒå¼æ¨ç†
6. **è¿ç»­æ‰¹å¤„ç†**ï¼šå®ç°å®æ—¶å’Œé«˜å¹¶å‘çš„æ¨ç†æœåŠ¡
7. **æ€§èƒ½ç›‘æ§**ï¼šç¡®ä¿ç”Ÿäº§ç¯å¢ƒçš„ç¨³å®šæ€§å’Œå¯è§‚å¯Ÿæ€§

### é€‰æ‹©åˆé€‚çš„æŠ€æœ¯

| åœºæ™¯ | æ¨èæŠ€æœ¯ | è¯´æ˜ |
|------|----------|------|
| å¿«é€ŸåŸå‹ | Pipeline API | ç®€å•æ˜“ç”¨ï¼Œå¼€å‘é€Ÿåº¦å¿« |
| ç”Ÿäº§éƒ¨ç½² | ä½çº§API + é‡åŒ– | æ€§èƒ½æœ€ä¼˜ï¼Œèµ„æºåˆ©ç”¨é«˜æ•ˆ |
| å®æ—¶åº”ç”¨ | è¿ç»­æ‰¹å¤„ç† | æ”¯æŒé«˜å¹¶å‘å’Œä½å»¶è¿Ÿ |
| èµ„æºå—é™ | æ¨¡å‹é‡åŒ– | å‡å°‘å†…å­˜å’Œè®¡ç®—éœ€æ±‚ |
| å¤§è§„æ¨¡éƒ¨ç½² | åˆ†å¸ƒå¼æ¨ç† | åˆ©ç”¨å¤šè®¾å¤‡ååŒå·¥ä½œ |

### æœªæ¥å‘å±•

éšç€æ¨¡å‹è§„æ¨¡çš„ä¸æ–­å¢é•¿å’Œåº”ç”¨åœºæ™¯çš„å¤šæ ·åŒ–ï¼Œæ¨ç†æŠ€æœ¯ä¹Ÿåœ¨æŒç»­æ¼”è¿›ï¼š

- **æ›´æ™ºèƒ½çš„æ‰¹å¤„ç†**ï¼šåŸºäºé¢„æµ‹çš„åŠ¨æ€æ‰¹å¤„ç†è°ƒåº¦
- **è¾¹ç¼˜æ¨ç†ä¼˜åŒ–**ï¼šé’ˆå¯¹ç§»åŠ¨è®¾å¤‡å’ŒIoTè®¾å¤‡çš„ä¸“é—¨ä¼˜åŒ–
- **å¤šæ¨¡æ€æ¨ç†**ï¼šæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§è¾“å…¥çš„ç»Ÿä¸€æ¨ç†æ¡†æ¶
- **è‡ªé€‚åº”é‡åŒ–**ï¼šæ ¹æ®ç¡¬ä»¶ç‰¹æ€§å’Œä»»åŠ¡éœ€æ±‚åŠ¨æ€é€‰æ‹©é‡åŒ–ç­–ç•¥

é€šè¿‡æŒæ¡è¿™äº›æ¨ç†æŠ€æœ¯å’Œæœ€ä½³å®è·µï¼Œå¼€å‘è€…å¯ä»¥æ„å»ºé«˜æ•ˆã€å¯é ã€å¯æ‰©å±•çš„AIæ¨ç†ç³»ç»Ÿï¼Œä¸ºç”¨æˆ·æä¾›ä¼˜è´¨çš„AIæœåŠ¡ä½“éªŒã€‚