# 生成配置

<cite>
**本文档中引用的文件**   
- [configuration_utils.py](file://src/transformers/generation/configuration_utils.py)
- [run_generation.py](file://examples/pytorch/text-generation/run_generation.py)
- [generation.yaml](file://benchmark/config/generation.yaml)
- [test_configuration_utils.py](file://tests/generation/test_configuration_utils.py)
</cite>

## 目录
1. [简介](#简介)
2. [核心参数详解](#核心参数详解)
3. [配置设置方法](#配置设置方法)
4. [不同任务的推荐配置](#不同任务的推荐配置)
5. [参数相互影响与最佳实践](#参数相互影响与最佳实践)
6. [配置验证与错误处理](#配置验证与错误处理)
7. [配置示例](#配置示例)

## 简介
生成配置（GenerationConfig）是Hugging Face Transformers库中用于控制文本生成过程的核心类。它提供了一套完整的参数体系，用于精确控制生成文本的长度、风格、多样性和质量。通过合理配置这些参数，用户可以针对不同的应用场景（如文本生成、翻译、摘要等）优化生成结果。

**Section sources**
- [configuration_utils.py](file://src/transformers/generation/configuration_utils.py#L81-L1152)

## 核心参数详解

### 长度控制参数
这些参数用于控制生成文本的长度。

- **max_length** (`int`, 可选, 默认为20): 生成标记的最大长度。对应于输入提示的长度加上`max_new_tokens`。如果同时设置了`max_new_tokens`，则其效果会被覆盖。
- **max_new_tokens** (`int`, 可选): 要生成的最大标记数，忽略提示中的标记数。
- **min_length** (`int`, 可选, 默认为0): 要生成的序列的最小长度。对应于输入提示的长度加上`min_new_tokens`。如果同时设置了`min_new_tokens`，则其效果会被覆盖。
- **min_new_tokens** (`int`, 可选): 要生成的最小标记数，忽略提示中的标记数。

### 生成策略参数
这些参数决定了生成文本所采用的策略。

- **do_sample** (`bool`, 可选, 默认为`False`): 是否使用采样；否则使用贪婪解码。
- **num_beams** (`int`, 可选, 默认为1): 用于束搜索的束数。1表示不使用束搜索。

### 缓存参数
这些参数控制生成过程中的缓存使用。

- **use_cache** (`bool`, 可选, 默认为`True`): 模型是否应使用过去的最后键/值注意力（如果模型适用）来加速解码。
- **cache_implementation** (`str`, 可选, 默认为`None`): 在`generate`中实例化的缓存类的名称，用于更快的解码。

### 输出对数概率操作参数
这些参数用于操纵模型输出的对数概率。

- **temperature** (`float`, 可选, 默认为1.0): 用于调节下一个标记的概率值。值为1.0表示无调节。
- **top_k** (`int`, 可选, 默认为50): 用于top-k过滤的最高概率词汇标记的数量。
- **top_p** (`float`, 可选, 默认为1.0): 如果设置为小于1的浮点数，则仅保留累积概率达到或超过`top_p`的最可能标记的最小集合用于生成。
- **repetition_penalty** (`float`, 可选, 默认为1.0): 重复惩罚的参数。1.0表示无惩罚。

**Section sources**
- [configuration_utils.py](file://src/transformers/generation/configuration_utils.py#L81-L533)

## 配置设置方法

### 通过代码设置
可以通过直接实例化`GenerationConfig`类来设置生成参数。

```python
from transformers import GenerationConfig

config = GenerationConfig(
    max_new_tokens=100,
    temperature=0.7,
    top_k=50,
    top_p=0.9,
    repetition_penalty=1.2
)
```

### 通过配置文件设置
生成配置也可以通过JSON文件进行设置，通常在模型的`generation_config.json`文件中定义。

```yaml
# generation.yaml 示例
generate_kwargs:
  max_new_tokens: 128
  min_new_tokens: 128
  do_sample: false
```

**Section sources**
- [configuration_utils.py](file://src/transformers/generation/configuration_utils.py#L81-L1152)
- [generation.yaml](file://benchmark/config/generation.yaml#L1-L57)

## 不同任务的推荐配置

### 文本生成
对于创意性文本生成，推荐使用采样策略以增加多样性。

- `do_sample=True`
- `temperature=0.7-1.0`
- `top_p=0.9`
- `repetition_penalty=1.2`

### 翻译
对于翻译任务，推荐使用束搜索以获得更准确的结果。

- `num_beams=4-6`
- `do_sample=False`
- `repetition_penalty=1.0`

### 摘要
对于摘要任务，需要平衡长度和信息密度。

- `max_new_tokens=150-300`
- `num_beams=4`
- `length_penalty=1.0`

**Section sources**
- [run_generation.py](file://examples/pytorch/text-generation/run_generation.py#L1-L450)
- [test_configuration_utils.py](file://tests/generation/test_configuration_utils.py#L1-L771)

## 参数相互影响与最佳实践

### 参数组合注意事项
某些参数组合可能会导致冲突或无效配置。例如：
- 当`do_sample=False`时，`temperature`、`top_k`、`top_p`等采样相关参数将被忽略。
- 当`num_beams=1`时，`early_stopping`、`length_penalty`等束搜索相关参数将被忽略。

### 最佳实践
1. **避免冲突配置**: 确保参数组合逻辑一致，避免设置相互矛盾的参数。
2. **逐步调优**: 从默认配置开始，逐步调整关键参数以优化结果。
3. **任务导向**: 根据具体任务需求选择合适的生成策略。

**Section sources**
- [configuration_utils.py](file://src/transformers/generation/configuration_utils.py#L81-L1152)

## 配置验证与错误处理

### 配置验证
`GenerationConfig`类提供了`validate()`方法来验证配置的有效性。

```python
config = GenerationConfig(temperature=0.5, do_sample=False)
# 这将产生警告，因为temperature在do_sample=False时无效
config.validate()
```

### 错误处理
当配置存在严重问题时，系统会抛出异常。例如：
- 尝试保存无效配置时会抛出`ValueError`。
- 传递`generate()`专用参数到`GenerationConfig`会引发错误。

**Section sources**
- [configuration_utils.py](file://src/transformers/generation/configuration_utils.py#L81-L1152)
- [test_configuration_utils.py](file://tests/generation/test_configuration_utils.py#L1-L771)

## 配置示例

### 基础配置示例
```python
from transformers import GenerationConfig

# 简单的生成配置
basic_config = GenerationConfig(
    max_new_tokens=50,
    do_sample=True,
    temperature=0.8
)
```

### 高级调优示例
```python
# 复杂的调优配置
advanced_config = GenerationConfig(
    max_new_tokens=200,
    min_new_tokens=50,
    do_sample=True,
    temperature=0.7,
    top_k=50,
    top_p=0.9,
    repetition_penalty=1.2,
    num_return_sequences=3
)
```

**Section sources**
- [run_generation.py](file://examples/pytorch/text-generation/run_generation.py#L1-L450)
- [test_configuration_utils.py](file://tests/generation/test_configuration_utils.py#L1-L771)