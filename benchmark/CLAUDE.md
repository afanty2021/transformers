[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > **benchmark**

# Benchmark æ¨¡å—æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `benchmark/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 90%

## æ¨¡å—èŒè´£

Benchmarkæ¨¡å—æä¾›äº†å…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°Transformersä¸­å„ç§æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€å†…å­˜ä½¿ç”¨ã€ååé‡ç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡ã€‚è¿™äº›åŸºå‡†æµ‹è¯•å¯¹äºæ¨¡å‹é€‰æ‹©ã€ä¼˜åŒ–å’Œç”Ÿäº§éƒ¨ç½²è‡³å…³é‡è¦ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **å¤šç»´è¯„ä¼°**: é€Ÿåº¦ã€å†…å­˜ã€ååé‡ã€å»¶è¿Ÿç­‰å¤šç»´åº¦æ€§èƒ½æŒ‡æ ‡
- **å¤šç¡¬ä»¶æ”¯æŒ**: CPUã€GPUã€TPUç­‰ä¸åŒç¡¬ä»¶å¹³å°æµ‹è¯•
- **æ¨¡å‹è¦†ç›–**: æ¶µç›–NLPã€CVã€è¯­éŸ³ç­‰å¤šæ¨¡æ€æ¨¡å‹
- **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°çš„åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹
- **æŒç»­ç›‘æ§**: æ”¯æŒæŒç»­æ€§èƒ½ç›‘æ§å’Œå›å½’æ£€æµ‹

## ç›®å½•ç»“æ„

```
benchmark/
â”œâ”€â”€ README.md                                    # æ¦‚è¿°å’Œä½¿ç”¨æŒ‡å—
â”œâ”€â”€ __init__.py                                 # æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ benchmark.py                                # æ ¸å¿ƒåŸºå‡†æµ‹è¯•æ¡†æ¶
â”œâ”€â”€ benchmarks_entrypoint.py                   # åŸºå‡†æµ‹è¯•å…¥å£ç‚¹
â”œâ”€â”€ optimum_benchmark_wrapper.py                # Optimumé›†æˆåŒ…è£…å™¨
â”œâ”€â”€ default.yml                                 # é»˜è®¤é…ç½®æ–‡ä»¶
â”œâ”€â”€ grafana_dashboard.json                      # Grafanaä»ªè¡¨æ¿é…ç½®
â”œâ”€â”€ grafana_datasource.yaml                     # Grafanaæ•°æ®æºé…ç½®
â”œâ”€â”€ requirements.txt                            # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ *.py                                       # å…·ä½“æ¨¡å‹åŸºå‡†æµ‹è¯•è„šæœ¬
```

## æ ¸å¿ƒç»„ä»¶åˆ†æ

### 1. benchmark.py - åŸºå‡†æµ‹è¯•æ¡†æ¶

#### æ¦‚è¿°
æä¾›ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•æ¥å£å’Œåº¦é‡æ”¶é›†æ¡†æ¶ã€‚

#### æ ¸å¿ƒåŠŸèƒ½
```python
import time
import torch
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    model_name_or_path: str
    device: str = "auto"
    batch_size: int = 1
    sequence_length: int = 512
    num_iterations: int = 100
    warmup_iterations: int = 10
    torch_dtype: Optional[str] = None
    trust_remote_code: bool = False
    use_cache: bool = True

@dataclass
class BenchmarkResults:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    model_name: str
    device: str
    batch_size: int
    sequence_length: int

    # æ—¶é—´æŒ‡æ ‡
    model_load_time: float
    inference_time: float
    time_to_first_token: float
    tokens_per_second: float

    # å†…å­˜æŒ‡æ ‡
    memory_usage_mb: float
    gpu_memory_usage_mb: float

    # ååé‡æŒ‡æ ‡
    throughput_samples_per_second: float
    throughput_tokens_per_second: float

class ModelBenchmark:
    """æ¨¡å‹åŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.results = BenchmarkResults(
            model_name=config.model_name_or_path,
            device=config.device,
            batch_size=config.batch_size,
            sequence_length=config.sequence_length,
            # åˆå§‹åŒ–å…¶ä»–å­—æ®µä¸º0
            model_load_time=0.0,
            inference_time=0.0,
            time_to_first_token=0.0,
            tokens_per_second=0.0,
            memory_usage_mb=0.0,
            gpu_memory_usage_mb=0.0,
            throughput_samples_per_second=0.0,
            throughput_tokens_per_second=0.0
        )

    def run_benchmark(self) -> BenchmarkResults:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        # 1. æµ‹é‡æ¨¡å‹åŠ è½½æ—¶é—´
        start_time = time.time()
        model = self._load_model()
        self.results.model_load_time = time.time() - start_time

        # 2. é¢„çƒ­
        self._warmup(model)

        # 3. æµ‹é‡æ¨ç†æ€§èƒ½
        self._measure_inference(model)

        # 4. æµ‹é‡å†…å­˜ä½¿ç”¨
        self._measure_memory(model)

        return self.results
```

#### å…³é”®åŠŸèƒ½æ¨¡å—

##### æ¨¡å‹åŠ è½½æµ‹é‡
```python
def _load_model(self):
    """åŠ è½½æ¨¡å‹å¹¶æµ‹é‡åŠ è½½æ—¶é—´"""
    start_memory = self._get_memory_usage()

    # æ ¹æ®æ¨¡å‹ç±»å‹åŠ è½½
    if "bert" in self.config.model_name_or_path.lower():
        from transformers import AutoModelForSequenceClassification
        model = AutoModelForSequenceClassification.from_pretrained(
            self.config.model_name_or_path,
            torch_dtype=self.config.torch_dtype,
            trust_remote_code=self.config.trust_remote_code
        )
    elif "gpt" in self.config.model_name_or_path.lower():
        from transformers import AutoModelForCausalLM
        model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name_or_path,
            torch_dtype=self.config.torch_dtype,
            trust_remote_code=self.config.trust_remote_code
        )
    else:
        # é€šç”¨æ¨¡å‹åŠ è½½
        from transformers import AutoModel
        model = AutoModel.from_pretrained(
            self.config.model_name_or_path,
            torch_dtype=self.config.torch_dtype,
            trust_remote_code=self.config.trust_remote_code
        )

    # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    if self.config.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = self.config.device

    model = model.to(device)
    model.eval()

    end_memory = self._get_memory_usage()
    self.results.memory_usage_mb = end_memory - start_memory

    return model
```

##### æ¨ç†æ€§èƒ½æµ‹é‡
```python
def _measure_inference(self, model):
    """æµ‹é‡æ¨ç†æ€§èƒ½"""
    # å‡†å¤‡è¾“å…¥æ•°æ®
    inputs = self._prepare_inputs()

    # é¢„çƒ­
    for _ in range(self.config.warmup_iterations):
        with torch.no_grad():
            _ = model(**inputs)

    # åŒæ­¥GPU
    if torch.cuda.is_available():
        torch.cuda.synchronize()

    # æµ‹é‡æ—¶é—´
    start_time = time.time()

    for i in range(self.config.num_iterations):
        with torch.no_grad():
            if i == 0:
                # æµ‹é‡é¦–æ¬¡æ¨ç†æ—¶é—´
                first_token_start = time.time()
                output = model(**inputs)
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                self.results.time_to_first_token = time.time() - first_token_start
            else:
                output = model(**inputs)

    if torch.cuda.is_available():
        torch.cuda.synchronize()

    end_time = time.time()

    # è®¡ç®—æŒ‡æ ‡
    total_time = end_time - start_time
    avg_inference_time = total_time / self.config.num_iterations
    self.results.inference_time = avg_inference_time

    # è®¡ç®—ååé‡
    self.results.throughput_samples_per_second = (
        self.config.batch_size * self.config.num_iterations / total_time
    )

    # è®¡ç®—tokenååé‡ï¼ˆå¯¹äºç”Ÿæˆæ¨¡å‹ï¼‰
    if hasattr(output, 'logits') and output.logits is not None:
        total_tokens = (self.config.batch_size *
                       self.config.sequence_length *
                       self.config.num_iterations)
        self.results.throughput_tokens_per_second = total_tokens / total_time
        self.results.tokens_per_second = (
            self.config.batch_size * self.config.sequence_length / avg_inference_time
        )
```

##### å†…å­˜ä½¿ç”¨æµ‹é‡
```python
def _measure_memory(self, model):
    """æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

        # è¿è¡Œæ¨ç†å¹¶æµ‹é‡å³°å€¼å†…å­˜
        inputs = self._prepare_inputs()

        with torch.no_grad():
            _ = model(**inputs)

        if torch.cuda.is_available():
            torch.cuda.synchronize()

            # GPUå†…å­˜ä½¿ç”¨
            self.results.gpu_memory_usage_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)
```

### 2. benchmarks_entrypoint.py - ç»Ÿä¸€å…¥å£ç‚¹

#### æ¦‚è¿°
æä¾›æ‰€æœ‰åŸºå‡†æµ‹è¯•çš„ç»Ÿä¸€å…¥å£ç‚¹å’Œç»“æœæ”¶é›†æœºåˆ¶ã€‚

#### æ ¸å¿ƒåŠŸèƒ½
```python
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any
from benchmark import ModelBenchmark, BenchmarkConfig

class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.logger = self._setup_logger()
        self.results = []

    def run_all_benchmarks(self, config_file: str):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        import yaml
        with open(config_file, 'r') as f:
            configs = yaml.safe_load(f)

        for config_dict in configs['benchmarks']:
            try:
                config = BenchmarkConfig(**config_dict)
                benchmark = ModelBenchmark(config)
                results = benchmark.run_benchmark()
                self.results.append(results)
                self.logger.info(f"Completed benchmark for {config.model_name_or_path}")

            except Exception as e:
                self.logger.error(f"Benchmark failed for {config_dict.get('model_name_or_path', 'unknown')}: {e}")

    def save_results(self, output_file: str):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        import json

        # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        results_dict = {
            'benchmark_results': [vars(result) for result in self.results],
            'summary': self._generate_summary()
        }

        with open(output_file, 'w') as f:
            json.dump(results_dict, f, indent=2)

    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»“æœæ‘˜è¦"""
        if not self.results:
            return {}

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        inference_times = [r.inference_time for r in self.results]
        memory_usages = [r.memory_usage_mb for r in self.results]
        throughputs = [r.throughput_samples_per_second for r in self.results]

        return {
            'total_benchmarks': len(self.results),
            'average_inference_time': sum(inference_times) / len(inference_times),
            'max_inference_time': max(inference_times),
            'min_inference_time': min(inference_times),
            'average_memory_usage_mb': sum(memory_usages) / len(memory_usages),
            'max_memory_usage_mb': max(memory_usages),
            'average_throughput': sum(throughputs) / len(throughputs),
            'best_throughput': max(throughputs),
            'worst_throughput': min(throughputs)
        }

def main():
    """ä¸»å…¥å£å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Run Transformers Benchmarks")
    parser.add_argument("--config", type=str, required=True,
                       help="Configuration file for benchmarks")
    parser.add_argument("--output", type=str, default="benchmark_results.json",
                       help="Output file for results")
    parser.add_argument("--log-level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"])

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(level=getattr(logging, args.log_level))

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    runner = BenchmarkRunner()
    runner.run_all_benchmarks(args.config)
    runner.save_results(args.output)

    print(f"Benchmark completed. Results saved to {args.output}")

if __name__ == "__main__":
    main()
```

### 3. optimum_benchmark_wrapper.py - Optimumé›†æˆ

#### æ¦‚è¿°
é›†æˆHuggingFace Optimumåº“ï¼Œæä¾›ä¼˜åŒ–çš„åŸºå‡†æµ‹è¯•æ”¯æŒã€‚

#### æ ¸å¿ƒåŠŸèƒ½
```python
from optimum.benchmark import Benchmark, BenchmarkConfig, BenchmarkReport
from optimum.benchmark.backend import (
    PyTorchBackendConfig,
    TensorRTBackendConfig,
    ONNXRuntimeBackendConfig,
)

class OptimumBenchmarkWrapper:
    """OptimumåŸºå‡†æµ‹è¯•åŒ…è£…å™¨"""

    def __init__(self, model_name: str, backend: str = "pytorch"):
        self.model_name = model_name
        self.backend = backend
        self.benchmark = None

    def create_pytorch_benchmark(self, **kwargs):
        """åˆ›å»ºPyTorchåŸºå‡†æµ‹è¯•"""
        config = BenchmarkConfig(
            model_name_or_path=self.model_name,
            backend="pytorch",
            backend_config=PyTorchBackendConfig(
                device="cuda" if torch.cuda.is_available() else "cpu",
                torch_dtype="float16" if torch.cuda.is_available() else "float32",
            ),
            **kwargs
        )

        self.benchmark = Benchmark(config)

    def create_onnx_benchmark(self, **kwargs):
        """åˆ›å»ºONNXåŸºå‡†æµ‹è¯•"""
        config = BenchmarkConfig(
            model_name_or_path=self.model_name,
            backend="onnx_runtime",
            backend_config=ONNXRuntimeBackendConfig(
                device="cuda" if torch.cuda.is_available() else "cpu",
                provider="CUDAExecutionProvider" if torch.cuda.is_available() else "CPUExecutionProvider",
            ),
            **kwargs
        )

        self.benchmark = Benchmark(config)

    def create_tensorrt_benchmark(self, **kwargs):
        """åˆ›å»ºTensorRTåŸºå‡†æµ‹è¯•"""
        config = BenchmarkConfig(
            model_name_or_path=self.model_name,
            backend="tensorrt",
            backend_config=TensorRTBackendConfig(
                device="cuda",
                precision="fp16",
            ),
            **kwargs
        )

        self.benchmark = Benchmark(config)

    def run(self):
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        if self.benchmark is None:
            raise ValueError("Benchmark not created. Call create_*_benchmark first.")

        report = self.benchmark.run()
        return report

    def compare_backends(self, backends: List[str], **common_kwargs):
        """æ¯”è¾ƒä¸åŒåç«¯çš„æ€§èƒ½"""
        results = {}

        for backend in backends:
            print(f"Running benchmark for {backend} backend...")

            if backend == "pytorch":
                self.create_pytorch_benchmark(**common_kwargs)
            elif backend == "onnx":
                self.create_onnx_benchmark(**common_kwargs)
            elif backend == "tensorrt":
                self.create_tensorrt_benchmark(**common_kwargs)
            else:
                print(f"Unsupported backend: {backend}")
                continue

            try:
                report = self.run()
                results[backend] = report
            except Exception as e:
                print(f"Benchmark failed for {backend}: {e}")

        return results
```

## é…ç½®æ–‡ä»¶å’Œä»ªè¡¨æ¿

### 1. default.yml - é»˜è®¤é…ç½®

#### æ¦‚è¿°
å®šä¹‰åŸºå‡†æµ‹è¯•çš„é»˜è®¤å‚æ•°å’Œæ¨¡å‹åˆ—è¡¨ã€‚

#### ç¤ºä¾‹é…ç½®
```yaml
# åŸºå‡†æµ‹è¯•é…ç½®
benchmarks:
  # å°å‹æ¨¡å‹
  - model_name_or_path: "bert-base-uncased"
    batch_size: 1
    sequence_length: 128
    num_iterations: 100
    device: "cuda"

  - model_name_or_path: "distilbert-base-uncased"
    batch_size: 1
    sequence_length: 128
    num_iterations: 100
    device: "cuda"

  # ä¸­å‹æ¨¡å‹
  - model_name_or_path: "bert-large-uncased"
    batch_size: 1
    sequence_length: 512
    num_iterations: 50
    device: "cuda"

  - model_name_or_path: "roberta-large"
    batch_size: 1
    sequence_length: 512
    num_iterations: 50
    device: "cuda"

  # ç”Ÿæˆæ¨¡å‹
  - model_name_or_path: "gpt2"
    batch_size: 1
    sequence_length: 1024
    num_iterations: 20
    device: "cuda"

  - model_name_or_path: "facebook/opt-6.7b"
    batch_size: 1
    sequence_length: 2048
    num_iterations: 10
    device: "cuda"

  # å¤šæ¨¡æ€æ¨¡å‹
  - model_name_or_path: "openai/clip-vit-base-patch32"
    batch_size: 4
    num_iterations: 100
    device: "cuda"

# å…¨å±€è®¾ç½®
global_settings:
  warmup_iterations: 10
  torch_dtype: "float16"
  trust_remote_code: false
  use_cache: true

# è¾“å‡ºè®¾ç½®
output_settings:
  save_detailed_results: true
  save_model_info: true
  save_system_info: true
  generate_plots: true
```

### 2. Grafanaä»ªè¡¨æ¿

#### æ¦‚è¿°
æä¾›å®æ—¶çš„æ€§èƒ½ç›‘æ§å’Œå¯è§†åŒ–ä»ªè¡¨æ¿ã€‚

#### å…³é”®æŒ‡æ ‡
- **æ¨¡å‹æ¨ç†æ—¶é—´**: å»¶è¿Ÿå’Œååé‡
- **å†…å­˜ä½¿ç”¨**: CPUå’ŒGPUå†…å­˜å ç”¨
- **æ¨¡å‹åŠ è½½æ—¶é—´**: æ¨¡å‹åˆå§‹åŒ–æ—¶é—´
- **ååé‡**: æ¯ç§’å¤„ç†çš„æ ·æœ¬/tokenæ•°

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€åŸºå‡†æµ‹è¯•

```python
from benchmark import ModelBenchmark, BenchmarkConfig

# åˆ›å»ºåŸºå‡†æµ‹è¯•é…ç½®
config = BenchmarkConfig(
    model_name_or_path="bert-base-uncased",
    device="cuda",
    batch_size=8,
    sequence_length=512,
    num_iterations=100,
    torch_dtype="float16"
)

# è¿è¡ŒåŸºå‡†æµ‹è¯•
benchmark = ModelBenchmark(config)
results = benchmark.run_benchmark()

# æ‰“å°ç»“æœ
print(f"Model: {results.model_name}")
print(f"Batch size: {results.batch_size}")
print(f"Inference time: {results.inference_time:.4f}s")
print(f"Throughput: {results.throughput_samples_per_second:.2f} samples/s")
print(f"Memory usage: {results.memory_usage_mb:.2f} MB")
```

### 2. æ‰¹é‡åŸºå‡†æµ‹è¯•

```python
import yaml
from benchmark import ModelBenchmark, BenchmarkConfig

# ä»é…ç½®æ–‡ä»¶åŠ è½½æµ‹è¯•åˆ—è¡¨
with open("benchmark/default.yml", 'r') as f:
    config_data = yaml.safe_load(f)

results = []

for benchmark_config in config_data['benchmarks']:
    print(f"Running benchmark for {benchmark_config['model_name_or_path']}")

    config = BenchmarkConfig(**benchmark_config)
    benchmark = ModelBenchmark(config)
    result = benchmark.run_benchmark()

    results.append(result)

    print(f"  Inference time: {result.inference_time:.4f}s")
    print(f"  Throughput: {result.throughput_samples_per_second:.2f} samples/s")
    print(f"  Memory usage: {result.memory_usage_mb:.2f} MB")

# ä¿å­˜ç»“æœ
import json
with open("benchmark_results.json", 'w') as f:
    json.dump([vars(r) for r in results], f, indent=2)
```

### 3. æ¨¡å‹å¯¹æ¯”æµ‹è¯•

```python
from benchmark import ModelBenchmark, BenchmarkConfig

models_to_compare = [
    "bert-base-uncased",
    "distilbert-base-uncased",
    "roberta-base",
    "albert-base-v2"
]

results = {}

for model_name in models_to_compare:
    config = BenchmarkConfig(
        model_name_or_path=model_name,
        device="cuda",
        batch_size=16,
        sequence_length=128,
        num_iterations=100
    )

    benchmark = ModelBenchmark(config)
    result = benchmark.run_benchmark()
    results[model_name] = result

# åˆ†æå’Œæ¯”è¾ƒç»“æœ
print("Model Comparison Results:")
print("-" * 60)
print(f"{'Model':<25} {'Inference (s)':<15} {'Throughput (samples/s)':<20} {'Memory (MB)':<15}")
print("-" * 60)

for model_name, result in results.items():
    print(f"{model_name:<25} {result.inference_time:<15.4f} "
          f"{result.throughput_samples_per_second:<20.2f} {result.memory_usage_mb:<15.2f}")

# æ‰¾å‡ºæœ€ä½³æ€§èƒ½
best_throughput = max(results.items(), key=lambda x: x[1].throughput_samples_per_second)
lowest_memory = min(results.items(), key=lambda x: x[1].memory_usage_mb)

print(f"\nBest throughput: {best_throughput[0]} "
      f"({best_throughput[1].throughput_samples_per_second:.2f} samples/s)")
print(f"Lowest memory: {lowest_memory[0]} "
      f"({lowest_memory[1].memory_usage_mb:.2f} MB)")
```

### 4. ç¡¬ä»¶æ€§èƒ½æµ‹è¯•

```python
def benchmark_across_devices(model_name, devices):
    """åœ¨ä¸åŒè®¾å¤‡ä¸Šæµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    results = {}

    for device in devices:
        try:
            config = BenchmarkConfig(
                model_name_or_path=model_name,
                device=device,
                batch_size=8,
                sequence_length=512,
                num_iterations=50
            )

            benchmark = ModelBenchmark(config)
            result = benchmark.run_benchmark()
            results[device] = result

            print(f"{device}: {result.inference_time:.4f}s, "
                  f"{result.throughput_samples_per_second:.2f} samples/s")

        except Exception as e:
            print(f"Failed to benchmark on {device}: {e}")

    return results

# æµ‹è¯•CPU vs GPUæ€§èƒ½
if torch.cuda.is_available():
    gpu_results = benchmark_across_devices("bert-base-uncased", ["cpu", "cuda"])
else:
    cpu_results = benchmark_across_devices("bert-base-uncased", ["cpu"])
```

### 5. Optimumåç«¯å¯¹æ¯”

```python
from benchmark.optimum_benchmark_wrapper import OptimumBenchmarkWrapper

# åˆ›å»ºOptimumåŸºå‡†æµ‹è¯•
wrapper = OptimumBenchmarkWrapper("bert-base-uncased")

# æ¯”è¾ƒä¸åŒåç«¯
results = wrapper.compare_backends(
    backends=["pytorch", "onnx", "tensorrt"],
    batch_size=8,
    sequence_length=512,
    num_iterations=100
)

# åˆ†æç»“æœ
for backend, report in results.items():
    print(f"\n{backend.upper()} Backend:")
    print(f"  Latency: {report.latency:.4f}s")
    print(f"  Throughput: {report.throughput:.2f} samples/s")
    if hasattr(report, 'memory'):
        print(f"  Memory: {report.memory:.2f} MB")
```

## æ€§èƒ½åˆ†æå’ŒæŠ¥å‘Š

### 1. æ€§èƒ½ç“¶é¢ˆåˆ†æ

```python
def analyze_performance_bottlenecks(results):
    """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
    bottlenecks = []

    for result in results:
        # æ£€æŸ¥æ¨ç†æ—¶é—´
        if result.inference_time > 1.0:  # è¶…è¿‡1ç§’
            bottlenecks.append({
                'model': result.model_name,
                'type': 'high_latency',
                'value': result.inference_time,
                'threshold': 1.0
            })

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        if result.memory_usage_mb > 8192:  # è¶…è¿‡8GB
            bottlenecks.append({
                'model': result.model_name,
                'type': 'high_memory',
                'value': result.memory_usage_mb,
                'threshold': 8192
            })

        # æ£€æŸ¥ååé‡
        if result.throughput_samples_per_second < 10:  # å°äº10 samples/s
            bottlenecks.append({
                'model': result.model_name,
                'type': 'low_throughput',
                'value': result.throughput_samples_per_second,
                'threshold': 10
            })

    return bottlenecks

# ä½¿ç”¨ç¤ºä¾‹
bottlenecks = analyze_performance_bottlenecks(results)
for bottleneck in bottlenecks:
    print(f"âš ï¸  {bottleneck['model']}: {bottleneck['type']} "
          f"({bottleneck['value']:.2f}, threshold: {bottleneck['threshold']})")
```

### 2. æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ

```python
def generate_performance_report(results, output_file="performance_report.html"):
    """ç”ŸæˆHTMLæ€§èƒ½æŠ¥å‘Š"""
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Transformers Performance Report</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #f2f2f2; }
            .good { color: green; }
            .warning { color: orange; }
            .bad { color: red; }
        </style>
    </head>
    <body>
        <h1>Transformers Performance Report</h1>
        <h2>Model Performance Summary</h2>
        <table>
            <tr>
                <th>Model</th>
                <th>Inference Time (s)</th>
                <th>Throughput (samples/s)</th>
                <th>Memory Usage (MB)</th>
                <th>Performance Rating</th>
            </tr>
            {table_rows}
        </table>

        <h2>Performance Analysis</h2>
        <h3>Best Performing Models</h3>
        <ul>{best_models}</ul>

        <h3>Performance Recommendations</h3>
        <ul>{recommendations}</ul>
    </body>
    </html>
    """

    # ç”Ÿæˆè¡¨æ ¼è¡Œ
    table_rows = ""
    for result in results:
        # æ€§èƒ½è¯„çº§
        if (result.inference_time < 0.1 and
            result.throughput_samples_per_second > 100 and
            result.memory_usage_mb < 1024):
            rating = '<span class="good">Excellent</span>'
        elif (result.inference_time < 0.5 and
              result.throughput_samples_per_second > 20 and
              result.memory_usage_mb < 4096):
            rating = '<span class="warning">Good</span>'
        else:
            rating = '<span class="bad">Needs Optimization</span>'

        table_rows += f"""
        <tr>
            <td>{result.model_name}</td>
            <td>{result.inference_time:.4f}</td>
            <td>{result.throughput_samples_per_second:.2f}</td>
            <td>{result.memory_usage_mb:.2f}</td>
            <td>{rating}</td>
        </tr>
        """

    # ç”Ÿæˆæœ€ä½³æ¨¡å‹åˆ—è¡¨
    sorted_by_throughput = sorted(results, key=lambda x: x.throughput_samples_per_second, reverse=True)
    best_models = ""
    for i, result in enumerate(sorted_by_throughput[:3], 1):
        best_models += f"<li>{i}. {result.model_name}: {result.throughput_samples_per_second:.2f} samples/s</li>"

    # ç”Ÿæˆå»ºè®®
    recommendations = """
    <li>For high-throughput applications, consider using distilled models like DistilBERT</li>
    <li>For memory-constrained environments, use smaller models or quantization</li>
    <li>Consider using ONNX or TensorRT for optimized inference</li>
    <li>Use mixed precision (FP16) where available to improve speed and reduce memory</li>
    """

    # ç”ŸæˆHTMLæŠ¥å‘Š
    html_content = html_template.format(
        table_rows=table_rows,
        best_models=best_models,
        recommendations=recommendations
    )

    with open(output_file, 'w') as f:
        f.write(html_content)

    print(f"Performance report generated: {output_file}")
```

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•å¤„ç†å¤§æ¨¡å‹çš„å†…å­˜ä¸è¶³é—®é¢˜ï¼Ÿ
A: è§£å†³æ–¹æ¡ˆï¼š
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- å¯ç”¨æ¨¡å‹å¹¶è¡Œ
- ä½¿ç”¨é‡åŒ–æŠ€æœ¯
- å‡å°‘æ‰¹å¤„ç†å¤§å°

### Q: åŸºå‡†æµ‹è¯•ç»“æœå¦‚ä½•ä¸å…¶ä»–ç ”ç©¶æ¯”è¾ƒï¼Ÿ
A: æ–¹æ³•ï¼š
- ä½¿ç”¨ç›¸åŒçš„æ•°æ®å’Œé…ç½®
- æŠ¥å‘Šç¡¬ä»¶é…ç½®è¯¦æƒ…
- è€ƒè™‘é¢„çƒ­æ—¶é—´
- å¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼

### Q: å¦‚ä½•ä¼˜åŒ–æ¨ç†æ€§èƒ½ï¼Ÿ
A: ä¼˜åŒ–ç­–ç•¥ï¼š
- æ¨¡å‹é‡åŒ– (FP16, INT8)
- ä½¿ç”¨ç¼–è¯‘ä¼˜åŒ– (TorchScript, ONNX)
- æ‰¹å¤„ç†ä¼˜åŒ–
- ç¡¬ä»¶ç‰¹å®šä¼˜åŒ–

### Q: åŸºå‡†æµ‹è¯•åº”è¯¥åŒ…å«å“ªäº›æŒ‡æ ‡ï¼Ÿ
A: å…³é”®æŒ‡æ ‡ï¼š
- å»¶è¿Ÿ (Latency)
- ååé‡ (Throughput)
- å†…å­˜ä½¿ç”¨ (Memory Usage)
- èƒ½æºæ•ˆç‡ (Power Consumption)
- å‡†ç¡®ç‡ (Accuracy)

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ–‡ä»¶
- `benchmark.py`: æ ¸å¿ƒåŸºå‡†æµ‹è¯•æ¡†æ¶
- `benchmarks_entrypoint.py`: ç»Ÿä¸€å…¥å£ç‚¹
- `optimum_benchmark_wrapper.py`: Optimumé›†æˆ
- `default.yml`: é»˜è®¤é…ç½®æ–‡ä»¶

### ç›‘æ§å’Œå¯è§†åŒ–
- `grafana_dashboard.json`: Grafanaä»ªè¡¨æ¿é…ç½®
- `grafana_datasource.yaml`: Grafanaæ•°æ®æºé…ç½®

### ä¾èµ–æ–‡ä»¶
- `requirements.txt`: ä¾èµ–åŒ…åˆ—è¡¨
- `__init__.py`: æ¨¡å—åˆå§‹åŒ–
- `README.md`: ä½¿ç”¨æŒ‡å—å’Œè¯´æ˜

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - è¯¦ç»†åˆ†æ
- âœ¨ å®ŒæˆBenchmarkæ¨¡å—ç»“æ„åˆ†æ
- ğŸ” è®°å½•æ ¸å¿ƒæµ‹è¯•æ¡†æ¶å’Œå·¥å…·
- ğŸ“Š åˆ†æé…ç½®æ–‡ä»¶å’Œç›‘æ§ä»ªè¡¨æ¿
- ğŸ¯ æä¾›å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹å’Œæ€§èƒ½åˆ†æ

### ä¸‹ä¸€æ­¥è®¡åˆ’
- [ ] åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•çš„è¯¦ç»†æŒ‡å—
- [ ] è®°å½•ä¸åŒç¡¬ä»¶å¹³å°çš„åŸºå‡†æ•°æ®
- [ ] åˆ†ææ€§èƒ½ä¼˜åŒ–çš„æœ€ä½³å®è·µ
- [ ] åˆ›å»ºæŒç»­é›†æˆä¸­çš„æ€§èƒ½ç›‘æ§æ–‡æ¡£

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 90%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 95%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20