[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > **examples**

# Examples æ¨¡å—æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `examples/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 90%

## æ¨¡å—èŒè´£

Examplesæ¨¡å—æä¾›äº†ä¸°å¯Œçš„ç¤ºä¾‹ä»£ç ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨ä¸åŒä»»åŠ¡å’Œåœºæ™¯ä¸­ä½¿ç”¨Transformersåº“ã€‚è¿™äº›ç¤ºä¾‹æ¶µç›–äº†è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸçš„æœ€ä½³å®è·µã€‚

### æ ¸å¿ƒç‰¹æ€§
- **ä»»åŠ¡å¯¼å‘**: æŒ‰MLä»»åŠ¡ç»„ç»‡çš„ç¤ºä¾‹ä»£ç 
- **æœ€ä½³å®è·µ**: å±•ç¤ºæ¨èçš„ä½¿ç”¨æ–¹æ³•å’Œé…ç½®
- **å®Œæ•´æµç¨‹**: ä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
- **å¤šæ¡†æ¶æ”¯æŒ**: PyTorchã€TensorFlowã€JAXç­‰åç«¯
- **æ‰©å±•æ€§**: æ˜“äºä¿®æ”¹å’Œæ‰©å±•åˆ°å…·ä½“ç”¨ä¾‹

## ç›®å½•ç»“æ„

```
examples/
â”œâ”€â”€ README.md                                    # æ¦‚è¿°å’Œå¿«é€Ÿå¼€å§‹æŒ‡å—
â”œâ”€â”€ legacy/                                      # æ—§ç‰ˆç¤ºä¾‹ï¼ˆç»´æŠ¤è¾ƒå°‘ï¼‰
â”‚   â”œâ”€â”€ benchmarking/                           # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”‚   â”œâ”€â”€ multiple_choice/                        # å¤šé€‰ä»»åŠ¡ç¤ºä¾‹
â”‚   â”œâ”€â”€ pytorch-lightning/                      # PyTorch Lightningé›†æˆ
â”‚   â”œâ”€â”€ question-answering/                     # é—®ç­”ä»»åŠ¡ç¤ºä¾‹
â”‚   â”œâ”€â”€ seq2seq/                                # åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
â”‚   â””â”€â”€ token-classification/                   # æ ‡è®°åˆ†ç±»ç¤ºä¾‹
â”œâ”€â”€ pytorch/                                     # PyTorchç¤ºä¾‹ï¼ˆä¸»è¦ç»´æŠ¤ï¼‰
â”‚   â”œâ”€â”€ language-modeling/                      # è¯­è¨€å»ºæ¨¡
â”‚   â”œâ”€â”€ multiple-choice/                        # å¤šé€‰ä»»åŠ¡
â”‚   â”œâ”€â”€ question-answering/                     # é—®ç­”ä»»åŠ¡
â”‚   â”œâ”€â”€ summarization/                          # æ–‡æœ¬æ‘˜è¦
â”‚   â”œâ”€â”€ text-classification/                    # æ–‡æœ¬åˆ†ç±»
â”‚   â”œâ”€â”€ text-generation/                        # æ–‡æœ¬ç”Ÿæˆ
â”‚   â”œâ”€â”€ token-classification/                   # æ ‡è®°åˆ†ç±»
â”‚   â”œâ”€â”€ translation/                            # æœºå™¨ç¿»è¯‘
â”‚   â”œâ”€â”€ speech-recognition/                     # è¯­éŸ³è¯†åˆ«
â”‚   â”œâ”€â”€ audio-classification/                   # éŸ³é¢‘åˆ†ç±»
â”‚   â”œâ”€â”€ image-pretraining/                      # å›¾åƒé¢„è®­ç»ƒ
â”‚   â”œâ”€â”€ image-classification/                   # å›¾åƒåˆ†ç±»
â”‚   â”œâ”€â”€ semantic-segmentation/                  # è¯­ä¹‰åˆ†å‰²
â”‚   â”œâ”€â”€ object-detection/                       # ç›®æ ‡æ£€æµ‹
â”‚   â””â”€â”€ instance-segmentation/                  # å®ä¾‹åˆ†å‰²
â”œâ”€â”€ tensorflow/                                 # TensorFlowç¤ºä¾‹
â”œâ”€â”€ flax/                                      # Flax/JAXç¤ºä¾‹
â”œâ”€â”€ research-projects/                          # ç ”ç©¶é¡¹ç›®
â”œâ”€â”€ scripts/                                   # è¾…åŠ©è„šæœ¬
â””â”€â”€ tests/                                     # ç¤ºä¾‹æµ‹è¯•
```

## æ ¸å¿ƒä»»åŠ¡ç¤ºä¾‹åˆ†æ

### 1. æ–‡æœ¬åˆ†ç±» (text-classification)

#### æ¦‚è¿°
æ–‡æœ¬åˆ†ç±»æ˜¯NLPçš„åŸºç¡€ä»»åŠ¡ï¼Œç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨å„ç§æ•°æ®é›†ä¸Šè¿›è¡Œæƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±»ç­‰ä»»åŠ¡ã€‚

#### æ ¸å¿ƒæ–‡ä»¶ç»“æ„
```
text-classification/
â”œâ”€â”€ run_glue.py                                # GLUEåŸºå‡†æµ‹è¯•è„šæœ¬
â”œâ”€â”€ run_xnli.py                                # å¤šè¯­è¨€ç†è§£ä»»åŠ¡
â”œâ”€â”€ requirements.txt                           # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ README.md                                  # è¯¦ç»†è¯´æ˜æ–‡æ¡£
```

#### å…³é”®ç‰¹æ€§
- **å¤šæ•°æ®é›†æ”¯æŒ**: GLUEã€XNLIã€IMDbç­‰
- **Traineré›†æˆ**: ä½¿ç”¨ğŸ¤— Trainerè¿›è¡Œè®­ç»ƒ
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šGPUå’ŒTPUè®­ç»ƒ
- **æ··åˆç²¾åº¦**: è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
- **æ¨¡å‹é€‰æ‹©**: æ”¯æŒBERTã€RoBERTaã€DistilBERTç­‰

#### ä½¿ç”¨ç¤ºä¾‹
```bash
# åŸºç¡€è®­ç»ƒ
python run_glue.py \
  --model_name_or_path bert-base-uncased \
  --task_name mrpc \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/mrpc/

# åˆ†å¸ƒå¼è®­ç»ƒ
python -m torch.distributed.launch \
  --nproc_per_node 8 run_glue.py \
  --model_name_or_path bert-large-uncased \
  --task_name mnli \
  --do_train \
  --do_eval \
  --per_device_train_batch_size 16 \
  --learning_rate 1e-5 \
  --num_train_epochs 5 \
  --output_dir /tmp/mnli/
```

### 2. è¯­è¨€å»ºæ¨¡ (language-modeling)

#### æ¦‚è¿°
è¯­è¨€å»ºæ¨¡ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•è¿›è¡Œè‡ªå›å½’å’Œæ©ç è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒå’Œå¾®è°ƒã€‚

#### æ ¸å¿ƒæ–‡ä»¶
```
language-modeling/
â”œâ”€â”€ run_clm.py                                 # å› æœè¯­è¨€å»ºæ¨¡ï¼ˆGPTé£æ ¼ï¼‰
â”œâ”€â”€ run_mlm.py                                 # æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆBERTé£æ ¼ï¼‰
â”œâ”€â”€ run_plm.py                                 # æ’åˆ—è¯­è¨€å»ºæ¨¡
â”œâ”€â”€ run_t5_mlm.py                              # T5æ©ç è¯­è¨€å»ºæ¨¡
â””â”€â”€ README.md                                  # è¯¦ç»†è¯´æ˜
```

#### å…³é”®ç‰¹æ€§
- **å¤šå»ºæ¨¡ç±»å‹**: CLMã€MLMã€PLMã€T5ç­‰
- **å¤§è§„æ¨¡æ•°æ®å¤„ç†**: æ”¯æŒå¤§è§„æ¨¡æ–‡æœ¬æ•°æ®é›†
- **å†…å­˜ä¼˜åŒ–**: æ”¯æŒæ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹
- **è‡ªå®šä¹‰æ•°æ®é›†**: æ˜“äºé›†æˆè‡ªå®šä¹‰è¯­æ–™

#### ä½¿ç”¨ç¤ºä¾‹
```bash
# å› æœè¯­è¨€å»ºæ¨¡
python run_clm.py \
  --model_name_or_path gpt2 \
  --train_file train.txt \
  --validation_file valid.txt \
  --do_train \
  --do_eval \
  --per_device_train_batch_size 4 \
  --gradient_accumulation_steps 8 \
  --learning_rate 5e-5 \
  --num_train_epochs 10 \
  --output_dir /tmp/clm/

# æ©ç è¯­è¨€å»ºæ¨¡
python run_mlm.py \
  --model_name_or_path roberta-base \
  --train_file train.txt \
  --do_train \
  --per_device_train_batch_size 32 \
  --learning_rate 1e-4 \
  --num_train_epochs 5 \
  --output_dir /tmp/mlm/
```

### 3. é—®ç­”ä»»åŠ¡ (question-answering)

#### æ¦‚è¿°
é—®ç­”ç¤ºä¾‹å±•ç¤ºäº†æŠ½å–å¼å’Œç”Ÿæˆå¼é—®ç­”ç³»ç»Ÿçš„å®ç°ï¼Œæ”¯æŒSQuADã€TriviaQAç­‰æ•°æ®é›†ã€‚

#### æ ¸å¿ƒåŠŸèƒ½
- **æŠ½å–å¼é—®ç­”**: ä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆç‰‡æ®µ
- **ç”Ÿæˆå¼é—®ç­”**: ç”Ÿæˆè‡ªç„¶è¯­è¨€ç­”æ¡ˆ
- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒå¤šè¯­è¨€é—®ç­”æ•°æ®é›†
- **åå¤„ç†**: ç­”æ¡ˆåå¤„ç†å’Œè¯„åˆ†

#### ä½¿ç”¨ç¤ºä¾‹
```bash
# SQuADè®­ç»ƒ
python run_qa.py \
  --model_name_or_path bert-base-uncased \
  --train_file squad-v2/train-v2.0.json \
  --validation_file squad-v2/dev-v2.0.json \
  --do_train \
  --do_eval \
  --version_2_with_negative \
  --learning_rate 3e-5 \
  --num_train_epochs 2 \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir /tmp/squad/
```

### 4. å›¾åƒåˆ†ç±» (image-classification)

#### æ¦‚è¿°
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ViTã€DeiTã€ConvNeXtç­‰æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚

#### æ ¸å¿ƒæ–‡ä»¶
```
image-classification/
â”œâ”€â”€ run_image_classification.py                # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ README.md                                  # è¯¦ç»†è¯´æ˜
â””â”€â”€ requirements.txt                           # ä¾èµ–åŒ…
```

#### å…³é”®ç‰¹æ€§
- **å¤šæ¨¡å‹æ”¯æŒ**: ViTã€DeiTã€ConvNeXtã€ResNetç­‰
- **æ•°æ®å¢å¼º**: ä¸°å¯Œçš„å›¾åƒå¢å¼ºæŠ€æœ¯
- **è¿ç§»å­¦ä¹ **: æ”¯æŒé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ
- **è¯„ä¼°æŒ‡æ ‡**: Top-1ã€Top-5å‡†ç¡®ç‡ç­‰

### 5. è¯­éŸ³è¯†åˆ« (speech-recognition)

#### æ¦‚è¿°
å±•ç¤ºWhisperã€Wav2Vec2ç­‰æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

#### æ ¸å¿ƒæ–‡ä»¶
```
speech-recognition/
â”œâ”€â”€ run_speech_recognition_ctc.py              # CTCæ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ run_speech_recognition_seq2seq.py          # Seq2Seqæ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ run_asr.py                                 # Whisperç¤ºä¾‹
â””â”€â”€ README.md                                  # è¯¦ç»†è¯´æ˜
```

## é«˜çº§åŠŸèƒ½å’Œä¼˜åŒ–

### 1. åˆ†å¸ƒå¼è®­ç»ƒ

#### å¤šGPUè®­ç»ƒ
```bash
# ä½¿ç”¨torch.distributed
python -m torch.distributed.launch \
  --nproc_per_node=NUM_GPUS \
  --nnodes=NUM_NODES \
  --node_rank=NODE_RANK \
  --master_addr=MASTER_ADDR \
  --master_port=MASTER_PORT \
  your_script.py

# ä½¿ç”¨accelerate
accelerate config
accelerate launch your_script.py
```

#### DeepSpeedé›†æˆ
```bash
# DeepSpeed ZeRO
deepspeed --num_gpus=8 your_script.py \
  --deepspeed_config ds_config.json
```

### 2. æ··åˆç²¾åº¦è®­ç»ƒ

#### è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)
```bash
# å¯ç”¨FP16
python your_script.py \
  --fp16 \
  --fp16_opt_level O1

# å¯ç”¨BF16
python your_script.py \
  --bf16
```

### 3. å†…å­˜ä¼˜åŒ–

#### æ¢¯åº¦æ£€æŸ¥ç‚¹
```bash
python your_script.py \
  --gradient_checkpointing \
  --gradient_checkpointing_kwargs "use_reentrant=False"
```

#### é‡åŒ–è®­ç»ƒ
```python
# 8ä½ä¼˜åŒ–å™¨
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config
)
```

### 4. æ•°æ®å¤„ç†ä¼˜åŒ–

#### ç¼“å­˜ç­–ç•¥
```python
# æ•°æ®é›†ç¼“å­˜
from datasets import load_dataset

dataset = load_dataset(
    "json",
    data_files="data.json",
    cache_dir="/path/to/cache"
)
```

#### æµå¼å¤„ç†
```python
# æµå¼æ•°æ®åŠ è½½
dataset = load_dataset(
    "json",
    data_files="large_data.json",
    streaming=True
)
```

## é…ç½®å’Œè°ƒä¼˜

### 1. è®­ç»ƒå‚æ•°

#### ä¼˜åŒ–å™¨è®¾ç½®
```bash
# AdamWä¼˜åŒ–å™¨
python your_script.py \
  --optim adamw_torch \
  --learning_rate 5e-5 \
  --weight_decay 0.01 \
  --adam_beta1 0.9 \
  --adam_beta2 0.999 \
  --adam_epsilon 1e-8
```

#### å­¦ä¹ ç‡è°ƒåº¦
```bash
# çº¿æ€§è¡°å‡
python your_script.py \
  --lr_scheduler_type linear \
  --warmup_steps 500 \
  --max_steps 10000

# ä½™å¼¦é€€ç«
python your_script.py \
  --lr_scheduler_type cosine \
  --warmup_steps 500 \
  --max_steps 10000
```

### 2. è¯„ä¼°å’ŒéªŒè¯

#### è¯„ä¼°ç­–ç•¥
```bash
# æ¯ä¸ªepochéªŒè¯
python your_script.py \
  --evaluation_strategy epoch \
  --eval_steps 500 \
  --metric_for_best_model eval_loss \
  --greater_is_better False
```

#### æ—©åœæœºåˆ¶
```bash
# æ—©åœé…ç½®
python your_script.py \
  --early_stopping True \
  --early_stopping_patience 3 \
  --load_best_model_at_end True
```

## æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†

### 1. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

#### ä¿å­˜æ¨¡å‹
```bash
python your_script.py \
  --output_dir ./results \
  --save_steps 1000 \
  --save_total_limit 3 \
  --save_strategy steps
```

#### æ¨ç†ä¼˜åŒ–
```python
# æ¨¡å‹é‡åŒ–
from transformers import AutoModelForSequenceClassification
import torch

model = AutoModelForSequenceClassification.from_pretrained(
    "./results",
    torch_dtype=torch.float16
)

# ONNXå¯¼å‡º
from transformers import AutoTokenizer
import onnxruntime as ort

tokenizer = AutoTokenizer.from_pretrained("./results")
# å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼
```

### 2. ç”Ÿäº§éƒ¨ç½²

#### APIæœåŠ¡
```python
from fastapi import FastAPI
from transformers import pipeline

app = FastAPI()
classifier = pipeline("sentiment-analysis", model="./results")

@app.post("/predict")
async def predict(text: str):
    result = classifier(text)
    return {"prediction": result[0]}
```

## ç›‘æ§å’Œè°ƒè¯•

### 1. è®­ç»ƒç›‘æ§

#### Wandbé›†æˆ
```bash
# å®‰è£…wandb
pip install wandb

# å¯ç”¨wandb
python your_script.py \
  --report_to wandb \
  --project_name my_project \
  --run_name experiment_1
```

#### TensorBoard
```bash
# å¯ç”¨TensorBoard
python your_script.py \
  --report_to tensorboard \
  --logging_dir ./logs

# å¯åŠ¨TensorBoard
tensorboard --logdir ./logs
```

### 2. é”™è¯¯å¤„ç†

#### è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python your_script.py \
  --logging_level debug \
  --log_level debug

# å‡å°‘æ•°æ®é‡è¿›è¡Œå¿«é€Ÿæµ‹è¯•
python your_script.py \
  --max_train_samples 100 \
  --max_eval_samples 50
```

## æœ€ä½³å®è·µ

### 1. æ•°æ®å‡†å¤‡

#### æ•°æ®é¢„å¤„ç†
```python
# ç»Ÿä¸€æ•°æ®æ ¼å¼
from datasets import Dataset

def preprocess_function(examples):
    # æ–‡æœ¬é¢„å¤„ç†
    examples["text"] = [text.lower() for text in examples["text"]]
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    examples["text"] = [re.sub(r"[^a-zA-Z0-9\s]", "", text) for text in examples["text"]]
    return examples

dataset = Dataset.from_dict(raw_data)
dataset = dataset.map(preprocess_function, batched=True)
```

#### æ•°æ®å¢å¼º
```python
# æ–‡æœ¬å¢å¼º
import nlpaug.augmenter.word as naw

aug = naw.SynonymAug(aug_src='wordnet', aug_p=0.1)
augmented_text = aug.augment(original_text)
```

### 2. è¶…å‚æ•°è°ƒä¼˜

#### ç½‘æ ¼æœç´¢
```bash
# ä½¿ç”¨Ray Tune
pip install ray[tune]

python your_script.py \
  --hp_search_backend ray \
  --hp_space config/hp_space.json
```

#### è´å¶æ–¯ä¼˜åŒ–
```python
# ä½¿ç”¨Optuna
pip install optuna

python your_script.py \
  --hp_search_backend optuna \
  --hp_space config/hp_space.json
```

### 3. æ¨¡å‹é€‰æ‹©

#### æ¶æ„å¯¹æ¯”
```python
# å¯¹æ¯”ä¸åŒæ¨¡å‹
models = [
    "bert-base-uncased",
    "roberta-base",
    "distilbert-base-uncased",
    "albert-base-v2"
]

for model_name in models:
    # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
    results = train_and_evaluate(model_name)
    print(f"{model_name}: {results['accuracy']}")
```

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼Ÿ
A: ç­–ç•¥ï¼š
- ä½¿ç”¨æµå¼å¤„ç†
- æ•°æ®åˆ†å—å¤„ç†
- æ¢¯åº¦ç´¯ç§¯
- åˆ†å¸ƒå¼è®­ç»ƒ

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ï¼Ÿ
A: æ–¹æ³•ï¼š
- å­¦ä¹ ç‡èŒƒå›´æµ‹è¯•
- ä½™å¼¦é€€ç«è°ƒåº¦
- é¢„çƒ­é˜¶æ®µ
- è‡ªé€‚åº”è°ƒæ•´

### Q: å¦‚ä½•é¿å…è¿‡æ‹Ÿåˆï¼Ÿ
A: æŠ€æœ¯ï¼š
- æ•°æ®å¢å¼º
- Dropoutæ­£åˆ™åŒ–
- æƒé‡è¡°å‡
- æ—©åœæœºåˆ¶

### Q: å¦‚ä½•ä¼˜åŒ–æ¨ç†é€Ÿåº¦ï¼Ÿ
A: ä¼˜åŒ–æ–¹æ³•ï¼š
- æ¨¡å‹é‡åŒ–
- æ‰¹å¤„ç†
- æ¨¡å‹è’¸é¦
- ONNXå¯¼å‡º

## ç›¸å…³æ–‡ä»¶æ¸…å•

### PyTorchç¤ºä¾‹
- `pytorch/language-modeling/`: è¯­è¨€å»ºæ¨¡ç¤ºä¾‹
- `pytorch/text-classification/`: æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹
- `pytorch/question-answering/`: é—®ç­”ä»»åŠ¡ç¤ºä¾‹
- `pytorch/image-classification/`: å›¾åƒåˆ†ç±»ç¤ºä¾‹
- `pytorch/speech-recognition/`: è¯­éŸ³è¯†åˆ«ç¤ºä¾‹

### æ—§ç‰ˆç¤ºä¾‹
- `legacy/seq2seq/`: åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
- `legacy/pytorch-lightning/`: PyTorch Lightningé›†æˆ
- `legacy/benchmarking/`: æ€§èƒ½åŸºå‡†æµ‹è¯•

### è¾…åŠ©è„šæœ¬
- `3D_parallel.py`: 3Då¹¶è¡Œå¤„ç†
- `run_on_remote.py`: è¿œç¨‹è®­ç»ƒæ”¯æŒ
- `continuous_batching.py`: è¿ç»­æ‰¹å¤„ç†

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - è¯¦ç»†åˆ†æ
- âœ¨ å®ŒæˆExamplesæ¨¡å—ç»“æ„åˆ†æ
- ğŸ” è®°å½•æ ¸å¿ƒä»»åŠ¡ç¤ºä¾‹å’Œæœ€ä½³å®è·µ
- ğŸ“Š åˆ†æé…ç½®å‚æ•°å’Œä¼˜åŒ–ç­–ç•¥
- ğŸ¯ æä¾›å®Œæ•´çš„ä½¿ç”¨æŒ‡å—å’Œéƒ¨ç½²æ–¹æ¡ˆ

### ä¸‹ä¸€æ­¥è®¡åˆ’
- [ ] åˆ›å»ºç‰¹å®šä»»åŠ¡çš„å¿«é€Ÿå¼€å§‹æŒ‡å—
- [ ] è®°å½•æ€§èƒ½è°ƒä¼˜çš„è¯¦ç»†æ¡ˆä¾‹
- [ ] åˆ†æä¸åŒç¡¬ä»¶ä¸Šçš„æœ€ä½³é…ç½®
- [ ] åˆ›å»ºç”Ÿäº§éƒ¨ç½²çš„æœ€ä½³å®è·µæ–‡æ¡£

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 90%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 95%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20