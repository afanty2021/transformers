[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > [src](/Users/berton/Github/transformers/src/CLAUDE.md) > [transformers](/Users/berton/Github/transformers/src/transformers/CLAUDE.md) > [models](/Users/berton/Github/transformers/src/transformers/models/CLAUDE.md) > **gpt2**

# GPT-2 æ¨¡å‹æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `src/transformers/models/gpt2/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 95%

## æ¨¡å—èŒè´£

GPT-2 (Generative Pre-trained Transformer 2) æ˜¯OpenAIå¼€å‘çš„å¤§å‹è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚ä¸BERTä¸åŒï¼ŒGPT-2é‡‡ç”¨å•å‘ï¼ˆä»å·¦åˆ°å³ï¼‰çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œéå¸¸é€‚åˆç”Ÿæˆå¼ä»»åŠ¡ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **è‡ªå›å½’ç”Ÿæˆ**: ä½¿ç”¨å› æœæ³¨æ„åŠ›æ©ç ï¼Œä»å·¦åˆ°å³ç”Ÿæˆæ–‡æœ¬
- **å¤§è§„æ¨¡é¢„è®­ç»ƒ**: åœ¨1500äº¿tokensçš„äº’è”ç½‘æ–‡æœ¬ä¸Šè®­ç»ƒ
- **é›¶æ ·æœ¬èƒ½åŠ›**: æ— éœ€å¾®è°ƒå³å¯åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½
- **å¤šå°ºåº¦æ¨¡å‹**: ä»117Måˆ°1.5Bå‚æ•°çš„ä¸åŒè§„æ¨¡ç‰ˆæœ¬

## æ–‡ä»¶ç»“æ„

```
gpt2/
â”œâ”€â”€ __init__.py                                    # æ¨¡å—å¯¼å‡ºå’Œæ¨¡å‹æ˜ å°„
â”œâ”€â”€ configuration_gpt2.py                          # GPT2Configé…ç½®ç±»
â”œâ”€â”€ modeling_gpt2.py                              # æ ¸å¿ƒæ¨¡å‹å®ç°
â”œâ”€â”€ tokenization_gpt2.py                          # GPT-2åˆ†è¯å™¨ï¼ˆBPEï¼‰
â”œâ”€â”€ tokenization_gpt2_fast.py                     # Fast GPT-2åˆ†è¯å™¨
â”œâ”€â”€ convert_gpt2_original_tf_checkpoint_to_pytorch.py  # TensorFlowæƒé‡è½¬æ¢
â””â”€â”€ CONVERSION.md                                 # è½¬æ¢è¯´æ˜æ–‡æ¡£
```

## æ ¸å¿ƒç»„ä»¶åˆ†æ

### 1. é…ç½®ç±» (GPT2Config)

```python
class GPT2Config(PreTrainedConfig):
    model_type = "gpt2"

    def __init__(
        self,
        vocab_size=50257,              # è¯æ±‡è¡¨å¤§å°ï¼ˆåŒ…å«ç‰¹æ®Štokenï¼‰
        n_positions=1024,              # æœ€å¤§åºåˆ—é•¿åº¦
        n_embd=768,                    # åµŒå…¥ç»´åº¦
        n_layer=12,                    # Transformerå±‚æ•°
        n_head=12,                     # æ³¨æ„åŠ›å¤´æ•°
        n_inner=None,                  # å‰é¦ˆç½‘ç»œå†…å±‚ç»´åº¦ï¼ˆé»˜è®¤ä¸º4*n_embdï¼‰
        activation_function="gelu",    # æ¿€æ´»å‡½æ•°
        resid_pdrop=0.1,               # æ®‹å·®dropout
        embd_pdrop=0.1,                # åµŒå…¥dropout
        attn_pdrop=0.1,                # æ³¨æ„åŠ›dropout
        layer_norm_epsilon=1e-5,       # LayerNorm epsilon
        initializer_range=0.02,        # åˆå§‹åŒ–èŒƒå›´
        summary_type="cls_token",      # æ±‡æ€»ç±»å‹
        summary_use_proj=True,         # æ˜¯å¦ä½¿ç”¨æŠ•å½±å±‚
        summary_activation=None,       # æ±‡æ€»æ¿€æ´»å‡½æ•°
        summary_proj_to_labels=True,   # æ˜¯å¦æŠ•å½±åˆ°æ ‡ç­¾ç©ºé—´
        summary_first_dropout=0.1,     # ç¬¬ä¸€ä¸ªdropout
        scale_attn_weights=True,       # æ˜¯å¦ç¼©æ”¾æ³¨æ„åŠ›æƒé‡
        use_cache=True,                # æ˜¯å¦ä½¿ç”¨KVç¼“å­˜
        bos_token_id=50256,            # BOS token ID
        eos_token_id=50256,            # EOS token ID
        **kwargs
    ):
        super().__init__(**kwargs)
        # å‚æ•°èµ‹å€¼...
```

**å…³é”®é…ç½®å‚æ•°**:
- `vocab_size`: GPT-2ä½¿ç”¨BPEåˆ†è¯ï¼ŒåŒ…å«50257ä¸ªtokens
- `n_positions`: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé»˜è®¤1024
- `n_embd`: æ¨¡å‹çš„åŸºç¡€ç»´åº¦
- `n_layer`: Transformerå—çš„æ•°é‡
- `scale_attn_weights`: æ˜¯å¦å¯¹æ³¨æ„åŠ›æƒé‡è¿›è¡Œç¼©æ”¾

### 2. æ ¸å¿ƒæ¨¡å‹ç»„ä»¶

#### GPT2Attention - å› æœæ³¨æ„åŠ›æœºåˆ¶
```python
class GPT2Attention(nn.Module):
    def __init__(self, config, is_cross_attention=False):
        super().__init__()
        max_positions = config.n_positions
        self.register_buffer(
            "bias",
            torch.tril(torch.ones(max_positions, max_positions)).view(
                1, 1, max_positions, max_positions
            ),
        )
        self.register_buffer("masked_bias", torch.tensor(-1e9))

        self.embed_dim = config.n_embd
        self.num_heads = config.n_head
        self.head_dim = self.embed_dim // self.num_heads
        self.split_size = self.embed_dim

        # Q, K, Vçº¿æ€§å˜æ¢ï¼ˆåˆå¹¶ä¸ºå•ä¸ªæƒé‡çŸ©é˜µä»¥æé«˜æ•ˆç‡ï¼‰
        self.c_attn = nn.Linear(self.embed_dim, 3 * self.embed_dim)
        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)

        # æ³¨æ„åŠ›dropout
        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        self.resid_dropout = nn.Dropout(config.resid_pdrop)
```

**æ ¸å¿ƒæœºåˆ¶**:
- **å› æœæ©ç **: ä½¿ç”¨ä¸‹ä¸‰è§’çŸ©é˜µç¡®ä¿æ¯ä¸ªtokenåªèƒ½çœ‹åˆ°å‰é¢çš„token
- **åˆå¹¶QKV**: å°†Qã€Kã€Vçš„çº¿æ€§å˜æ¢åˆå¹¶ä¸ºä¸€ä¸ªçŸ©é˜µï¼Œæé«˜è®¡ç®—æ•ˆç‡
- **å¤šå¤´æ³¨æ„åŠ›**: å°†æ³¨æ„åŠ›åˆ†æˆå¤šä¸ª"å¤´"æ•è·ä¸åŒçš„ä¾èµ–å…³ç³»
- **æƒé‡ç¼©æ”¾**: é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œç¨³å®šè®­ç»ƒ

#### GPT2MLP - å‰é¦ˆç½‘ç»œ
```python
class GPT2MLP(nn.Module):
    def __init__(self, intermediate_size, config):
        super().__init__()
        self.c_fc = nn.Linear(intermediate_size, 4 * intermediate_size)
        self.c_proj = nn.Linear(4 * intermediate_size, intermediate_size)
        self.act = ACT2FN[config.activation_function]
        self.dropout = nn.Dropout(config.resid_pdrop)
```

**ç‰¹ç‚¹**:
- æ‰©å±•å› å­ä¸º4ï¼šä¸­é—´å±‚ç»´åº¦æ˜¯è¾“å…¥çš„4å€
- GELUæ¿€æ´»å‡½æ•°ï¼šå¹³æ»‘çš„ReLUå˜ä½“
- æ®‹å·®è¿æ¥ï¼šé€šè¿‡dropoutå®ç°

#### GPT2Block - Transformerå—
```python
class GPT2Block(GradientCheckpointingLayer):
    def __init__(self, config, layer_idx=None):
        super().__init__()
        hidden_size = config.n_embd
        self.inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size

        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        self.attn = GPT2Attention(config, layer_idx=layer_idx)
        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)

        # äº¤å‰æ³¨æ„åŠ›ï¼ˆå¯é€‰ï¼‰
        if config.add_cross_attention:
            self.crossattention = GPT2Attention(config, is_cross_attention=True, layer_idx=layer_idx)
            self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)

        self.mlp = GPT2MLP(hidden_size, config)
```

**ç»“æ„**:
- Pre-LNç»“æ„ï¼šLayerNormåœ¨å­å±‚ä¹‹å‰
- è‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ
- å¯é€‰çš„äº¤å‰æ³¨æ„åŠ›æ”¯æŒ
- æ¢¯åº¦æ£€æŸ¥ç‚¹æ”¯æŒ

### 3. ä»»åŠ¡ç‰¹å®šæ¨¡å‹

#### GPT2LMHeadModel - è¯­è¨€æ¨¡å‹
```python
class GPT2LMHeadModel(GPT2PreTrainedModel, GenerationMixin):
    def __init__(self, config):
        super().__init__(config)
        self.transformer = GPT2Model(config)
        # è¯­è¨€æ¨¡å‹å¤´éƒ¨
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # æƒé‡ç»‘å®š
        self.lm_head.weight = self.transformer.wte.weight
```

**åŠŸèƒ½**:
- è‡ªå›å½’è¯­è¨€å»ºæ¨¡
- æ”¯æŒæ–‡æœ¬ç”Ÿæˆ
- æƒé‡ç»‘å®šå‡å°‘å‚æ•°

#### GPT2ForSequenceClassification - åºåˆ—åˆ†ç±»
```python
class GPT2ForSequenceClassification(GPT2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.transformer = GPT2Model(config)
        # åˆ†ç±»å™¨å¤´éƒ¨
        self.score = nn.Linear(config.n_embd, self.num_labels, bias=False)

        # æƒé‡åˆå§‹åŒ–
        self.post_init()
```

**ç‰¹ç‚¹**:
- ä½¿ç”¨æœ€åä¸€ä¸ªtokençš„è¡¨ç¤ºè¿›è¡Œåˆ†ç±»
- æ”¯æŒå¤šç±»åˆ«åˆ†ç±»
- å¯é€‰çš„æ± åŒ–ç­–ç•¥

#### GPT2DoubleHeadsModel - åŒå¤´æ¨¡å‹
```python
class GPT2DoubleHeadsModel(GPT2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        config.num_labels = 1
        self.transformer = GPT2Model(config)
        # LMå¤´
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # å¤šé€‰æ‹©åˆ†ç±»å¤´
        self.multiple_choice_head = nn.Linear(config.n_embd, 1, bias=False)
```

**åŠŸèƒ½**:
- åŒæ—¶æ”¯æŒè¯­è¨€å»ºæ¨¡å’Œåˆ†ç±»
- é€‚ç”¨äºå¤šé€‰ä»»åŠ¡
- å…±äº«Transformerç¼–ç å™¨

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€æ–‡æœ¬ç”Ÿæˆ
```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)

# ç¼–ç è¾“å…¥
prompt = "The future of artificial intelligence is"
inputs = tokenizer(prompt, return_tensors="pt")

# ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(
    inputs.input_ids,
    max_length=100,
    num_return_sequences=1,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    do_sample=True
)

# è§£ç è¾“å‡º
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

### 2. æ¡ä»¶ç”Ÿæˆ
```python
# è®¾ç½®ä¸åŒçš„ç”Ÿæˆç­–ç•¥
outputs = model.generate(
    inputs.input_ids,
    max_length=200,
    num_beams=5,              # æŸæœç´¢
    no_repeat_ngram_size=2,   # é¿å…é‡å¤n-gram
    early_stopping=True,      # æ—©åœ
    length_penalty=1.2,       # é•¿åº¦æƒ©ç½š
)
```

### 3. æ‰¹é‡ç”Ÿæˆ
```python
prompts = [
    "Once upon a time",
    "In a galaxy far away",
    "The meaning of life is"
]

inputs = tokenizer(prompts, padding=True, return_tensors="pt")
outputs = model.generate(
    inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_length=100,
    do_sample=True,
    temperature=0.8
)
```

### 4. è‡ªå®šä¹‰é…ç½®
```python
from transformers import GPT2Config, GPT2LMHeadModel

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = GPT2Config(
    vocab_size=50000,
    n_positions=2048,      # æ›´é•¿çš„ä¸Šä¸‹æ–‡
    n_embd=1024,           # æ›´å¤§çš„æ¨¡å‹
    n_layer=24,            # æ›´æ·±çš„ç½‘ç»œ
    n_head=16
)

# åˆ›å»ºæ¨¡å‹
model = GPT2LMHeadModel(config)
```

### 5. å¾®è°ƒç¤ºä¾‹
```python
from transformers import TextDataset, DataCollatorForLanguageModeling

# å‡†å¤‡æ•°æ®é›†
dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="train.txt",
    block_size=128
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # GPT-2ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡ï¼Œä¸æ˜¯æ©ç è¯­è¨€å»ºæ¨¡
)

# è®­ç»ƒ
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    save_steps=500,
    save_total_limit=2,
    prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()
```

## ç”Ÿæˆç­–ç•¥

### 1. é‡‡æ ·ç­–ç•¥
```python
# æ¸©åº¦é‡‡æ ·
outputs = model.generate(
    inputs.input_ids,
    do_sample=True,
    temperature=0.7,  # æ§åˆ¶éšæœºæ€§ï¼Œè¶Šé«˜è¶Šéšæœº
    top_k=50,        # é™åˆ¶å€™é€‰è¯æ•°é‡
    top_p=0.95,      # æ ¸é‡‡æ ·ï¼Œç´¯ç§¯æ¦‚ç‡é˜ˆå€¼
)

# ç¡®å®šæ€§é‡‡æ ·
outputs = model.generate(
    inputs.input_ids,
    do_sample=False,
    num_beams=5,     # æŸæœç´¢
    early_stopping=True
)
```

### 2. è´¨é‡æ§åˆ¶
```python
# é¿å…é‡å¤
outputs = model.generate(
    inputs.input_ids,
    no_repeat_ngram_size=2,  # é¿å…é‡å¤2-gram
    repetition_penalty=1.5,  # é‡å¤æƒ©ç½š
)

# é•¿åº¦æ§åˆ¶
outputs = model.generate(
    inputs.input_ids,
    min_length=50,           # æœ€å°é•¿åº¦
    max_length=200,          # æœ€å¤§é•¿åº¦
    length_penalty=1.2,      # é•¿åº¦æƒ©ç½š
)
```

### 3. å¤šæ ·æ€§æ§åˆ¶
```python
# å¤šæ ·æ€§æŸæœç´¢
outputs = model.generate(
    inputs.input_ids,
    num_beams=10,
    num_beam_groups=3,      # æŸç»„æ•°
    diversity_penalty=1.0,  # å¤šæ ·æ€§æƒ©ç½š
    num_return_sequences=3  # è¿”å›å¤šä¸ªç»“æœ
)
```

## æ€§èƒ½ä¼˜åŒ–

### 1. KVç¼“å­˜ä¼˜åŒ–
```python
# å¯ç”¨KVç¼“å­˜ï¼ˆé»˜è®¤å¼€å¯ï¼‰
model = GPT2LMHeadModel.from_pretrained("gpt2", use_cache=True)

# ç”Ÿæˆæ—¶é‡ç”¨ç¼“å­˜
past_key_values = None
for _ in range(max_new_tokens):
    outputs = model(
        input_ids,
        past_key_values=past_key_values,
        use_cache=True
    )
    past_key_values = outputs.past_key_values
    # å¤„ç†è¾“å‡º...
```

### 2. é‡åŒ–ä¼˜åŒ–
```python
# 8ä½é‡åŒ–
model = GPT2LMHeadModel.from_pretrained(
    "gpt2",
    load_in_8bit=True,
    device_map="auto"
)

# 4ä½é‡åŒ–
model = GPT2LMHeadModel.from_pretrained(
    "gpt2",
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
```

### 3. Flash Attention
```python
# å¯ç”¨Flash Attention 2
model = GPT2LMHeadModel.from_pretrained(
    "gpt2",
    use_flash_attention_2=True,
    torch_dtype=torch.float16
)
```

## æ¨¡å‹å˜ä½“

### 1. GPT-2æ¨¡å‹è§„æ¨¡
- **gpt2**: 117Må‚æ•°ï¼ŒåŸºç¡€ç‰ˆæœ¬
- **gpt2-medium**: 345Må‚æ•°ï¼Œä¸­ç­‰è§„æ¨¡
- **gpt2-large**: 774Må‚æ•°ï¼Œå¤§è§„æ¨¡
- **gpt2-xl**: 1.5Bå‚æ•°ï¼Œè¶…å¤§è§„æ¨¡

### 2. ç›¸å…³æ¨¡å‹
- **GPT-3**: æ›´å¤§çš„175Bå‚æ•°æ¨¡å‹
- **GPT-Neo**: EleutherAIçš„å¼€æºå®ç°
- **GPT-J**: 6Bå‚æ•°çš„ç±»GPTæ¨¡å‹

## æœ€ä½³å®è·µ

### 1. æç¤ºå·¥ç¨‹
```python
# ç»“æ„åŒ–æç¤º
prompt = """
Question: What is the capital of France?
Answer: The capital of France is Paris.

Question: Who wrote Romeo and Juliet?
Answer:
"""

# Few-shotç¤ºä¾‹
prompt = """
Translate English to French:
sea -> mer
car -> voiture
house -> maison
computer ->
"""

# æ€ç»´é“¾æç¤º
prompt = """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?
A: Let's think step by step.
Step 1: Roger starts with 5 balls.
Step 2: He buys 2 cans with 3 balls each, so 2 Ã— 3 = 6 balls.
Step 3: Total = 5 + 6 = 11 balls.
The answer is 11.
"""
```

### 2. åå¤„ç†
```python
import re

def clean_generated_text(text):
    # ç§»é™¤é‡å¤å†…å®¹
    text = re.sub(r'(.{10,}?)\1+', r'\1', text)

    # æˆªæ–­åˆ°ç¬¬ä¸€ä¸ªå¥å·æˆ–æ¢è¡Œ
    first_sentence = text.split('.')[0] + '.'
    if len(first_sentence) > len(text) * 0.3:
        text = first_sentence

    return text.strip()

generated_text = clean_generated_text(generated_text)
```

### 3. è¯„ä¼°æŒ‡æ ‡
```python
# å›°æƒ‘åº¦è®¡ç®—
import torch
import math

def calculate_perplexity(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        perplexity = math.exp(loss)
    return perplexity

# BLEUåˆ†æ•°è®¡ç®—
from nltk.translate.bleu_score import sentence_bleu

def calculate_bleu(reference, candidate):
    reference = [reference.split()]
    candidate = candidate.split()
    return sentence_bleu(reference, candidate)
```

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•é¿å…ç”Ÿæˆé‡å¤å†…å®¹ï¼Ÿ
A: ä½¿ç”¨ä»¥ä¸‹æŠ€æœ¯ï¼š
- è®¾ç½®`no_repeat_ngram_size=2`
- å¢åŠ `repetition_penalty`
- é™ä½`temperature`
- ä½¿ç”¨æŸæœç´¢è€Œéé‡‡æ ·

### Q: å¦‚ä½•æé«˜ç”Ÿæˆè´¨é‡ï¼Ÿ
A: æŠ€å·§åŒ…æ‹¬ï¼š
- æ›´å¥½çš„æç¤ºè®¾è®¡
- è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼ˆtemperature, top_p, top_kï¼‰
- ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
- å¾®è°ƒåœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Š

### Q: å¦‚ä½•æ§åˆ¶ç”Ÿæˆé•¿åº¦ï¼Ÿ
A: æ–¹æ³•ï¼š
- è®¾ç½®`max_length`æˆ–`max_new_tokens`
- ä½¿ç”¨`early_stopping=True`
- è°ƒæ•´`length_penalty`

### Q: å¦‚ä½•å®ç°æµå¼ç”Ÿæˆï¼Ÿ
A: ä½¿ç”¨generateçš„æµå¼APIæˆ–è‡ªå®šä¹‰å¾ªç¯ï¼š
```python
def stream_generate(model, tokenizer, prompt, max_length=100):
    inputs = tokenizer(prompt, return_tensors="pt")
    generated_ids = inputs["input_ids"].clone()

    for _ in range(max_length):
        outputs = model(generated_ids)
        next_token_logits = outputs.logits[:, -1, :]
        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)
        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)

        yield tokenizer.decode(next_token_id[0], skip_special_tokens=True)

        if next_token_id.item() == tokenizer.eos_token_id:
            break
```

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ–‡ä»¶
- `modeling_gpt2.py`: 1265è¡Œï¼ŒåŒ…å«å®Œæ•´çš„GPT-2å®ç°
- `configuration_gpt2.py`: GPT2Configé…ç½®ç±»
- `tokenization_gpt2.py`: BPEåˆ†è¯å™¨å®ç°
- `tokenization_gpt2_fast.py`: åŸºäºRustçš„å¿«é€Ÿåˆ†è¯å™¨

### è½¬æ¢è„šæœ¬
- `convert_gpt2_original_tf_checkpoint_to_pytorch.py`: TensorFlowåˆ°PyTorchè½¬æ¢
- `CONVERSION.md`: è½¬æ¢è¯´æ˜æ–‡æ¡£

### æµ‹è¯•æ–‡ä»¶
- `tests/test_modeling_gpt2.py`: GPT-2æ¨¡å‹æµ‹è¯•
- `tests/test_tokenization_gpt2.py`: åˆ†è¯å™¨æµ‹è¯•

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - è¯¦ç»†åˆ†æ
- âœ¨ å®ŒæˆGPT-2æ¨¡å‹æ ¸å¿ƒç»„ä»¶åˆ†æ
- ğŸ” è®°å½•æ‰€æœ‰ç”Ÿæˆç­–ç•¥å’ŒæŠ€å·§
- ğŸ“Š åˆ†æé…ç½®å‚æ•°å’Œæœ€ä½³å®è·µ
- ğŸ¯ æä¾›å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹å’Œä¼˜åŒ–æ–¹æ³•

### ä¸‹ä¸€æ­¥è®¡åˆ’
- [ ] åˆ†æGPT-2åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„åº”ç”¨
- [ ] åˆ›å»ºæç¤ºå·¥ç¨‹æœ€ä½³å®è·µæ–‡æ¡£
- [ ] è®°å½•GPT-2å˜ä½“çš„æ€§èƒ½å¯¹æ¯”
- [ ] åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œåè§é—®é¢˜

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 95%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 98%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20