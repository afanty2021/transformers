[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > [src](/Users/berton/Github/transformers/src/CLAUDE.md) > [transformers](/Users/berton/Github/transformers/src/transformers/CLAUDE.md) > [models](/Users/berton/Github/transformers/src/transformers/models/CLAUDE.md) > **vit**

# ViT (Vision Transformer) æ¨¡å‹æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `src/transformers/models/vit/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 95%

## æ¨¡å—èŒè´£

ViT (Vision Transformer) æ˜¯Googleæå‡ºçš„çº¯Transformeræ¶æ„çš„è§†è§‰æ¨¡å‹ï¼Œå°†å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„å—ï¼Œç„¶ååƒå¤„ç†åºåˆ—ä¸€æ ·å¤„ç†è¿™äº›å—ã€‚ViTè¯æ˜äº†Transformeræ¶æ„åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæˆä¸ºäº†ç°ä»£è§†è§‰æ¨¡å‹çš„åŸºç¡€æ¶æ„ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **çº¯Transformeræ¶æ„**: å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸ä½¿ç”¨å·ç§¯
- **å›¾åƒå—åˆ†å‰²**: å°†å›¾åƒè½¬æ¢ä¸ºåºåˆ—çš„patch
- **ä½ç½®ç¼–ç **: ä¿æŒå›¾åƒçš„ç©ºé—´ç»“æ„ä¿¡æ¯
- **å¤§è§„æ¨¡é¢„è®­ç»ƒ**: åœ¨å¤§è§„æ¨¡å›¾åƒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒ
- **å¼ºå¤§çš„è¿ç§»èƒ½åŠ›**: åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚

## æ–‡ä»¶ç»“æ„

```
vit/
â”œâ”€â”€ __init__.py                                    # æ¨¡å—å¯¼å‡ºå’Œæ¨¡å‹æ˜ å°„
â”œâ”€â”€ configuration_vit.py                          # ViTConfigé…ç½®ç±»
â”œâ”€â”€ modeling_vit.py                              # æ ¸å¿ƒæ¨¡å‹å®ç°
â”œâ”€â”€ image_processing_vit.py                      # å›¾åƒé¢„å¤„ç†å™¨
â”œâ”€â”€ image_processing_vit_fast.py                 # å¿«é€Ÿå›¾åƒå¤„ç†å™¨
â”œâ”€â”€ convert_dino_to_pytorch.py                   # DINOåˆ°PyTorchè½¬æ¢
â””â”€â”€ convert_vit_timm_to_pytorch.py               # timmæ¨¡å‹è½¬æ¢
```

## æ ¸å¿ƒç»„ä»¶åˆ†æ

### 1. é…ç½®ç±» (ViTConfig)

```python
class ViTConfig(PreTrainedConfig):
    model_type = "vit"

    def __init__(
        self,
        hidden_size=768,                # éšè—å±‚ç»´åº¦
        num_hidden_layers=12,           # Transformerå±‚æ•°
        num_attention_heads=12,         # æ³¨æ„åŠ›å¤´æ•°
        intermediate_size=3072,         # å‰é¦ˆç½‘ç»œç»´åº¦
        hidden_act="gelu",              # æ¿€æ´»å‡½æ•°
        hidden_dropout_prob=0.0,        # éšè—å±‚dropout
        attention_probs_dropout_prob=0.0,  # æ³¨æ„åŠ›dropout
        initializer_range=0.02,         # åˆå§‹åŒ–èŒƒå›´
        layer_norm_eps=1e-12,           # LayerNorm epsilon
        image_size=224,                 # è¾“å…¥å›¾åƒå°ºå¯¸
        patch_size=16,                  # å›¾åƒå—å¤§å°
        num_channels=3,                 # å›¾åƒé€šé“æ•°
        qkv_bias=True,                  # QKVåç½®
        encoder_stride=16,              # ç¼–ç å™¨æ­¥é•¿ï¼ˆç”¨äºåˆ†å‰²ï¼‰
        **kwargs
    ):
        super().__init__(**kwargs)
        # å‚æ•°èµ‹å€¼...
```

**å…³é”®é…ç½®å‚æ•°**:
- `image_size`: è¾“å…¥å›¾åƒçš„æ ‡å‡†å°ºå¯¸
- `patch_size`: æ¯ä¸ªpatchçš„åƒç´ å¤§å°ï¼Œå†³å®šäº†patchæ•°é‡
- `hidden_size`: Transformerçš„éšè—ç»´åº¦
- `num_hidden_layers`: Transformerå—çš„æ•°é‡
- `encoder_stride`: ç”¨äºåˆ†å‰²ä»»åŠ¡çš„ä¸‹é‡‡æ ·ç‡

### 2. æ ¸å¿ƒæ¨¡å‹ç»„ä»¶

#### ViTPatchEmbeddings - å›¾åƒå—åµŒå…¥
```python
class ViTPatchEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        image_size, patch_size = config.image_size, config.patch_size
        num_channels, hidden_size = config.num_channels, config.hidden_size

        # è®¡ç®—patchæ•°é‡
        self.num_patches = (image_size // patch_size) ** 2

        # å°†patchçº¿æ€§æŠ•å½±åˆ°åµŒå…¥ç©ºé—´
        self.projection = nn.Conv2d(
            num_channels, hidden_size,
            kernel_size=patch_size,
            stride=patch_size
        )

    def forward(self, pixel_values):
        batch_size, num_channels, height, width = pixel_values.shape
        # æŠ•å½±åˆ°åµŒå…¥ç©ºé—´
        embeddings = self.projection(pixel_values)
        # é‡æ’ä¸º (batch_size, num_patches, hidden_size)
        embeddings = embeddings.flatten(2).transpose(1, 2)
        return embeddings
```

**æ ¸å¿ƒæœºåˆ¶**:
- **å·ç§¯æŠ•å½±**: ä½¿ç”¨å·ç§¯å°†å›¾åƒå—æŠ•å½±åˆ°åµŒå…¥ç©ºé—´
- **æ‰å¹³åŒ–å¤„ç†**: å°†2Dç‰¹å¾å›¾è½¬æ¢ä¸º1Dåºåˆ—
- **ä½ç½®ä¿æŒ**: ä¿æŒpatchçš„ç©ºé—´é¡ºåº

#### ViTEmbeddings - å®Œæ•´åµŒå…¥å±‚
```python
class ViTEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.patch_embeddings = ViTPatchEmbeddings(config)

        # ç±»åˆ«token
        num_patches = self.patch_embeddings.num_patches
        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))

        # ä½ç½®åµŒå…¥
        self.position_embeddings = nn.Parameter(
            torch.randn(1, num_patches + 1, config.hidden_size)
        )

        # Dropout
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, pixel_values):
        batch_size = pixel_values.shape[0]
        embeddings = self.patch_embeddings(pixel_values)

        # æ·»åŠ ç±»åˆ«token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        embeddings = torch.cat([cls_tokens, embeddings], dim=1)

        # æ·»åŠ ä½ç½®åµŒå…¥
        embeddings = embeddings + self.position_embeddings
        embeddings = self.dropout(embeddings)

        return embeddings
```

**åŠŸèƒ½**:
- **patchåµŒå…¥**: å°†å›¾åƒè½¬æ¢ä¸ºåºåˆ—è¡¨ç¤º
- **ç±»åˆ«token**: å…¨å±€å›¾åƒè¡¨ç¤ºï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡
- **ä½ç½®ç¼–ç **: ä¸ºæ¯ä¸ªpatchæ·»åŠ ä½ç½®ä¿¡æ¯
- **Dropoutæ­£åˆ™åŒ–**

#### ViTSelfAttention - è‡ªæ³¨æ„åŠ›æœºåˆ¶
```python
class ViTSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError("hidden_size must be divisible by num_attention_heads")

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        # QKVçº¿æ€§å˜æ¢
        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)
        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)
        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)

        # Dropout
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        # é‡æ’ä¸ºå¤šå¤´æ³¨æ„åŠ›æ ¼å¼
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states):
        # è®¡ç®—Q, K, V
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)

        # è½¬æ¢ä¸ºå¤šå¤´æ ¼å¼
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)

        # Softmaxå½’ä¸€åŒ–
        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)

        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        context_layer = torch.matmul(attention_probs, value_layer)

        # é‡æ–°ç»„åˆè¾“å‡º
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        return context_layer
```

**æ ¸å¿ƒæœºåˆ¶**:
- **å¤šå¤´æ³¨æ„åŠ›**: æ•è·ä¸åŒç±»å‹çš„ç‰¹å¾å…³ç³»
- **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›**: é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
- **å…¨å±€æ„Ÿå—é‡**: æ¯ä¸ªpatchéƒ½èƒ½ä¸å…¶ä»–æ‰€æœ‰patchäº¤äº’

#### ViTLayer - Transformerå±‚
```python
class ViTLayer(GradientCheckpointingLayer):
    def __init__(self, config):
        super().__init__()
        self.attention = ViTAttention(config)
        self.intermediate = ViTIntermediate(config)
        self.output = ViTOutput(config)
        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, hidden_states):
        # Pre-LNç»“æ„
        attention_output = self.attention(self.layernorm_before(hidden_states))
        hidden_states = attention_output + hidden_states

        # å‰é¦ˆç½‘ç»œ
        layer_output = self.intermediate(self.layernorm_after(hidden_states))
        layer_output = self.output(layer_output) + hidden_states

        return layer_output
```

**ç»“æ„ç‰¹ç‚¹**:
- **Pre-LN**: LayerNormåœ¨å­å±‚ä¹‹å‰ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
- **æ®‹å·®è¿æ¥**: ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **ä½ç½®ç‹¬ç«‹**: æ¯ä¸ªå±‚å¤„ç†æ•´ä¸ªåºåˆ—

### 3. ä»»åŠ¡ç‰¹å®šæ¨¡å‹

#### ViTForImageClassification - å›¾åƒåˆ†ç±»
```python
class ViTForImageClassification(ViTPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.vit = ViTModel(config)
        # åˆ†ç±»å™¨
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        # æƒé‡åˆå§‹åŒ–
        self.post_init()

    def forward(self, pixel_values, labels=None):
        outputs = self.vit(pixel_values)
        # ä½¿ç”¨CLS tokenè¿›è¡Œåˆ†ç±»
        pooled_output = outputs[0][:, 0]
        logits = self.classifier(pooled_output)

        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            return {"loss": loss, "logits": logits}

        return {"logits": logits}
```

#### ViTForMaskedImageModeling - æ©ç å›¾åƒå»ºæ¨¡
```python
class ViTForMaskedImageModeling(ViTPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.vit = ViTModel(config)
        # è§£ç å™¨ï¼šé‡å»ºå›¾åƒ
        self.decoder = nn.Linear(config.hidden_size, config.patch_size**2 * config.num_channels)

    def forward(self, pixel_values, bool_masked_positions=None):
        outputs = self.vit(pixel_values)
        sequence_output = outputs[0]

        # åªé‡å»ºè¢«æ©ç çš„patch
        if bool_masked_positions is not None:
            sequence_output = sequence_output[bool_masked_positions]

        # é‡å»ºå›¾åƒ
        reconstructed_pixel_values = self.decoder(sequence_output)
        return reconstructed_pixel_values
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€å›¾åƒåˆ†ç±»
```python
from transformers import ViTImageProcessor, ViTForImageClassification
from PIL import Image
import torch

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå¤„ç†å™¨
model_name = "google/vit-base-patch16-224"
processor = ViTImageProcessor.from_pretrained(model_name)
model = ViTForImageClassification.from_pretrained(model_name)

# åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
image = Image.open("example.jpg").convert("RGB")
inputs = processor(images=image, return_tensors="pt")

# æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# è·å–é¢„æµ‹ç»“æœ
predicted_class_idx = logits.argmax(-1).item()
predicted_class = model.config.id2label[predicted_class_idx]
confidence = torch.softmax(logits, dim=-1).max().item()

print(f"Predicted: {predicted_class} (confidence: {confidence:.3f})")
```

### 2. æ‰¹é‡å›¾åƒåˆ†ç±»
```python
from torchvision import transforms
from pathlib import Path

def batch_classify(image_paths, model, processor, batch_size=32):
    """æ‰¹é‡å›¾åƒåˆ†ç±»"""
    results = []

    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i+batch_size]
        batch_images = [Image.open(path).convert("RGB") for path in batch_paths]

        # æ‰¹é‡å¤„ç†
        inputs = processor(images=batch_images, return_tensors="pt")

        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits

        # è·å–é¢„æµ‹ç»“æœ
        probs = torch.softmax(logits, dim=-1)
        predicted_classes = probs.argmax(dim=-1)
        confidences = probs.max(dim=-1).values

        for path, pred_idx, conf in zip(batch_paths, predicted_classes, confidences):
            pred_class = model.config.id2label[pred_idx.item()]
            results.append({
                "image": path,
                "predicted_class": pred_class,
                "confidence": conf.item()
            })

    return results
```

### 3. ç‰¹å¾æå–
```python
def extract_vit_features(images, model, processor, layer_idx=-1):
    """æå–ViTç‰¹å¾"""
    # åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä¸åŒ…å«åˆ†ç±»å¤´ï¼‰
    vit_model = ViTModel.from_pretrained("google/vit-base-patch16-224")

    inputs = processor(images=images, return_tensors="pt")

    with torch.no_grad():
        outputs = vit_model(**inputs, output_hidden_states=True)

    # é€‰æ‹©ç‰¹å®šå±‚çš„ç‰¹å¾
    hidden_states = outputs.hidden_states
    selected_features = hidden_states[layer_idx]  # æœ€åä¸€å±‚

    # CLS tokenç‰¹å¾ï¼ˆç”¨äºåˆ†ç±»ï¼‰
    cls_features = selected_features[:, 0, :]

    # æ‰€æœ‰patchç‰¹å¾ï¼ˆç”¨äºåˆ†å‰²ã€æ£€æµ‹ç­‰ï¼‰
    patch_features = selected_features[:, 1:, :]

    return {
        "cls_features": cls_features,
        "patch_features": patch_features,
        "all_hidden_states": hidden_states
    }
```

### 4. å¯è§†åŒ–æ³¨æ„åŠ›
```python
import matplotlib.pyplot as plt
import numpy as np

def visualize_attention(image, model, processor, layer_idx=0, head_idx=0):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    # ä¿®æ”¹æ¨¡å‹ä»¥è¾“å‡ºæ³¨æ„åŠ›æƒé‡
    vit_model = ViTModel.from_pretrained(
        "google/vit-base-patch16-224",
        output_attentions=True
    )

    inputs = processor(images=image, return_tensors="pt")

    with torch.no_grad():
        outputs = vit_model(**inputs)
        attentions = outputs.attentions

    # è·å–æŒ‡å®šå±‚çš„æ³¨æ„åŠ›
    attention = attentions[layer_idx][0, head_idx, 0, 1:]  # CLS tokenå¯¹å…¶ä»–patchçš„æ³¨æ„åŠ›

    # é‡æ’ä¸ºå›¾åƒç½‘æ ¼
    patch_size = 16
    image_size = 224
    num_patches_per_side = image_size // patch_size

    attention_map = attention.reshape(num_patches_per_side, num_patches_per_side)
    attention_map = attention_map.cpu().numpy()

    # ä¸Šé‡‡æ ·åˆ°åŸå§‹å›¾åƒå°ºå¯¸
    from skimage.transform import resize
    attention_resized = resize(attention_map, (image_size, image_size), order=1)

    # å¯è§†åŒ–
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 3, 1)
    plt.imshow(image)
    plt.title("Original Image")
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(attention_resized, cmap='hot')
    plt.title(f"Attention (Layer {layer_idx}, Head {head_idx})")
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(image)
    plt.imshow(attention_resized, cmap='hot', alpha=0.5)
    plt.title("Overlay")
    plt.axis('off')

    plt.tight_layout()
    plt.show()
```

### 5. è‡ªå®šä¹‰ViTé…ç½®
```python
from transformers import ViTConfig, ViTForImageClassification

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = ViTConfig(
    image_size=384,              # æ›´å¤§çš„è¾“å…¥å›¾åƒ
    patch_size=16,               # ä¿æŒpatchå¤§å°
    hidden_size=1024,            # æ›´å¤§çš„éšè—ç»´åº¦
    num_hidden_layers=24,        # æ›´æ·±çš„ç½‘ç»œ
    num_attention_heads=16,      # æ›´å¤šæ³¨æ„åŠ›å¤´
    intermediate_size=4096,      # æ›´å¤§çš„å‰é¦ˆç½‘ç»œ
    num_labels=1000,             # ImageNetç±»åˆ«æ•°
)

# åˆ›å»ºæ¨¡å‹
model = ViTForImageClassification(config)

# éšæœºåˆå§‹åŒ–æˆ–ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
# model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224", config=config)
```

### 6. å¾®è°ƒç¤ºä¾‹
```python
from transformers import Trainer, TrainingArguments, ViTImageProcessor
from datasets import load_dataset
import torchvision.transforms as transforms

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("cifar10")

# æ•°æ®å¢å¼º
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

def preprocess_function(examples):
    examples['pixel_values'] = [transform(image.convert("RGB")) for image in examples['img']]
    examples['labels'] = examples['label']
    return examples

# é¢„å¤„ç†æ•°æ®
processed_dataset = dataset.map(preprocess_function, remove_columns=['img'], batched=True)

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./vit-finetuned",
    num_train_epochs=10,
    per_device_train_batch_size=32,
    learning_rate=3e-5,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

# åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset["train"],
    eval_dataset=processed_dataset["test"],
)

# å¼€å§‹å¾®è°ƒ
trainer.train()
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æ¨ç†ä¼˜åŒ–
```python
# ä½¿ç”¨FP16æ¨ç†
model = ViTForImageClassification.from_pretrained(
    "google/vit-base-patch16-224",
    torch_dtype=torch.float16
).to("cuda")

# é‡åŒ–
model = ViTForImageClassification.from_pretrained(
    "google/vit-base-patch16-224",
    load_in_8bit=True,
    device_map="auto"
)

# Flash Attention
model = ViTForImageClassification.from_pretrained(
    "google/vit-base-patch16-224",
    use_flash_attention_2=True
)
```

### 2. æ•°æ®åŠ è½½ä¼˜åŒ–
```python
from torch.utils.data import DataLoader
from torchvision import transforms

class EfficientViTDataset(torch.utils.data.Dataset):
    def __init__(self, image_paths, labels, processor):
        self.image_paths = image_paths
        self.labels = labels
        self.processor = processor

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert("RGB")
        label = self.labels[idx]

        # é¢„å¤„ç†
        inputs = self.processor(images=image, return_tensors="pt")
        return {
            "pixel_values": inputs.pixel_values.squeeze(),
            "labels": torch.tensor(label, dtype=torch.long)
        }

# é«˜æ•ˆçš„æ•°æ®åŠ è½½å™¨
dataset = EfficientViTDataset(image_paths, labels, processor)
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True,
    persistent_workers=True
)
```

### 3. å†…å­˜ä¼˜åŒ–
```python
# æ¢¯åº¦æ£€æŸ¥ç‚¹
model = ViTForImageClassification.from_pretrained(
    "google/vit-base-patch16-224",
    gradient_checkpointing=True
)

# æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    optimizer.zero_grad()

    with autocast():
        outputs = model(**batch)
        loss = outputs.loss

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

## æ¨¡å‹å˜ä½“

### 1. ä¸åŒå°ºå¯¸çš„ViT
- **ViT-Base**: 12å±‚ï¼Œ768éšè—ç»´åº¦ï¼Œ~86Må‚æ•°
- **ViT-Large**: 24å±‚ï¼Œ1024éšè—ç»´åº¦ï¼Œ~307Må‚æ•°
- **ViT-Huge**: 32å±‚ï¼Œ1280éšè—ç»´åº¦ï¼Œ~632Må‚æ•°

### 2. ä¸åŒpatchå°ºå¯¸
- **patch16**: 16x16 patchï¼Œé€‚ç”¨äºåˆ†ç±»ä»»åŠ¡
- **patch32**: 32x32 patchï¼Œæ›´é«˜æ•ˆç‡
- **patch8**: 8x8 patchï¼Œæ›´é«˜åˆ†è¾¨ç‡

### 3. é¢„è®­ç»ƒå˜ä½“
- **ViT-Base-Patch16-224**: ImageNet-21ké¢„è®­ç»ƒ
- **ViT-Base-Patch16-384**: æ›´é«˜åˆ†è¾¨ç‡ç‰ˆæœ¬
- **ViT-Large-Patch16-224**: æ›´å¤§è§„æ¨¡ç‰ˆæœ¬

### 4. ä¸“é—¨æ¨¡å‹
- **DeiT**: Data-efficient Image Transformers
- **Swin Transformer**: å±‚æ¬¡åŒ–Vision Transformer
- **MAE**: Masked Autoencoders

## æœ€ä½³å®è·µ

### 1. æ•°æ®é¢„å¤„ç†
```python
def optimal_preprocessing(image_size=224):
    """æœ€ä¼˜é¢„å¤„ç†ç­–ç•¥"""
    train_transform = transforms.Compose([
        transforms.Resize((image_size + 32, image_size + 32)),
        transforms.RandomCrop(image_size),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    return train_transform, val_transform
```

### 2. å­¦ä¹ ç‡è°ƒåº¦
```python
from transformers import get_cosine_schedule_with_warmup

# ä½™å¼¦é€€ç«å­¦ä¹ ç‡
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=1000,
    num_training_steps=10000
)
```

### 3. æ¨¡å‹é›†æˆ
```python
def ensemble_predict(images, models, processor):
    """æ¨¡å‹é›†æˆé¢„æµ‹"""
    all_predictions = []

    for model in models:
        inputs = processor(images=images, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)
        all_predictions.append(probs)

    # å¹³å‡é¢„æµ‹
    avg_probs = torch.stack(all_predictions).mean(dim=0)
    predictions = avg_probs.argmax(dim=-1)
    return predictions, avg_probs
```

## å¸¸è§é—®é¢˜ (FAQ)

### Q: ViTç›¸æ¯”CNNæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
A: ä¼˜åŠ¿ï¼š
- å…¨å±€æ„Ÿå—é‡ï¼Œèƒ½æ•è·é•¿è·ç¦»ä¾èµ–
- å‚æ•°æ•ˆç‡é«˜ï¼Œè®¡ç®—å¤æ‚åº¦ä¸åºåˆ—é•¿åº¦å¹³æ–¹æˆæ­£æ¯”
- å¯æ‰©å±•æ€§å¼ºï¼Œå®¹æ˜“å¢åŠ æ¨¡å‹å®¹é‡
- æ¶æ„ç»Ÿä¸€ï¼Œä¾¿äºå¤šæ¨¡æ€å­¦ä¹ 

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ViTæ¨¡å‹ï¼Ÿ
A: æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š
- **é€Ÿåº¦ä¼˜å…ˆ**: ViT-Base, patch32
- **ç²¾åº¦ä¼˜å…ˆ**: ViT-Large, patch16
- **å†…å­˜å—é™**: ViT-Base + é‡åŒ–
- **é«˜åˆ†è¾¨ç‡**: ViT-Large-384

### Q: ViTé€‚åˆå°æ•°æ®é›†å—ï¼Ÿ
A: å»ºè®®ï¼š
- ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ + å¾®è°ƒ
- å¼ºæ•°æ®å¢å¼º
- æ­£åˆ™åŒ–æŠ€æœ¯
- è€ƒè™‘ä½¿ç”¨DeiTç­‰æ•°æ®é«˜æ•ˆç‰ˆæœ¬

### Q: å¦‚ä½•å¤„ç†ä¸åŒå°ºå¯¸çš„å›¾åƒï¼Ÿ
A: æ–¹æ³•ï¼š
- è°ƒæ•´åˆ°æ¨¡å‹è®­ç»ƒæ—¶çš„å°ºå¯¸
- ä½¿ç”¨é€‚åº”æ€§patchå¤§å°
- ä½ç½®ç¼–ç æ’å€¼
- åˆ†å±‚å¤„ç†

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ–‡ä»¶
- `modeling_vit.py`: 749è¡Œï¼ŒåŒ…å«å®Œæ•´çš„ViTå®ç°
- `configuration_vit.py`: ViTConfigé…ç½®ç±»
- `image_processing_vit.py`: å›¾åƒé¢„å¤„ç†å™¨

### è½¬æ¢è„šæœ¬
- `convert_dino_to_pytorch.py`: DINOæ¨¡å‹è½¬æ¢
- `convert_vit_timm_to_pytorch.py`: timmæ¨¡å‹è½¬æ¢

### æµ‹è¯•æ–‡ä»¶
- `tests/test_modeling_vit.py`: ViTæ¨¡å‹æµ‹è¯•
- `tests/test_image_processing_vit.py`: å›¾åƒå¤„ç†å™¨æµ‹è¯•

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - è¯¦ç»†åˆ†æ
- âœ¨ å®ŒæˆViTæ¨¡å‹æ ¸å¿ƒç»„ä»¶åˆ†æ
- ğŸ” è®°å½•Transformeræ¶æ„åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨
- ğŸ“Š åˆ†æé…ç½®å‚æ•°å’Œæœ€ä½³å®è·µ
- ğŸ¯ æä¾›å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹å’Œä¼˜åŒ–æ–¹æ³•

### ä¸‹ä¸€æ­¥è®¡åˆ’
- [ ] åˆ†æViTåœ¨å…¶ä»–è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨
- [ ] åˆ›å»ºè§†è§‰Transformeræœ€ä½³å®è·µæ–‡æ¡£
- [ ] è®°å½•ViTå˜ä½“çš„æ€§èƒ½å¯¹æ¯”
- [ ] åˆ†æViTçš„è®¡ç®—å¤æ‚åº¦å’Œæ•ˆç‡

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 95%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 98%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20