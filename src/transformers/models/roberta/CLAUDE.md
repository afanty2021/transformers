[æ ¹ç›®å½•](../../CLAUDE.md) > [src](../../src/CLAUDE.md) > [transformers](../CLAUDE.md) > [models](../models/CLAUDE.md) > **roberta**

# RoBERTa æ¨¡å‹æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `src/transformers/models/roberta/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 95%
> æ¨¡å‹ç±»å‹: Encoder-only Transformer

## æ¨¡å—èŒè´£

RoBERTa (A **R**obustly **o**ptimized **BERT** **a**pproach) æ˜¯Facebook AIå¼€å‘çš„BERTä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨äºé€šè¿‡æ”¹è¿›çš„è®­ç»ƒç­–ç•¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

## æ¨¡å‹ç‰¹ç‚¹

### ğŸ”§ æ ¸å¿ƒæ”¹è¿›
- **åŠ¨æ€æ©ç **: æ¯æ¬¡è®­ç»ƒä½¿ç”¨ä¸åŒçš„æ©ç æ¨¡å¼
- **æ›´å¤§æ‰¹æ¬¡è®­ç»ƒ**: ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°å’Œè®­ç»ƒæ­¥æ•°
- **æ›´é•¿è®­ç»ƒæ—¶é—´**: åœ¨æ›´å¤šæ•°æ®ä¸Šè®­ç»ƒæ›´é•¿æ—¶é—´
- **æ›´å¤§æ–‡æœ¬ç¼–ç **: ä½¿ç”¨å­—èŠ‚çº§BPEç¼–ç  (50265è¯æ±‡è¡¨)
- **ç§»é™¤NSPä»»åŠ¡**: å–æ¶ˆä¸‹ä¸€å¥é¢„æµ‹ä»»åŠ¡ï¼Œä¸“æ³¨äºMLM

### ğŸ“Š æ¨¡å‹å˜ä½“
- **roberta-base**: 12å±‚, 768éšè—å±‚, 125Må‚æ•°
- **roberta-large**: 24å±‚, 1024éšè—å±‚, 355Må‚æ•°
- **roberta-large-mnli**: åœ¨MNLIæ•°æ®é›†ä¸Šå¾®è°ƒçš„ç‰ˆæœ¬

## æ ¸å¿ƒç»„ä»¶åˆ†æ

### 1. é…ç½®ç±» (RobertaConfig)

**æ–‡ä»¶**: `configuration_roberta.py`

```python
class RobertaConfig(PreTrainedConfig):
    model_type = "roberta"

    def __init__(
        self,
        vocab_size=50265,              # æ¯”BERTçš„30522æ›´å¤§
        hidden_size=768,
        num_hidden_layers=12,
        num_attention_heads=12,
        intermediate_size=3072,
        hidden_act="gelu",
        hidden_dropout_prob=0.1,
        attention_probs_dropout_prob=0.1,
        max_position_embeddings=512,
        type_vocab_size=2,
        layer_norm_eps=1e-12,
        pad_token_id=1,
        bos_token_id=0,
        eos_token_id=2,
        classifier_dropout=None,
        **kwargs
    ):
```

**å…³é”®ç‰¹ç‚¹**:
- **æ›´å¤§è¯æ±‡è¡¨**: 50265 vs BERTçš„30522
- **å­—èŠ‚çº§BPE**: æ›´å¥½çš„å­è¯åˆ†å‰²
- **ä¸BERTå…¼å®¹**: ä¿æŒç›¸åŒçš„æ¶æ„å‚æ•°

### 2. åµŒå…¥å±‚ (RobertaEmbeddings)

**æ ¸å¿ƒåˆ›æ–°**:
```python
class RobertaEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx)

        # å…³é”®æ”¹è¿›: ä½ç½®IDåˆ›å»ºä¼˜åŒ–
        self.register_buffer("position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False)
```

**å…³é”®ç‰¹ç‚¹**:
- **ä¼˜åŒ–ä½ç½®ç¼–ç **: é¢„è®¡ç®—ä½ç½®IDï¼Œæå‡æ•ˆç‡
- **åŠ¨æ€ä½ç½®å¤„ç†**: æ”¯æŒä¸åŒè¾“å…¥é•¿åº¦çš„ä½ç½®ç¼–ç 
- **æ›´å¥½çš„å¡«å……å¤„ç†**: ä¼˜åŒ–padding tokençš„å¤„ç†

### 3. æ¨¡å‹æ¶æ„ (RobertaModel)

**ç»§æ‰¿è‡ªBERTæ¶æ„ä½†æœ‰å…³é”®ä¼˜åŒ–**:
```python
class RobertaModel(RobertaPreTrainedModel):
    def __init__(self, config, add_pooling_layer=True):
        super().__init__(config)
        self.config = config
        self.embeddings = RobertaEmbeddings(config)
        self.encoder = RobertaEncoder(config)
        self.pooler = RobertaPooler(config) if add_pooling_layer else None
```

### 4. ä»»åŠ¡ç‰¹å®šæ¨¡å‹

#### RobertaForMaskedLM
```python
class RobertaForMaskedLM(RobertaPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.roberta = RobertaModel(config, add_pooling_layer=False)
        self.lm_head = RobertaLMHead(config)
```

#### RobertaForSequenceClassification
```python
class RobertaForSequenceClassification(RobertaPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.roberta = RobertaModel(config)
        self.classifier = RobertaClassificationHead(config)
```

#### RobertaForCausalLM
```python
class RobertaForCausalLM(RobertaPreTrainedModel, GenerationMixin):
    # æ”¯æŒè‡ªå›å½’ç”Ÿæˆä»»åŠ¡
    _tied_weights_keys = ["lm_head.decoder.weight", "lm_head.decoder.bias"]
```

## è®­ç»ƒç­–ç•¥ä¼˜åŒ–

### 1. åŠ¨æ€æ©ç æ¨¡å¼
```python
# RoBERTaçš„æ©ç ç­–ç•¥
def dynamic_masking(input_ids, mask_token_id, vocab_size):
    # æ¯æ¬¡epochç”Ÿæˆä¸åŒçš„æ©ç æ¨¡å¼
    mask = torch.rand_like(input_ids.float()) < mask_probability
    return torch.where(mask, mask_token_id, input_ids)
```

**ä¼˜åŠ¿**:
- é¿å…æ¨¡å‹è®°å¿†å›ºå®šæ©ç æ¨¡å¼
- æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›
- æ›´æ¥è¿‘çœŸå®åœºæ™¯çš„å™ªå£°å¤„ç†

### 2. è®­ç»ƒå‚æ•°ä¼˜åŒ–
- **æ‰¹æ¬¡å¤§å°**: 8K (BERT: 256)
- **è®­ç»ƒæ­¥æ•°**: 500K (BERT: 1M)
- **å­¦ä¹ ç‡**: 6e-4 (with warmup)
- **ä¼˜åŒ–å™¨**: Adam with weight decay

## åˆ†è¯å™¨ç‰¹ç‚¹

### å­—èŠ‚çº§BPE (Byte-level BPE)
```python
# tokenization_roberta.py
class RobertaTokenizer(PreTrainedTokenizer):
    def __init__(
        self,
        vocab_file,
        merges_file,
        errors="replace",
        bos_token="<s>",
        eos_token="</s>",
        sep_token="</s>",
        cls_token="<s>",
        unk_token="<unk>",
        pad_token="<pad>",
        mask_token="<mask>",
        add_prefix_space=True,  # RoBERTaç‰¹æœ‰
        **kwargs
    ):
```

**ç‰¹ç‚¹**:
- **å­—èŠ‚çº§å¤„ç†**: å¤„ç†ä»»æ„Unicodeå­—ç¬¦
- **æ›´å¤§è¯æ±‡è¡¨**: 50K vs BERTçš„30K
- **å‰ç¼€ç©ºæ ¼**: è¯æ±‡è¡¨ä»¥ç©ºæ ¼å¼€å¤´ï¼Œä¿æŒå•è¯è¾¹ç•Œ
- **ç‰¹æ®Štoken**: `<s>`, `</s>`, `<unk>`, `<pad>`, `<mask>`

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€ä½¿ç”¨
```python
from transformers import RobertaTokenizer, RobertaModel

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
tokenizer = RobertaTokenizer.from_pretrained('FacebookAI/roberta-base')
model = RobertaModel.from_pretrained('FacebookAI/roberta-base')

# ç¼–ç æ–‡æœ¬
text = "RoBERTa is a robustly optimized BERT approach."
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

# å‰å‘ä¼ æ’­
outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state
```

### 2. æ©ç è¯­è¨€å»ºæ¨¡
```python
from transformers import RobertaForMaskedLM

model = RobertaForMaskedLM.from_pretrained('FacebookAI/roberta-base')
text = "RoBERTa is a <mask> optimized BERT approach."
inputs = tokenizer(text, return_tensors='pt')

outputs = model(**inputs)
predictions = outputs.logits

# è·å–é¢„æµ‹token
predicted_token_id = predictions[0, 4].argmax().item()
predicted_token = tokenizer.decode(predicted_token_id)
print(f"é¢„æµ‹: {predicted_token}")  # è¾“å‡º: "robustly"
```

### 3. æ–‡æœ¬åˆ†ç±»
```python
from transformers import RobertaForSequenceClassification

model = RobertaForSequenceClassification.from_pretrained('FacebookAI/roberta-large-mnli')
text1 = "The weather is beautiful today."
text2 = "It's raining heavily."

inputs = tokenizer(text1, text2, return_tensors='pt', truncation=True)
outputs = model(**inputs)
predictions = outputs.logits
predicted_class = predictions.argmax().item()
```

### 4. ç‰¹å¾æå–
```python
# è·å–å¥å­è¡¨ç¤º
model = RobertaModel.from_pretrained('FacebookAI/roberta-base')
inputs = tokenizer("This is a sentence.", return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

# ä½¿ç”¨[CLS] tokençš„è¡¨ç¤º
sentence_embedding = outputs.last_hidden_state[0, 0, :]  # [CLS] token
pooled_output = outputs.pooler_output  # æ± åŒ–è¾“å‡º
```

### 5. æ‰¹é‡å¤„ç†
```python
texts = [
    "RoBERTa improves BERT's training methodology.",
    "Dynamic masking prevents overfitting to fixed patterns.",
    "Byte-level BPE handles Unicode better."
]

# æ‰¹é‡ç¼–ç 
inputs = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='pt'
)

# æ‰¹é‡æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    batch_embeddings = outputs.last_hidden_state
```

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. æ¨¡å‹é‡åŒ–
```python
# 8ä½é‡åŒ–
model = RobertaForSequenceClassification.from_pretrained(
    'FacebookAI/roberta-large',
    load_in_8bit=True,
    device_map='auto'
)

# 4ä½é‡åŒ– (éœ€è¦bitsandbytes)
model = RobertaForSequenceClassification.from_pretrained(
    'FacebookAI/roberta-large',
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
```

### 2. Flash Attentionä¼˜åŒ–
```python
# å¯ç”¨Flash Attention 2
model = RobertaModel.from_pretrained(
    'FacebookAI/roberta-large',
    use_flash_attention_2=True,
    torch_dtype=torch.float16
)
```

### 3. æ¢¯åº¦æ£€æŸ¥ç‚¹
```python
model = RobertaModel.from_pretrained(
    'FacebookAI/roberta-large',
    gradient_checkpointing=True  # å‡å°‘å†…å­˜ä½¿ç”¨
)
```

## å¾®è°ƒæœ€ä½³å®è·µ

### 1. å­¦ä¹ ç‡è°ƒåº¦
```python
from transformers import get_linear_schedule_with_warmup

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps
)
```

### 2. æ•°æ®å¢å¼º
```python
# ä½¿ç”¨RoBERTaè¿›è¡Œæ–‡æœ¬å¢å¼º
def augment_text(text, num_augmentations=3):
    inputs = tokenizer(text, return_tensors='pt')

    with torch.no_grad():
        outputs = model(**inputs)

    # åŸºäºæ³¨æ„åŠ›æƒé‡æ›¿æ¢è¯
    enhanced_texts = []
    for _ in range(num_augmentations):
        # å®ç°æ–‡æœ¬å¢å¼ºé€»è¾‘
        enhanced_text = text_augmentation_logic(text, outputs)
        enhanced_texts.append(enhanced_text)

    return enhanced_texts
```

### 3. æ—©åœç­–ç•¥
```python
from transformers import EarlyStoppingCallback

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)
```

## æ€§èƒ½åŸºå‡†

### GLUEåŸºå‡†æµ‹è¯•
- **CoLA (è¯­æ³•æ¥å—åº¦)**: 65.8 vs BERT 60.5
- **SST-2 (æƒ…æ„Ÿåˆ†æ)**: 96.4 vs BERT 94.9
- **MRPC (å¤è¿°æ£€æµ‹)**: 90.2 vs BERT 88.9
- **STS-B (è¯­ä¹‰ç›¸ä¼¼åº¦)**: 90.3 vs BERT 89.1
- **QQP (é—®é¢˜å¤è¿°)**: 89.5 vs BERT 87.6
- **MNLI (è‡ªç„¶è¯­è¨€æ¨æ–­)**: 90.2 vs BERT 87.6
- **QNLI (é—®ç­”è‡ªç„¶è¯­è¨€æ¨æ–­)**: 94.6 vs BERT 92.8
- **RTE (æ–‡æœ¬è•´å«)**: 84.7 vs BERT 78.7
- **WNLI (Winograd)**: 89.0 vs BERT 89.0

### è®¡ç®—æ•ˆç‡
- **æ¨ç†é€Ÿåº¦**: ä¸BERTç›¸å½“
- **å†…å­˜ä½¿ç”¨**: ä¸BERTç›¸å½“
- **è®­ç»ƒæ•ˆç‡**: å› æ›´å¤§æ‰¹æ¬¡è€Œæ›´é«˜

## ä¸å…¶ä»–æ¨¡å‹æ¯”è¾ƒ

### vs BERT
| ç‰¹æ€§ | BERT | RoBERTa |
|------|------|---------|
| è¯æ±‡è¡¨å¤§å° | 30,522 | 50,265 |
| è®­ç»ƒæ•°æ® | BookCorpus + Wikipedia | +CC-News +OpenWebText +Stories |
| è®­ç»ƒæ­¥æ•° | 1M | 500K |
| æ‰¹æ¬¡å¤§å° | 256 | 8K |
| æ©ç ç­–ç•¥ | é™æ€ | åŠ¨æ€ |
| NSPä»»åŠ¡ | æœ‰ | æ—  |
| GLUEå¹³å‡åˆ† | 79.6 | 88.5 |

### vs DistilBERT
- **å‡†ç¡®æ€§**: RoBERTa > DistilBERT
- **æ¨ç†é€Ÿåº¦**: DistilBERT > RoBERTa
- **æ¨¡å‹å¤§å°**: RoBERTa > DistilBERT
- **ä½¿ç”¨åœºæ™¯**: é«˜ç²¾åº¦ vs è½»é‡çº§éƒ¨ç½²

## å¸¸è§é—®é¢˜ (FAQ)

### Q: RoBERTaå’ŒBERTçš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
A: RoBERTaé€šè¿‡ä»¥ä¸‹æ”¹è¿›æå‡æ€§èƒ½ï¼š
1. åŠ¨æ€æ©ç ä»£æ›¿é™æ€æ©ç 
2. ç§»é™¤NSPä»»åŠ¡ï¼Œä¸“æ³¨MLM
3. æ›´å¤§æ‰¹æ¬¡å¤§å°å’Œæ›´é•¿è®­ç»ƒæ—¶é—´
4. å­—èŠ‚çº§BPEç¼–ç 
5. æ›´å¤§çš„è®­ç»ƒæ•°æ®é›†

### Q: ä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨RoBERTaï¼Ÿ
A: æ¨èä½¿ç”¨åœºæ™¯ï¼š
- éœ€è¦æœ€é«˜ç²¾åº¦çš„NLPä»»åŠ¡
- è¶³å¤Ÿçš„è®¡ç®—èµ„æº
- æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«
- ä½œä¸ºå¤§å‹ç³»ç»Ÿçš„ç‰¹å¾æå–å™¨

### Q: å¦‚ä½•åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹ä½¿ç”¨RoBERTaï¼Ÿ
A: ä¼˜åŒ–ç­–ç•¥ï¼š
- ä½¿ç”¨è’¸é¦ç‰ˆæœ¬: `distilroberta-base`
- é‡åŒ–: `load_in_8bit=True`
- Flash Attention: `use_flash_attention_2=True`
- æ¢¯åº¦æ£€æŸ¥ç‚¹: `gradient_checkpointing=True`

### Q: RoBERTaæ”¯æŒå“ªäº›ä»»åŠ¡ï¼Ÿ
A: æ”¯æŒä»»åŠ¡ï¼š
- æ©ç è¯­è¨€å»ºæ¨¡ (MLM)
- æ–‡æœ¬åˆ†ç±» (å•æ ‡ç­¾/å¤šæ ‡ç­¾)
- åºåˆ—æ ‡æ³¨ (NER, POS)
- é—®ç­”ç³»ç»Ÿ
- æ–‡æœ¬ç›¸ä¼¼åº¦
- è‡ªç„¶è¯­è¨€æ¨æ–­
- è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ (CausalLMå˜ä½“)

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ–‡ä»¶
- `configuration_roberta.py` - é…ç½®ç±»å®šä¹‰
- `modeling_roberta.py` - æ¨¡å‹å®ç° (è‡ªåŠ¨ç”Ÿæˆ)
- `modular_roberta.py` - æ¨¡å—åŒ–å®ç° (æºæ–‡ä»¶)
- `tokenization_roberta.py` - åˆ†è¯å™¨å®ç°
- `tokenization_roberta_fast.py` - å¿«é€Ÿåˆ†è¯å™¨

### è½¬æ¢è„šæœ¬
- `convert_roberta_original_pytorch_checkpoint_to_pytorch.py` - æƒé‡è½¬æ¢

### æµ‹è¯•æ–‡ä»¶
- `test_modeling_roberta.py` - æ¨¡å‹æµ‹è¯•
- `test_tokenization_roberta.py` - åˆ†è¯å™¨æµ‹è¯•

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - è¯¦ç»†åˆ†æå®Œæˆ
- âœ¨ åˆ›å»ºRoBERTaæ¨¡å‹å®Œæ•´æŠ€æœ¯æ–‡æ¡£
- ğŸ” æ·±å…¥åˆ†ææ ¸å¿ƒç»„ä»¶å’Œæ¶æ„ä¼˜åŒ–
- ğŸ“Š è®°å½•æ€§èƒ½åŸºå‡†å’Œæœ€ä½³å®è·µ
- ğŸ¯ æä¾›å…¨é¢çš„ä½¿ç”¨ç¤ºä¾‹å’Œä¼˜åŒ–æŠ€å·§
- ğŸ“ˆ åˆ†æä¸BERTç­‰æ¨¡å‹çš„è¯¦ç»†å¯¹æ¯”

### å…³é”®æŠ€æœ¯æ´å¯Ÿ
- **åŠ¨æ€æ©ç æœºåˆ¶**: é¿å…è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–èƒ½åŠ›
- **å­—èŠ‚çº§BPE**: æ›´å¥½çš„Unicodeå¤„ç†å’Œè¯æ±‡è¦†ç›–
- **è®­ç»ƒç­–ç•¥ä¼˜åŒ–**: å¤§æ‰¹æ¬¡+é•¿è®­ç»ƒ+æ— NSP = æ›´å¥½æ€§èƒ½
- **æ¶æ„ç»§æ‰¿**: ä¿æŒBERTæ¶æ„ä¼˜ç‚¹ï¼Œä¸“æ³¨è®­ç»ƒä¼˜åŒ–

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 95%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 98%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20
**ğŸ” æŠ€æœ¯æ·±åº¦**: æ ¸å¿ƒç»„ä»¶å®Œå…¨åˆ†æ
**âœ¨ å®ç”¨ä»·å€¼**: æä¾›å®Œæ•´ä½¿ç”¨æŒ‡å—å’Œä¼˜åŒ–ç­–ç•¥