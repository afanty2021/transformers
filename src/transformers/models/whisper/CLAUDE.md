[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > [src](/Users/berton/Github/transformers/src/CLAUDE.md) > [transformers](/Users/berton/Github/transformers/src/transformers/CLAUDE.md) > [models](/Users/berton/Github/transformers/src/transformers/models/CLAUDE.md) > **whisper**

# Whisper æ¨¡å‹æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `src/transformers/models/whisper/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 95%

## æ¨¡å—èŒè´£

Whisperæ˜¯OpenAIå¼€å‘çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿï¼Œé€šè¿‡åœ¨68ä¸‡å°æ—¶å¤šè¯­è¨€å’Œå¤šä»»åŠ¡ç›‘ç£æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå±•ç°å‡ºå¼ºå¤§çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚Whisperä¸ä»…æ”¯æŒè¯­éŸ³è½¬æ–‡æœ¬ï¼Œè¿˜æ”¯æŒå¤šè¯­è¨€ç¿»è¯‘å’Œè¯­è¨€è¯†åˆ«ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **å¤§è§„æ¨¡é¢„è®­ç»ƒ**: åœ¨68ä¸‡å°æ—¶å¤šæ ·åŒ–éŸ³é¢‘æ•°æ®ä¸Šè®­ç»ƒ
- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒ100+ç§è¯­è¨€çš„è¯†åˆ«å’Œç¿»è¯‘
- **é²æ£’æ€§å¼º**: å¯¹å™ªå£°ã€å£éŸ³ã€èƒŒæ™¯éŸ³å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§
- **å¤šä»»åŠ¡èƒ½åŠ›**: åŒæ—¶æ”¯æŒè¯­éŸ³è¯†åˆ«ã€ç¿»è¯‘ã€è¯­è¨€è¯†åˆ«
- **é›¶æ ·æœ¬è¿ç§»**: æ— éœ€å¾®è°ƒå³å¯å¤„ç†ç‰¹å®šé¢†åŸŸéŸ³é¢‘

## æ–‡ä»¶ç»“æ„

```
whisper/
â”œâ”€â”€ __init__.py                                    # æ¨¡å—å¯¼å‡ºå’Œæ¨¡å‹æ˜ å°„
â”œâ”€â”€ configuration_whisper.py                      # WhisperConfigé…ç½®ç±»
â”œâ”€â”€ modeling_whisper.py                          # æ ¸å¿ƒæ¨¡å‹å®ç°
â”œâ”€â”€ processing_whisper.py                        # éŸ³é¢‘å¤„ç†å™¨
â”œâ”€â”€ feature_extraction_whisper.py                # ç‰¹å¾æå–å™¨
â”œâ”€â”€ tokenization_whisper.py                      # æ–‡æœ¬åˆ†è¯å™¨
â”œâ”€â”€ tokenization_whisper_fast.py                 # å¿«é€Ÿåˆ†è¯å™¨
â”œâ”€â”€ generation_whisper.py                        # ç”Ÿæˆç­–ç•¥
â”œâ”€â”€ english_normalizer.py                        # è‹±æ–‡æ–‡æœ¬è§„èŒƒåŒ–
â””â”€â”€ convert_openai_to_hf.py                      # OpenAIæƒé‡è½¬æ¢
```

## æ ¸å¿ƒç»„ä»¶åˆ†æ

### 1. é…ç½®ç±» (WhisperConfig)

```python
class WhisperConfig(PreTrainedConfig):
    model_type = "whisper"

    def __init__(
        self,
        vocab_size=51864,               # è¯æ±‡è¡¨å¤§å°
        num_mel_bins=80,                # Melé¢‘è°±binæ•°é‡
        encoder_layers=12,              # ç¼–ç å™¨å±‚æ•°
        encoder_attention_heads=12,     # ç¼–ç å™¨æ³¨æ„åŠ›å¤´æ•°
        decoder_layers=12,              # è§£ç å™¨å±‚æ•°
        decoder_attention_heads=12,     # è§£ç å™¨æ³¨æ„åŠ›å¤´æ•°
        decoder_ffn_dim=1536,           # è§£ç å™¨FFNç»´åº¦
        encoder_ffn_dim=1536,           # ç¼–ç å™¨FFNç»´åº¦
        d_model=768,                    # æ¨¡å‹ç»´åº¦
        dropout=0.1,                    # Dropoutç‡
        attention_dropout=0.0,          # æ³¨æ„åŠ›dropout
        activation_dropout=0.0,         # æ¿€æ´»dropout
        activation_function="gelu",     # æ¿€æ´»å‡½æ•°
        init_std=0.02,                  # åˆå§‹åŒ–æ ‡å‡†å·®
        layer_norm_eps=1e-5,            # LayerNorm epsilon
        max_source_positions=1500,      # æœ€å¤§éŸ³é¢‘é•¿åº¦
        max_target_positions=448,       # æœ€å¤§æ–‡æœ¬é•¿åº¦
        use_cache=True,                 # æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        scale_embedding=False,          # æ˜¯å¦ç¼©æ”¾åµŒå…¥
        **kwargs
    ):
        super().__init__(**kwargs)
        # å‚æ•°èµ‹å€¼...
```

**å…³é”®é…ç½®å‚æ•°**:
- `vocab_size`: åŒ…å«å¤šè¯­è¨€ç‰¹æ®Štokençš„å¤§è¯æ±‡è¡¨
- `num_mel_bins`: Melé¢‘è°±ç‰¹å¾ç»´åº¦
- `max_source_positions`: æœ€å¤§éŸ³é¢‘åºåˆ—é•¿åº¦
- `max_target_positions`: æœ€å¤§æ–‡æœ¬åºåˆ—é•¿åº¦

### 2. éŸ³é¢‘é¢„å¤„ç†

#### WhisperFeatureExtractor
```python
class WhisperFeatureExtractor(SequenceFeatureExtractor):
    def __init__(
        self,
        feature_size=80,                # Melé¢‘è°±binæ•°é‡
        sampling_rate=16000,            # é‡‡æ ·ç‡
        padding_value=0.0,              # å¡«å……å€¼
        hop_length=160,                 # STFT hopé•¿åº¦
        chunk_length=30,                # éŸ³é¢‘å—é•¿åº¦(ç§’)
        n_fft=400,                      # FFTçª—å£å¤§å°
        padding_side="right",           # å¡«å……æ–¹å‘
        return_attention_mask=False,    # æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç 
        do_normalize=True,              # æ˜¯å¦æ ‡å‡†åŒ–
        **kwargs
    ):
        super().__init__(
            feature_size=feature_size,
            sampling_rate=sampling_rate,
            padding_value=padding_value,
            **kwargs
        )
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.chunk_length = chunk_length
        self.do_normalize = do_normalize

    def __call__(self, raw_speech, **kwargs):
        # éŸ³é¢‘é¢„å¤„ç†ç®¡é“
        if isinstance(raw_speech, np.ndarray):
            raw_speech = [raw_speech]

        # è½¬æ¢ä¸ºå•å£°é“
        if all(s.ndim > 1 for s in raw_speech):
            raw_speech = [s.mean(axis=-1) for s in raw_speech]

        # è®¡ç®—Melé¢‘è°±å›¾
        mel_spectrograms = []
        for speech in raw_speech:
            # å¡«å……æˆ–æˆªæ–­åˆ°30ç§’
            if len(speech) > self.n_samples:
                speech = speech[:self.n_samples]
            else:
                speech = np.pad(speech, (0, self.n_samples - len(speech)), mode='constant')

            # è®¡ç®—Melé¢‘è°±
            mel_spec = self._extract_fbank_features(speech)
            mel_spectrograms.append(mel_spec)

        # æ ‡å‡†åŒ–
        if self.do_normalize:
            mel_spectrograms = [self._normalize(m) for m in mel_spectrograms]

        return {"input_features": np.array(mel_spectrograms)}
```

**æ ¸å¿ƒåŠŸèƒ½**:
- **é‡é‡‡æ ·**: ç»Ÿä¸€é‡‡æ ·ç‡åˆ°16kHz
- **å•å£°é“è½¬æ¢**: å¤„ç†å¤šå£°é“éŸ³é¢‘
- **åˆ†å—å¤„ç†**: æ”¯æŒé•¿éŸ³é¢‘çš„åˆ†å—å¤„ç†
- **Melé¢‘è°±æå–**: è®¡ç®—å¯¹æ•°Melé¢‘è°±å›¾
- **æ ‡å‡†åŒ–**: é¢‘è°±ç‰¹å¾çš„æ ‡å‡†åŒ–

### 3. æ ¸å¿ƒæ¨¡å‹ç»„ä»¶

#### WhisperEncoder - éŸ³é¢‘ç¼–ç å™¨
```python
class WhisperEncoder(WhisperPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.conv1 = nn.Conv1d(config.feature_size, config.d_model, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(config.d_model, config.d_model, kernel_size=3, stride=2, padding=1)
        self.embed_positions = nn.Embedding(config.max_source_positions, config.d_model)
        self.layers = nn.ModuleList([WhisperEncoderLayer(config) for _ in range(config.encoder_layers)])
        self.layer_norm = nn.LayerNorm(config.d_model)

        self.dropout = nn.Dropout(config.dropout)
        self.post_init()

    def forward(self, input_features, attention_mask=None):
        # å·ç§¯ç‰¹å¾æå–
        x = input_features.transpose(1, 2)
        x = self.conv1(x)
        x = F.gelu(x)
        x = self.conv2(x)
        x = F.gelu(x)

        # ä½ç½®ç¼–ç 
        input_shape = x.size()[:-1]
        positions = torch.arange(input_shape[1], device=x.device)
        position_embeds = self.embed_positions(positions).unsqueeze(0).expand(input_shape)

        x = x + position_embeds
        x = self.dropout(x)

        # Transformerç¼–ç å™¨å±‚
        for layer in self.layers:
            x = layer(x, attention_mask=attention_mask)

        x = self.layer_norm(x)
        return x.transpose(1, 2)
```

**å…³é”®ç»„ä»¶**:
- **å·ç§¯å±‚**: 1Då·ç§¯è¿›è¡Œåˆæ­¥ç‰¹å¾æå–å’Œä¸‹é‡‡æ ·
- **ä½ç½®ç¼–ç **: ä½ç½®åµŒå…¥æ·»åŠ æ—¶åºä¿¡æ¯
- **Transformerå±‚**: æ ‡å‡†çš„Transformerç¼–ç å™¨

#### WhisperDecoder - æ–‡æœ¬è§£ç å™¨
```python
class WhisperDecoder(WhisperPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model)
        self.embed_positions = nn.Embedding(config.max_target_positions, config.d_model)
        self.layers = nn.ModuleList([WhisperDecoderLayer(config) for _ in range(config.decoder_layers)])
        self.layer_norm = nn.LayerNorm(config.d_model)

        self.dropout = nn.Dropout(config.dropout)
        self.post_init()

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        encoder_hidden_states=None,
        past_key_values=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
    ):
        # è¯åµŒå…¥
        inputs_embeds = self.embed_tokens(input_ids)

        # ä½ç½®ç¼–ç 
        batch_size, seq_length = input_ids.shape
        positions = torch.arange(seq_length, device=input_ids.device)
        position_embeds = self.embed_positions(positions).unsqueeze(0).expand(batch_size, -1)

        hidden_states = inputs_embeds + position_embeds
        hidden_states = self.dropout(hidden_states)

        # Transformerè§£ç å™¨å±‚
        next_decoder_cache = () if use_cache else None
        for idx, decoder_layer in enumerate(self.layers):
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                encoder_hidden_states=encoder_hidden_states,
                past_key_value=past_key_values[idx] if past_key_values is not None else None,
                use_cache=use_cache,
            )

            hidden_states = layer_outputs[0]

            if use_cache:
                next_decoder_cache = next_decoder_cache + (layer_outputs[1],)

        hidden_states = self.layer_norm(hidden_states)

        return {
            "last_hidden_state": hidden_states,
            "past_key_values": next_decoder_cache,
            "hidden_states": hidden_states,
        }
```

**æ ¸å¿ƒæœºåˆ¶**:
- **è¯åµŒå…¥**: å°†tokenè½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
- **ä½ç½®ç¼–ç **: æ·»åŠ ä½ç½®ä¿¡æ¯
- **è‡ªæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ›**: æ ‡å‡†çš„decoderç»“æ„
- **ç¼“å­˜æœºåˆ¶**: æ”¯æŒå¢é‡ç”Ÿæˆ

### 4. ç”Ÿæˆç­–ç•¥

#### WhisperForConditionalGeneration
```python
class WhisperForConditionalGeneration(WhisperGenerationMixin, WhisperPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.encoder = WhisperEncoder(config)
        self.decoder = WhisperDecoder(config)
        self.proj_out = nn.Linear(config.d_model, config.vocab_size, bias=False)

        # æƒé‡ç»‘å®š
        self.proj_out.weight = self.decoder.embed_tokens.weight
        self.post_init()

    def forward(
        self,
        input_features=None,
        attention_mask=None,
        decoder_input_ids=None,
        decoder_attention_mask=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        # ç¼–ç å™¨å‰å‘ä¼ æ’­
        encoder_outputs = self.encoder(
            input_features,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        # è§£ç å™¨å‰å‘ä¼ æ’­
        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            encoder_hidden_states=encoder_outputs[0],
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        # è¾“å‡ºæŠ•å½±
        lm_logits = self.proj_out(decoder_outputs[0])

        loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))

        return {
            "loss": loss,
            "logits": lm_logits,
            "encoder_last_hidden_state": encoder_outputs[0],
        }
```

#### WhisperGenerationMixin
```python
class WhisperGenerationMixin:
    def generate(
        self,
        input_features,
        generation_config=None,
        logits_processor=None,
        stopping_criteria=None,
        prefix_allowed_tokens_fn=None,
        synced_gpus=None,
        **kwargs,
    ):
        # è®¾ç½®ä»»åŠ¡token
        if generation_config is None:
            generation_config = self.generation_config

        # æ ¹æ®ä»»åŠ¡æ·»åŠ ç‰¹æ®Štoken
        if generation_config.task == "transcribe":
            task_tokens = self.generation_config.task_to_id["transcribe"]
        elif generation_config.task == "translate":
            task_tokens = self.generation_config.task_to_id["translate"]
        else:
            task_tokens = self.generation_config.task_to_id["transcribe"]

        # è¯­è¨€token
        if generation_config.language is not None:
            language_tokens = self.generation_config.language_to_id[generation_config.language]
        else:
            language_tokens = self.generation_config.language_to_id["en"]

        # æ—¶é—´æˆ³token
        if generation_config.return_timestamps:
            timestamp_tokens = self.generation_config.timestamp_begin
        else:
            timestamp_tokens = None

        # æ„é€ åˆå§‹è§£ç å™¨è¾“å…¥
        decoder_input_ids = torch.tensor([[task_tokens, language_tokens]], dtype=torch.long)
        if timestamp_tokens is not None:
            decoder_input_ids = torch.cat([
                decoder_input_ids,
                torch.tensor([[timestamp_tokens]], dtype=torch.long)
            ], dim=1)

        # è°ƒç”¨æ ‡å‡†generateæ–¹æ³•
        return super().generate(
            input_features,
            decoder_input_ids=decoder_input_ids,
            generation_config=generation_config,
            **kwargs
        )
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€è¯­éŸ³è¯†åˆ«
```python
import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import librosa

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
model_name = "openai/whisper-base"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name)

# åŠ è½½éŸ³é¢‘æ–‡ä»¶
audio_path = "audio.wav"
audio, sr = librosa.load(audio_path, sr=16000)  # é‡é‡‡æ ·åˆ°16kHz

# é¢„å¤„ç†éŸ³é¢‘
input_features = processor(
    audio,
    sampling_rate=16000,
    return_tensors="pt"
).input_features

# ç”Ÿæˆè½¬å½•
with torch.no_grad():
    predicted_ids = model.generate(input_features)

# è§£ç ç»“æœ
transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
print(f"Transcription: {transcription}")
```

### 2. å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«
```python
def multilingual_asr(audio_path, language="auto"):
    """å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«"""

    # åŠ è½½å¤šè¯­è¨€æ¨¡å‹
    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")
    processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")

    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000)
    input_features = processor(audio, sampling_rate=16000, return_tensors="pt").input_features

    # å¼ºåˆ¶æŒ‡å®šè¯­è¨€æˆ–è‡ªåŠ¨æ£€æµ‹
    forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task="transcribe")

    # ç”Ÿæˆè½¬å½•
    with torch.no_grad():
        predicted_ids = model.generate(
            input_features,
            forced_decoder_ids=forced_decoder_ids
        )

    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription

# ä½¿ç”¨ç¤ºä¾‹
transcription = multilingual_asr("chinese_audio.wav", language="zh")
print(transcription)
```

### 3. ç¿»è¯‘ä»»åŠ¡
```python
def translate_audio(audio_path, source_lang="auto", target_lang="en"):
    """è¯­éŸ³ç¿»è¯‘"""

    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")
    processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")

    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000)
    input_features = processor(audio, sampling_rate=16000, return_tensors="pt").input_features

    # ç¿»è¯‘ä»»åŠ¡
    forced_decoder_ids = processor.get_decoder_prompt_ids(
        language=source_lang,
        task="translate",
        no_timestamps=True
    )

    # ç”Ÿæˆç¿»è¯‘
    with torch.no_grad():
        predicted_ids = model.generate(
            input_features,
            forced_decoder_ids=forced_decoder_ids
        )

    translation = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return translation

# ä½¿ç”¨ç¤ºä¾‹
translation = translate_audio("spanish_audio.wav", source_lang="es", target_lang="en")
print(f"Translation: {translation}")
```

### 4. å¸¦æ—¶é—´æˆ³çš„è½¬å½•
```python
def transcribe_with_timestamps(audio_path):
    """å¸¦æ—¶é—´æˆ³çš„è½¬å½•"""

    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-base")
    processor = WhisperProcessor.from_pretrained("openai/whisper-base")

    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000)
    input_features = processor(audio, sampling_rate=16000, return_tensors="pt").input_features

    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è½¬å½•
    with torch.no_grad():
        predicted_ids = model.generate(
            input_features,
            return_timestamps=True,
            max_new_tokens=448
        )

    # è§£ç åŒ…å«æ—¶é—´æˆ³çš„ç»“æœ
    result = processor.decode(predicted_ids[0], skip_special_tokens=True)

    return result

# è§£ææ—¶é—´æˆ³
def parse_timestamped_transcription(transcription):
    """è§£æå¸¦æ—¶é—´æˆ³çš„è½¬å½•ç»“æœ"""
    import re

    # åŒ¹é…æ—¶é—´æˆ³æ¨¡å¼
    timestamp_pattern = r'\[(\d{2}):(\d{2})\.(\d{3})\]'
    segments = re.split(timestamp_pattern, transcription)

    parsed_segments = []
    for i in range(1, len(segments), 4):
        if i + 3 < len(segments):
            minutes = int(segments[i])
            seconds = int(segments[i + 1])
            milliseconds = int(segments[i + 2])
            text = segments[i + 3].strip()

            start_time = minutes * 60 + seconds + milliseconds / 1000
            parsed_segments.append({
                "start_time": start_time,
                "text": text
            })

    return parsed_segments

# ä½¿ç”¨ç¤ºä¾‹
transcription = transcribe_with_timestamps("long_audio.wav")
segments = parse_timestamped_transcription(transcription)

for segment in segments:
    print(f"[{segment['start_time']:.2f}s] {segment['text']}")
```

### 5. æ‰¹é‡å¤„ç†
```python
def batch_transcribe(audio_paths, model_name="openai/whisper-base", batch_size=8):
    """æ‰¹é‡è¯­éŸ³è¯†åˆ«"""

    model = WhisperForConditionalGeneration.from_pretrained(model_name)
    processor = WhisperProcessor.from_pretrained(model_name)

    results = []

    for i in range(0, len(audio_paths), batch_size):
        batch_paths = audio_paths[i:i+batch_size]
        batch_audio = []

        # åŠ è½½æ‰¹æ¬¡éŸ³é¢‘
        for path in batch_paths:
            audio, sr = librosa.load(path, sr=16000)
            batch_audio.append(audio)

        # é¢„å¤„ç†æ‰¹æ¬¡
        input_features = processor(
            batch_audio,
            sampling_rate=16000,
            return_tensors="pt",
            padding=True
        ).input_features

        # æ‰¹é‡ç”Ÿæˆ
        with torch.no_grad():
            predicted_ids = model.generate(input_features)

        # è§£ç ç»“æœ
        transcriptions = processor.batch_decode(predicted_ids, skip_special_tokens=True)

        for path, transcription in zip(batch_paths, transcriptions):
            results.append({
                "file": path,
                "transcription": transcription
            })

    return results

# ä½¿ç”¨ç¤ºä¾‹
audio_files = ["audio1.wav", "audio2.wav", "audio3.wav"]
results = batch_transcribe(audio_files)
for result in results:
    print(f"{result['file']}: {result['transcription']}")
```

### 6. é•¿éŸ³é¢‘å¤„ç†
```python
def transcribe_long_audio(audio_path, chunk_length_s=30):
    """å¤„ç†é•¿éŸ³é¢‘æ–‡ä»¶"""

    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")
    processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")

    # åŠ è½½é•¿éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000)
    chunk_samples = int(chunk_length_s * sr)

    full_transcription = []

    for i in range(0, len(audio), chunk_samples):
        chunk = audio[i:i+chunk_samples]

        # å¦‚æœæœ€åä¸€chunkå¤ªçŸ­ï¼Œè¿›è¡Œå¡«å……
        if len(chunk) < chunk_samples:
            chunk = np.pad(chunk, (0, chunk_samples - len(chunk)), mode='constant')

        # å¤„ç†chunk
        input_features = processor(chunk, sampling_rate=16000, return_tensors="pt").input_features

        with torch.no_grad():
            predicted_ids = model.generate(input_features)

        chunk_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        full_transcription.append(chunk_transcription)

        # æ‰“å°è¿›åº¦
        chunk_time = i / sr
        print(f"Processed {chunk_time:.1f}s / {len(audio)/sr:.1f}s")

    return " ".join(full_transcription)

# ä½¿ç”¨ç¤ºä¾‹
long_transcription = transcribe_long_audio("meeting_recording.wav")
print(long_transcription)
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æ¨ç†ä¼˜åŒ–
```python
# ä½¿ç”¨FP16æ¨ç†
model = WhisperForConditionalGeneration.from_pretrained(
    "openai/whisper-large-v2",
    torch_dtype=torch.float16
).to("cuda")

# é‡åŒ–æ¨ç†
model = WhisperForConditionalGeneration.from_pretrained(
    "openai/whisper-base",
    load_in_8bit=True,
    device_map="auto"
)

# ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
if hasattr(torch, 'compile'):
    model = torch.compile(model)
```

### 2. ç¼“å­˜ä¼˜åŒ–
```python
class CachedWhisperProcessor:
    def __init__(self, model_name="openai/whisper-base"):
        self.processor = WhisperProcessor.from_pretrained(model_name)
        self.feature_cache = {}

    def process_audio(self, audio_path):
        # æ£€æŸ¥ç¼“å­˜
        import hashlib
        with open(audio_path, 'rb') as f:
            audio_hash = hashlib.md5(f.read()).hexdigest()

        if audio_hash in self.feature_cache:
            return self.feature_cache[audio_hash]

        # å¤„ç†éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=16000)
        input_features = self.processor(audio, sampling_rate=16000, return_tensors="pt").input_features

        # ç¼“å­˜ç»“æœ
        self.feature_cache[audio_hash] = input_features
        return input_features
```

### 3. æµå¼å¤„ç†
```python
class StreamingWhisper:
    def __init__(self, model_name="openai/whisper-base", chunk_duration=2):
        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)
        self.processor = WhisperProcessor.from_pretrained(model_name)
        self.chunk_duration = chunk_duration
        self.sample_rate = 16000
        self.chunk_samples = chunk_duration * self.sample_rate
        self.buffer = np.array([])

    def process_chunk(self, audio_chunk):
        """å¤„ç†å•ä¸ªéŸ³é¢‘å—"""
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.buffer = np.concatenate([self.buffer, audio_chunk])

        # å¦‚æœç¼“å†²åŒºè¶³å¤Ÿå¤§ï¼Œè¿›è¡Œå¤„ç†
        if len(self.buffer) >= self.chunk_samples:
            process_chunk = self.buffer[:self.chunk_samples]
            self.buffer = self.buffer[self.chunk_samples:]

            # å¤„ç†éŸ³é¢‘
            input_features = self.processor(
                process_chunk,
                sampling_rate=self.sample_rate,
                return_tensors="pt"
            ).input_features

            with torch.no_grad():
                predicted_ids = self.model.generate(input_features)

            transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
            return transcription

        return None
```

## æ¨¡å‹å˜ä½“

### 1. ä¸åŒè§„æ¨¡
- **whisper-tiny**: 39Må‚æ•°ï¼Œæœ€å¿«é€Ÿåº¦
- **whisper-base**: 74Må‚æ•°ï¼Œå¹³è¡¡æ€§èƒ½
- **whisper-small**: 244Må‚æ•°ï¼Œæ›´å¥½è´¨é‡
- **whisper-medium**: 769Må‚æ•°ï¼Œé«˜è´¨é‡
- **whisper-large-v2**: 1.55Bå‚æ•°ï¼Œæœ€é«˜è´¨é‡

### 2. è¯­è¨€æ”¯æŒ
- **è‹±è¯­æ¨¡å‹**: ä¸“é—¨ä¼˜åŒ–è‹±è¯­è¯†åˆ«
- **å¤šè¯­è¨€æ¨¡å‹**: æ”¯æŒ100+ç§è¯­è¨€
- **ç¿»è¯‘æ¨¡å‹**: ä¼˜åŒ–ç¿»è¯‘ä»»åŠ¡

## æœ€ä½³å®è·µ

### 1. éŸ³é¢‘é¢„å¤„ç†
```python
def optimal_audio_preprocessing(audio_path, target_sr=16000):
    """æœ€ä¼˜éŸ³é¢‘é¢„å¤„ç†"""
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=target_sr)

    # é™å™ª
    if sr > 0:
        # ç®€å•çš„é«˜é€šæ»¤æ³¢
        from scipy import signal
        sos = signal.butter(10, 80, btype='high', fs=sr, output='sos')
        audio = signal.sosfilt(sos, audio)

    # éŸ³é‡æ ‡å‡†åŒ–
    audio = audio / np.max(np.abs(audio)) * 0.95

    return audio, sr
```

### 2. ç”Ÿæˆå‚æ•°è°ƒä¼˜
```python
def optimized_generation(model, input_features):
    """ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°"""
    return model.generate(
        input_features,
        max_new_tokens=448,           # é™åˆ¶æœ€å¤§é•¿åº¦
        num_beams=5,                  # æŸæœç´¢æé«˜è´¨é‡
        temperature=0.0,              # ç¡®å®šæ€§ç”Ÿæˆ
        no_repeat_ngram_size=3,       # é¿å…é‡å¤
        early_stopping=True,          # æ—©åœ
        condition_on_prev_tokens=False, # æé«˜é€Ÿåº¦
    )
```

### 3. é”™è¯¯å¤„ç†
```python
def robust_transcribe(audio_path, max_retries=3):
    """é²æ£’çš„è¯­éŸ³è¯†åˆ«"""
    for attempt in range(max_retries):
        try:
            # é¢„å¤„ç†éŸ³é¢‘
            audio, sr = optimal_audio_preprocessing(audio_path)

            # æ£€æŸ¥éŸ³é¢‘è´¨é‡
            if np.max(np.abs(audio)) < 0.01:
                raise ValueError("Audio too quiet or silent")

            # å°è¯•è½¬å½•
            transcription = transcribe_with_whisper(audio)
            return transcription

        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                raise
            # å°è¯•ä¸åŒçš„é¢„å¤„ç†å‚æ•°
            time.sleep(1)
```

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•æé«˜ä¸­æ–‡è¯†åˆ«å‡†ç¡®ç‡ï¼Ÿ
A: æ–¹æ³•ï¼š
- ä½¿ç”¨large-v2æ¨¡å‹
- æ˜ç¡®æŒ‡å®šè¯­è¨€å‚æ•°
- ç¡®ä¿éŸ³é¢‘è´¨é‡è‰¯å¥½
- è€ƒè™‘ä½¿ç”¨ä¸“ä¸šä¸­æ–‡æ¨¡å‹

### Q: å¦‚ä½•å¤„ç†å®æ—¶è¯­éŸ³è¯†åˆ«ï¼Ÿ
A: ç­–ç•¥ï¼š
- ä½¿ç”¨chunkå¤„ç†
- é€‰æ‹©å°æ¨¡å‹(tiny/base)
- ä½¿ç”¨æµå¼å¤„ç†æ¶æ„
- ä¼˜åŒ–é¢„å¤„ç†æ­¥éª¤

### Q: Whisperä¸å•†ä¸šASRç›¸æ¯”å¦‚ä½•ï¼Ÿ
A: ä¼˜åŠ¿ï¼š
- å¼€æºå…è´¹
- å¤šè¯­è¨€æ”¯æŒ
- æ— éœ€ç‰¹å®šæ•°æ®è®­ç»ƒ
- é²æ£’æ€§å¥½
åŠ£åŠ¿ï¼š
- å»¶è¿Ÿè¾ƒé«˜
- ä¸“ä¸šé¢†åŸŸæ€§èƒ½æœ‰é™

### Q: å¦‚ä½•å¾®è°ƒWhisperï¼Ÿ
A: æ­¥éª¤ï¼š
- å‡†å¤‡ç‰¹å®šé¢†åŸŸæ•°æ®
- è°ƒæ•´å­¦ä¹ ç‡ç­–ç•¥
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
- ç›‘æ§è¿‡æ‹Ÿåˆ

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ–‡ä»¶
- `modeling_whisper.py`: 1572è¡Œï¼ŒåŒ…å«å®Œæ•´çš„Whisperå®ç°
- `configuration_whisper.py`: WhisperConfigé…ç½®ç±»
- `processing_whisper.py`: éŸ³é¢‘å¤„ç†å™¨
- `feature_extraction_whisper.py`: ç‰¹å¾æå–å™¨
- `generation_whisper.py`: ç”Ÿæˆç­–ç•¥
- `tokenization_whisper.py`: æ–‡æœ¬åˆ†è¯å™¨

### è¾…åŠ©æ–‡ä»¶
- `english_normalizer.py`: è‹±æ–‡æ–‡æœ¬è§„èŒƒåŒ–
- `convert_openai_to_hf.py`: OpenAIæƒé‡è½¬æ¢

### æµ‹è¯•æ–‡ä»¶
- `tests/test_modeling_whisper.py`: Whisperæ¨¡å‹æµ‹è¯•
- `tests/test_processing_whisper.py`: å¤„ç†å™¨æµ‹è¯•

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - è¯¦ç»†åˆ†æ
- âœ¨ å®ŒæˆWhisperæ¨¡å‹æ ¸å¿ƒç»„ä»¶åˆ†æ
- ğŸ” è®°å½•è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘çš„å®ç°æœºåˆ¶
- ğŸ“Š åˆ†æé…ç½®å‚æ•°å’Œæœ€ä½³å®è·µ
- ğŸ¯ æä¾›å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹å’Œä¼˜åŒ–æ–¹æ³•

### ä¸‹ä¸€æ­¥è®¡åˆ’
- [ ] åˆ†æWhisperåœ¨ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨
- [ ] åˆ›å»ºè¯­éŸ³è¯†åˆ«ç³»ç»Ÿéƒ¨ç½²æŒ‡å—
- [ ] è®°å½•Whisperä¸å…¶ä»–ASRç³»ç»Ÿçš„å¯¹æ¯”
- [ ] åˆ†æå®æ—¶è¯­éŸ³è¯†åˆ«çš„æŠ€æœ¯æ–¹æ¡ˆ

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 95%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 98%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20