[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > [src](/Users/berton/Github/transformers/src/CLAUDE.md) > [transformers](/Users/berton/Github/transformers/src/transformers/CLAUDE.md) > **models**

# Models æ¨¡å—æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `src/transformers/models/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: æ­£åœ¨åˆ†æ...

## æ¨¡å—èŒè´£

Modelsæ¨¡å—æ˜¯Transformersçš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…å«100+é¢„è®­ç»ƒæ¨¡å‹çš„å®ç°ï¼Œè´Ÿè´£ï¼š

1. **æ¨¡å‹æ¶æ„**: å„ç§Transformerå˜ä½“çš„å…·ä½“å®ç°
2. **é…ç½®ç®¡ç†**: æ¯ä¸ªæ¨¡å‹çš„å‚æ•°å’Œè¶…å‚æ•°é…ç½®
3. **é¢„å¤„ç†**: æ¨¡å‹ç‰¹å®šçš„æ•°æ®é¢„å¤„ç†å’Œåˆ†è¯å™¨
4. **æƒé‡è½¬æ¢**: ä»åŸå§‹æ ¼å¼åˆ°Transformersæ ¼å¼çš„è½¬æ¢
5. **æ¨¡å—åŒ–æ”¯æŒ**: æ–°çš„æ¨¡å—åŒ–æ¨¡å‹æ¶æ„æ”¯æŒ

## æ¨¡å‹åˆ†ç±»

### ğŸ§  è¯­è¨€æ¨¡å‹ (Language Models)

#### Encoder-onlyæ¨¡å‹
- **BERT**: åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼Œé€‚ç”¨äºç†è§£ä»»åŠ¡
- **RoBERTa**: ä¼˜åŒ–çš„BERTè®­ç»ƒæ–¹æ³•
- **ALBERT**: è½»é‡çº§BERTï¼Œå‚æ•°å…±äº«
- **DistilBERT**: çŸ¥è¯†è’¸é¦çš„è½»é‡çº§BERT
- **DeBERTa**: è§£è€¦æ³¨æ„åŠ›æœºåˆ¶çš„BERT

#### Decoder-onlyæ¨¡å‹
- **GPTç³»åˆ—**: GPT, GPT-2, GPT-3é£æ ¼çš„ç”Ÿæˆæ¨¡å‹
- **BLOOM**: å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹
- **Llamaç³»åˆ—**: Metaçš„å¼€æºè¯­è¨€æ¨¡å‹
- **Mistral**: Mistral AIçš„é«˜æ•ˆè¯­è¨€æ¨¡å‹
- **Phiç³»åˆ—**: Microsoftçš„å°å‹è¯­è¨€æ¨¡å‹

#### Encoder-Decoderæ¨¡å‹
- **BART**: å»å™ªè‡ªç¼–ç å™¨ï¼Œé€‚ç”¨äºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡
- **T5**: æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨
- **Pegasus**: ä¸“ä¸ºæ‘˜è¦ä¼˜åŒ–çš„æ¨¡å‹
- **LED**: Longformerçš„ç¼–ç å™¨-è§£ç å™¨ç‰ˆæœ¬

### ğŸ‘ï¸ è§†è§‰æ¨¡å‹ (Vision Models)

#### å›¾åƒåˆ†ç±»
- **ViT**: Vision Transformer
- **DeiT**: Data-efficient Vision Transformers
- **BEiT**: æ©ç å›¾åƒå»ºæ¨¡çš„è§†è§‰æ¨¡å‹
- **ConvNeXt**: çº¯å·ç§¯ç½‘ç»œï¼Œå¯¹æ ‡Transformer

#### ç›®æ ‡æ£€æµ‹
- **DETR**: åŸºäºTransformerçš„ç«¯åˆ°ç«¯ç›®æ ‡æ£€æµ‹
- **Deformable DETR**: å¯å˜å½¢DETR
- **Conditional DETR**: æ¡ä»¶DETR

#### å›¾åƒåˆ†å‰²
- **Segmenter**: ç”¨äºåˆ†å‰²çš„Transformer
- **MaskFormer**: æ©ç è¡¨ç¤ºçš„åˆ†å‰²
- **DINOv2**: è‡ªç›‘ç£è§†è§‰æ¨¡å‹

### ğŸµ å¤šæ¨¡æ€æ¨¡å‹ (Multimodal Models)

#### è§†è§‰-è¯­è¨€
- **CLIP**: å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ
- **BLIP**: å›¾åƒ-è¯­è¨€é¢„è®­ç»ƒ
- **FLAVA**: å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹
- **LLaVA**: å¤§å‹è¯­è¨€è§†è§‰åŠ©æ‰‹

#### éŸ³é¢‘-æ–‡æœ¬
- **Whisper**: OpenAIçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹
- **Wav2Vec2**: Facebookçš„è¯­éŸ³æ¨¡å‹
- **HuBERT**: éšè—å•å…ƒBERT
- **Data2Vec**: ç»Ÿä¸€çš„å¤šæ¨¡æ€é¢„è®­ç»ƒ

#### è§†é¢‘
- **VideoMAE**: è§†é¢‘æ©ç è‡ªç¼–ç å™¨
- **TimeSformer**: ç”¨äºè§†é¢‘çš„æ—¶ç©ºTransformer

### ğŸ”§ ç‰¹æ®Šæ¶æ„ (Specialized Architectures)

#### ç”Ÿç‰©å­¦
- **ESM**: æ¼”åŒ–è§„æ¨¡å»ºæ¨¡çš„è›‹ç™½è´¨æ¨¡å‹
- **ProtBERT**: è›‹ç™½è´¨BERT

#### æ—¶é—´åºåˆ—
- **Time Series Transformer**: æ—¶é—´åºåˆ—é¢„æµ‹
- **Informer**: é•¿åºåˆ—æ—¶é—´åºåˆ—é¢„æµ‹

#### å¼ºåŒ–å­¦ä¹ 
- **Decision Transformer**: ç”¨äºå¼ºåŒ–å­¦ä¹ çš„å†³ç­–Transformer
- **Trajectory Transformer**: è½¨è¿¹é¢„æµ‹

## æ ¸å¿ƒæ¶æ„æ¨¡å¼

### 1. æ ‡å‡†æ¨¡å‹ç»“æ„
æ¯ä¸ªæ¨¡å‹é€šå¸¸åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š
```
model_name/
â”œâ”€â”€ __init__.py                    # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ configuration_model_name.py    # é…ç½®ç±»
â”œâ”€â”€ modeling_model_name.py        # æ¨¡å‹å®ç°
â”œâ”€â”€ tokenization_model_name.py    # åˆ†è¯å™¨ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ tokenization_model_name_fast.py  # å¿«é€Ÿåˆ†è¯å™¨ï¼ˆå¯é€‰ï¼‰
â””â”€â”€ convert_*.py                  # æƒé‡è½¬æ¢è„šæœ¬ï¼ˆå¯é€‰ï¼‰
```

### 2. é…ç½®ç±»æ¨¡å¼
```python
class ModelConfig(PreTrainedConfig):
    model_type = "model_name"

    def __init__(
        self,
        vocab_size=30522,
        hidden_size=768,
        num_hidden_layers=12,
        num_attention_heads=12,
        # ... å…¶ä»–å‚æ•°
        **kwargs
    ):
        super().__init__(**kwargs)
        # å‚æ•°èµ‹å€¼
```

### 3. æ¨¡å‹ç±»æ¨¡å¼
```python
class ModelNameModel(ModelNamePreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        # æ¨¡å‹ç»„ä»¶åˆå§‹åŒ–

    def forward(self, input_ids, attention_mask=None, **kwargs):
        # å‰å‘ä¼ æ’­é€»è¾‘
        return outputs
```

## å…³é”®æ¨¡å‹ç¤ºä¾‹

### BERTç³»åˆ— (bert/)
```python
# æ–‡ä»¶ç»“æ„
bert/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ configuration_bert.py      # BertConfig
â”œâ”€â”€ modeling_bert.py          # BertModel, BertForSequenceClassificationç­‰
â”œâ”€â”€ tokenization_bert.py      # BertTokenizer
â”œâ”€â”€ tokenization_bert_fast.py # BertTokenizerFast
â””â”€â”€ convert_*.py              # TensorFlowåˆ°PyTorchè½¬æ¢

# æ ¸å¿ƒç»„ä»¶
- BertEmbeddings: è¯åµŒå…¥ã€ä½ç½®åµŒå…¥ã€æ®µåµŒå…¥
- BertSelfAttention: å¤šå¤´è‡ªæ³¨æ„åŠ›
- BertSelfOutput: æ³¨æ„åŠ›è¾“å‡ºå¤„ç†
- BertIntermediate: å‰é¦ˆç½‘ç»œ
- BertOutput: è¾“å‡ºå±‚å¤„ç†
- BertPooler: [CLS] tokenæ± åŒ–
```

### GPTç³»åˆ— (gpt2/, gpt_neo/, llama/)
```python
# ç‰¹ç‚¹
- å› æœè‡ªæ³¨æ„åŠ›æ©ç 
- ç”Ÿæˆä¼˜åŒ–
- å¤§è§„æ¨¡å‚æ•°æ”¯æŒ
- æ—‹è½¬ä½ç½®ç¼–ç (RoPE)

# æ ¸å¿ƒç»„ä»¶
- GPT2Block: Transformerå—
- GPT2Attention: å› æœæ³¨æ„åŠ›
- GPT2MLP: å‰é¦ˆç½‘ç»œ
```

### CLIPç³»åˆ— (clip/)
```python
# åŒå¡”æ¶æ„
- CLIPTextModel: æ–‡æœ¬ç¼–ç å™¨
- CLIPVisionModel: å›¾åƒç¼–ç å™¨
- CLIPModel: å¯¹æ¯”å­¦ä¹ æ¨¡å‹

# å…³é”®ç‰¹æ€§
- å›¾åƒ-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ 
- é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
- æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆ
```

### ViTç³»åˆ— (vit/)
```python
# Vision Transformeræ ¸å¿ƒ
- ViTEmbeddings: å›¾åƒå—åµŒå…¥
- ViTAttention: å›¾åƒæ³¨æ„åŠ›
- ViTLayer: Transformerå±‚
- ViTModel: å®Œæ•´æ¨¡å‹

# ç‰¹ç‚¹
- å›¾åƒå—åˆ‡åˆ†
- ä½ç½®ç¼–ç 
- åˆ†ç±»token
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€æ¨¡å‹åŠ è½½
```python
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# ç¼–ç è¾“å…¥
inputs = tokenizer("Hello, world!", return_tensors="pt")
outputs = model(**inputs)
```

### 2. ä»»åŠ¡ç‰¹å®šæ¨¡å‹
```python
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForQuestionAnswering,
    AutoModelForTokenClassification
)

# åºåˆ—åˆ†ç±»
classifier = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
outputs = classifier(**inputs)

# é—®ç­”
qa_model = AutoModelForQuestionAnswering.from_pretrained("bert-base-uncased")
outputs = qa_model(**inputs)

# æ ‡è®°åˆ†ç±»
ner_model = AutoModelForTokenClassification.from_pretrained("bert-base-uncased")
outputs = ner_model(**inputs)
```

### 3. å¤šæ¨¡æ€æ¨¡å‹
```python
from transformers import AutoProcessor, AutoModelForVision2Seq

# å›¾åƒæè¿°ç”Ÿæˆ
processor = AutoProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
model = AutoModelForVision2Seq.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

# å¤„ç†å›¾åƒå’Œæ–‡æœ¬
inputs = processor(images=image, text="A photo of", return_tensors="pt")
outputs = model.generate(**inputs)
```

### 4. è‡ªå®šä¹‰é…ç½®
```python
from transformers import BertConfig, BertModel

# è‡ªå®šä¹‰é…ç½®
config = BertConfig(
    vocab_size=50000,
    hidden_size=1024,
    num_hidden_layers=24,
    num_attention_heads=16
)

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆ›å»ºæ¨¡å‹
model = BertModel(config)
```

## æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯

### 1. é‡åŒ–æ”¯æŒ
```python
# 8ä½é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    load_in_8bit=True,
    device_map="auto"
)

# 4ä½é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
```

### 2. Flash Attention
```python
# å¯ç”¨Flash Attentionä¼˜åŒ–
model = AutoModel.from_pretrained(
    "model_name",
    use_flash_attention_2=True
)
```

### 3. æ¨¡å‹å¹¶è¡Œ
```python
# è®¾å¤‡æ˜ å°„
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    device_map="auto",
    torch_dtype=torch.float16
)
```

## æ¨¡å‹è½¬æ¢

### 1. æƒé‡æ ¼å¼è½¬æ¢
```python
# TensorFlowåˆ°PyTorch
python convert_bert_original_tf_checkpoint_to_pytorch.py \
    --tf_checkpoint_path bert_model.ckpt \
    --bert_config_file bert_config.json \
    --pytorch_dump_path pytorch_model.bin
```

### 2. æ¨¡å‹å¯¼å‡º
```python
# ONNXå¯¼å‡º
from transformers import AutoModel
import torch

model = AutoModel.from_pretrained("bert-base-uncased")
dummy_input = torch.randint(0, 1000, (1, 10))
torch.onnx.export(model, dummy_input, "model.onnx")
```

## æµ‹è¯•ç­–ç•¥

### 1. æ¨¡å‹ä¸€è‡´æ€§æµ‹è¯•
- æƒé‡åŠ è½½ä¸€è‡´æ€§
- è¾“å‡ºæ•°å€¼ä¸€è‡´æ€§
- ä¸åŸå§‹å®ç°çš„å¯¹æ¯”

### 2. æ€§èƒ½æµ‹è¯•
- æ¨ç†é€Ÿåº¦æµ‹è¯•
- å†…å­˜ä½¿ç”¨æµ‹è¯•
- å¤§è§„æ¨¡æ¨¡å‹ç¨³å®šæ€§æµ‹è¯•

### 3. ä»»åŠ¡ç‰¹å®šæµ‹è¯•
- ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æµ‹è¯•
- å¾®è°ƒæ”¶æ•›æ€§æµ‹è¯•

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ
A: æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©ï¼š
- **æ–‡æœ¬ç†è§£**: BERT, RoBERTa, DeBERTa
- **æ–‡æœ¬ç”Ÿæˆ**: GPTç³»åˆ—, Llama, Mistral
- **æ–‡æœ¬åˆ†ç±»**: DistilBERT, ALBERTï¼ˆè½»é‡çº§é€‰é¡¹ï¼‰
- **å¤šæ¨¡æ€**: CLIP, BLIP, LLaVA

### Q: å¦‚ä½•å¤„ç†å¤§å‹æ¨¡å‹ï¼Ÿ
A: ä½¿ç”¨ä»¥ä¸‹æŠ€æœ¯ï¼š
- é‡åŒ–ï¼š`load_in_4bit=True`æˆ–`load_in_8bit=True`
- æ¨¡å‹å¹¶è¡Œï¼š`device_map="auto"`
- Flash Attentionï¼š`use_flash_attention_2=True`
- æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼š`gradient_checkpointing=True`

### Q: å¦‚ä½•æ·»åŠ æ–°æ¨¡å‹ï¼Ÿ
A: éµå¾ªæ ‡å‡†æ¨¡æ¿ï¼š
1. åˆ›å»ºé…ç½®ç±»ç»§æ‰¿`PreTrainedConfig`
2. åˆ›å»ºæ¨¡å‹ç±»ç»§æ‰¿`PreTrainedModel`
3. å®ç°æ ‡å‡†æ–¹æ³•ï¼š`__init__`, `forward`
4. æ·»åŠ è½¬æ¢è„šæœ¬ï¼ˆå¦‚éœ€è¦ï¼‰
5. ç¼–å†™æµ‹è¯•å’Œæ–‡æ¡£

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ¨¡å‹ç±»åˆ«æ¸…å•ï¼ˆéƒ¨åˆ†ï¼‰

#### è¯­è¨€æ¨¡å‹
- `bert/` - BERTåŠå…¶å˜ä½“
- `roberta/` - RoBERTaæ¨¡å‹
- `gpt2/` - GPT-2æ¨¡å‹
- `llama/` - Llamaç³»åˆ—æ¨¡å‹
- `mistral/` - Mistralæ¨¡å‹
- `t5/` - T5æ¨¡å‹
- `bart/` - BARTæ¨¡å‹

#### è§†è§‰æ¨¡å‹
- `vit/` - Vision Transformer
- `detr/` - DETRç›®æ ‡æ£€æµ‹
- `beit/` - BEiTæ¨¡å‹
- `clip/` - CLIPå¤šæ¨¡æ€æ¨¡å‹

#### éŸ³é¢‘æ¨¡å‹
- `wav2vec2/` - Wav2Vec2è¯­éŸ³æ¨¡å‹
- `whisper/` - Whisperè¯­éŸ³è¯†åˆ«
- `hubert/` - HuBERTéŸ³é¢‘æ¨¡å‹

#### ç‰¹æ®Šæ¶æ„
- `deberta_v2/` - DeBERTa v2
- `distilbert/` - DistilBERT
- `albert/` - ALBERTè½»é‡çº§æ¨¡å‹

#### è¾…åŠ©æ¨¡å‹
- `auto/` - è‡ªåŠ¨æ¨¡å‹é€‰æ‹©
- `deprecated/` - å·²å¼ƒç”¨æ¨¡å‹

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - åˆå§‹åˆ†æ
- âœ¨ åˆ›å»ºmodelsæ¨¡å—æ¦‚è§ˆæ–‡æ¡£
- ğŸ” åˆ†ææ¨¡å‹åˆ†ç±»å’Œæ¶æ„æ¨¡å¼
- ğŸ“Š è®°å½•ä¸»è¦æ¨¡å‹ç³»åˆ—ç‰¹ç‚¹
- ğŸ¯ ç¡®å®šè¿›ä¸€æ­¥åˆ†æçš„é‡ç‚¹æ¨¡å‹

### ä¸‹ä¸€æ­¥è®¡åˆ’
- [ ] è¯¦ç»†åˆ†ææ ¸å¿ƒæ¨¡å‹ï¼ˆBERT, GPT, CLIPç­‰ï¼‰
- [ ] åˆ›å»ºæ¯ä¸ªæ¨¡å‹çš„ä¸“é—¨æ–‡æ¡£
- [ ] è®°å½•æ¨¡å‹é—´çš„è½¬æ¢å’Œè¿ç§»
- [ ] åˆ†ææ¨¡å‹æ€§èƒ½åŸºå‡†å’Œæœ€ä½³å®è·µ

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: æ­£åœ¨åˆ†æ...
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 98%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20