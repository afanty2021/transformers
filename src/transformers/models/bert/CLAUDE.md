[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > [src](/Users/berton/Github/transformers/src/CLAUDE.md) > [transformers](/Users/berton/Github/transformers/src/transformers/CLAUDE.md) > [models](/Users/berton/Github/transformers/src/transformers/models/CLAUDE.md) > **bert**

# BERT æ¨¡å‹æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `src/transformers/models/bert/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 95%

## æ¨¡å—èŒè´£

BERT (Bidirectional Encoder Representations from Transformers) æ˜¯Googleæå‡ºçš„é©å‘½æ€§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **åŒå‘ç¼–ç **: ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡(MLM)å®ç°åŒå‘ä¸Šä¸‹æ–‡ç†è§£
- **é¢„è®­ç»ƒ-å¾®è°ƒ**: åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šé¢„è®­ç»ƒï¼Œç„¶ååœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒ
- **å¤šä»»åŠ¡æ”¯æŒ**: æ”¯æŒåˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€é—®ç­”ç­‰å¤šç§NLPä»»åŠ¡
- **å˜ä½“ä¸°å¯Œ**: åŒ…å«RoBERTaã€ALBERTã€DistilBERTç­‰å¤šä¸ªä¼˜åŒ–ç‰ˆæœ¬

## æ–‡ä»¶ç»“æ„

```
bert/
â”œâ”€â”€ __init__.py                                    # æ¨¡å—å¯¼å‡ºå’Œæ¨¡å‹æ˜ å°„
â”œâ”€â”€ configuration_bert.py                          # BertConfigé…ç½®ç±»
â”œâ”€â”€ modeling_bert.py                              # æ ¸å¿ƒæ¨¡å‹å®ç°
â”œâ”€â”€ tokenization_bert.py                          # BERTåˆ†è¯å™¨
â”œâ”€â”€ tokenization_bert_fast.py                     # Fast BERTåˆ†è¯å™¨
â”œâ”€â”€ convert_bert_original_tf_checkpoint_to_pytorch.py  # TensorFlowæƒé‡è½¬æ¢
â”œâ”€â”€ convert_bert_original_tf2_checkpoint_to_pytorch.py # TensorFlow 2.xè½¬æ¢
â””â”€â”€ convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py # Token droppingè½¬æ¢
```

## æ ¸å¿ƒç»„ä»¶åˆ†æ

### 1. é…ç½®ç±» (BertConfig)

```python
class BertConfig(PreTrainedConfig):
    model_type = "bert"

    def __init__(
        self,
        vocab_size=30522,              # è¯æ±‡è¡¨å¤§å°
        hidden_size=768,               # éšè—å±‚ç»´åº¦
        num_hidden_layers=12,          # Transformerå±‚æ•°
        num_attention_heads=12,        # æ³¨æ„åŠ›å¤´æ•°
        intermediate_size=3072,        # å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦
        hidden_act="gelu",             # æ¿€æ´»å‡½æ•°
        hidden_dropout_prob=0.1,       # éšè—å±‚dropout
        attention_probs_dropout_prob=0.1,  # æ³¨æ„åŠ›dropout
        max_position_embeddings=512,   # æœ€å¤§åºåˆ—é•¿åº¦
        type_vocab_size=2,             # æ®µç±»å‹æ•°é‡
        initializer_range=0.02,        # åˆå§‹åŒ–èŒƒå›´
        layer_norm_eps=1e-12,          # LayerNorm epsilon
        pad_token_id=0,                # PAD token ID
        position_embedding_type="absolute",  # ä½ç½®ç¼–ç ç±»å‹
        use_cache=True,                # æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        classifier_dropout=None,       # åˆ†ç±»å™¨dropout
        **kwargs
    ):
        super().__init__(**kwargs)
        # å‚æ•°èµ‹å€¼...
```

**å…³é”®é…ç½®å‚æ•°**:
- `vocab_size`: æ”¯æŒçš„è¯æ±‡æ•°é‡
- `hidden_size`: æ¨¡å‹çš„åŸºç¡€ç»´åº¦ï¼Œå½±å“è¡¨ç¤ºèƒ½åŠ›
- `num_hidden_layers`: Transformerå—çš„æ•°é‡ï¼Œå†³å®šæ¨¡å‹æ·±åº¦
- `num_attention_heads`: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
- `max_position_embeddings`: æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦

### 2. æ ¸å¿ƒæ¨¡å‹ç»„ä»¶

#### BertEmbeddings - åµŒå…¥å±‚
```python
class BertEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)

        # LayerNormå’ŒDropout
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
```

**åŠŸèƒ½**:
- è¯åµŒå…¥ï¼šå°†tokenè½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
- ä½ç½®åµŒå…¥ï¼šè¡¨ç¤ºtokenåœ¨åºåˆ—ä¸­çš„ä½ç½®
- æ®µåµŒå…¥ï¼šåŒºåˆ†ä¸åŒå¥å­(ç”¨äºNSPä»»åŠ¡)
- å±‚å½’ä¸€åŒ–ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹

#### BertSelfAttention - è‡ªæ³¨æ„åŠ›æœºåˆ¶
```python
class BertSelfAttention(nn.Module):
    def __init__(self, config, position_embedding_type=None):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError("hidden_size must be divisible by num_attention_heads")

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        # Q, K, Vçº¿æ€§å˜æ¢
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        # Dropoutå’Œä½ç½®ç¼–ç 
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.position_embedding_type = position_embedding_type
```

**æ ¸å¿ƒæœºåˆ¶**:
- å¤šå¤´æ³¨æ„åŠ›ï¼šå°†æ³¨æ„åŠ›åˆ†æˆå¤šä¸ª"å¤´"
- ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼šé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
- ä½ç½®æ„ŸçŸ¥ï¼šæ”¯æŒç›¸å¯¹å’Œç»å¯¹ä½ç½®ç¼–ç 
- æ³¨æ„åŠ›æ©ç ï¼šå¤„ç†paddingå’Œå› æœæ©ç 

#### BertLayer - Transformerå±‚
```python
class BertLayer(GradientCheckpointingLayer):
    def __init__(self, config):
        super().__init__()
        self.attention = BertAttention(config)
        self.intermediate = BertIntermediate(config)
        self.output = BertOutput(config)

        # äº¤å‰æ³¨æ„åŠ›(å¯é€‰)
        if config.add_cross_attention:
            self.crossattention = BertAttention(config, is_cross_attention=True)
```

**ç»“æ„**:
- è‡ªæ³¨æ„åŠ›å­å±‚ï¼šæ•è·åºåˆ—å†…ä¾èµ–å…³ç³»
- å‰é¦ˆç½‘ç»œå­å±‚ï¼šéçº¿æ€§å˜æ¢
- æ®‹å·®è¿æ¥å’ŒLayerNormï¼šç¨³å®šè®­ç»ƒ
- äº¤å‰æ³¨æ„åŠ›ï¼šæ”¯æŒencoder-decoderæ¶æ„

### 3. ä»»åŠ¡ç‰¹å®šæ¨¡å‹

#### BertForSequenceClassification - åºåˆ—åˆ†ç±»
```python
class BertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        # BERTä¸»ä½“
        self.bert = BertModel(config)
        # åˆ†ç±»å™¨
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        # æƒé‡åˆå§‹åŒ–
        self.post_init()
```

**æ”¯æŒä»»åŠ¡**:
- æƒ…æ„Ÿåˆ†æ
- ä¸»é¢˜åˆ†ç±»
- å¥å­å¯¹åˆ†ç±»
- é‡å¤å¥å­æ£€æµ‹

#### BertForTokenClassification - æ ‡è®°åˆ†ç±»
```python
class BertForTokenClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        # æ¯ä¸ªtokençš„åˆ†ç±»å™¨
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
```

**æ”¯æŒä»»åŠ¡**:
- å‘½åå®ä½“è¯†åˆ«(NER)
- è¯æ€§æ ‡æ³¨(POS)
- åˆ†å—è¯†åˆ«
- è¯­ä¹‰è§’è‰²æ ‡æ³¨

#### BertForQuestionAnswering - é—®ç­”ä»»åŠ¡
```python
class BertForQuestionAnswering(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        self.bert = BertModel(config)
        # QAè¾“å‡ºå±‚ï¼šstartå’Œendä½ç½®
        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)
```

**åŠŸèƒ½**:
- æŠ½å–å¼é—®ç­”
- å¼€å§‹å’Œç»“æŸä½ç½®é¢„æµ‹
- æ”¯æŒSQuADæ ¼å¼çš„æ•°æ®é›†

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€ä½¿ç”¨
```python
from transformers import BertModel, BertTokenizer

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# ç¼–ç è¾“å…¥
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

# è·å–æ¨¡å‹è¾“å‡º
outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state
pooler_output = outputs.pooler_output
```

### 2. åºåˆ—åˆ†ç±»
```python
from transformers import BertForSequenceClassification

# åŠ è½½åˆ†ç±»æ¨¡å‹
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2  # äºŒåˆ†ç±»
)

# å‰å‘ä¼ æ’­
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits
```

### 3. é—®ç­”ä»»åŠ¡
```python
from transformers import BertForQuestionAnswering

# åŠ è½½é—®ç­”æ¨¡å‹
model = BertForQuestionAnswering.from_pretrained("bert-base-uncased")

# é—®ç­”è¾“å…¥
question = "What is the capital of France?"
context = "France is a country in Europe. Paris is its capital."
inputs = tokenizer(question, context, return_tensors="pt")

# è·å–ç­”æ¡ˆ
outputs = model(**inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

# æå–ç­”æ¡ˆ
answer_start = torch.argmax(start_logits)
answer_end = torch.argmax(end_logits)
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end+1])
)
```

### 4. è‡ªå®šä¹‰é…ç½®
```python
from transformers import BertConfig, BertForSequenceClassification

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = BertConfig(
    vocab_size=50000,
    hidden_size=1024,
    num_hidden_layers=24,
    num_attention_heads=16,
    intermediate_size=4096,
    max_position_embeddings=1024
)

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆ›å»ºæ¨¡å‹
model = BertForSequenceClassification(config)
```

### 5. å¾®è°ƒç¤ºä¾‹
```python
from transformers import Trainer, TrainingArguments

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
)

# åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

## æ€§èƒ½ä¼˜åŒ–

### 1. é‡åŒ–ä¼˜åŒ–
```python
# 8ä½é‡åŒ–
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    load_in_8bit=True,
    device_map="auto"
)

# 4ä½é‡åŒ–
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
```

### 2. Flash Attention
```python
# å¯ç”¨Flash Attention 2
model = BertModel.from_pretrained(
    "bert-base-uncased",
    use_flash_attention_2=True,
    torch_dtype=torch.float16
)
```

### 3. æ¢¯åº¦æ£€æŸ¥ç‚¹
```python
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    gradient_checkpointing=True
)
```

## æ¨¡å‹å˜ä½“

### 1. RoBERTa
- **ä¼˜åŒ–ç‚¹**: æ›´é•¿æ—¶é—´çš„è®­ç»ƒã€æ›´å¤§æ‰¹å¤§å°ã€åŠ¨æ€æ©ç 
- **æ€§èƒ½**: åœ¨GLUEåŸºå‡†ä¸Šè¶…è¶ŠBERT
- **ä½¿ç”¨**: `roberta-base`, `roberta-large`

### 2. DistilBERT
- **ç‰¹ç‚¹**: çŸ¥è¯†è’¸é¦çš„è½»é‡ç‰ˆæœ¬ï¼Œå‚æ•°å‡å°‘40%
- **æ€§èƒ½**: ä¿æŒ97%çš„BERTæ€§èƒ½
- **ä½¿ç”¨**: `distilbert-base-uncased`

### 3. ALBERT
- **æŠ€æœ¯**: å‚æ•°å…±äº«ã€å› å­åˆ†è§£åµŒå…¥
- **ä¼˜åŠ¿**: å¤§å¹…å‡å°‘å‚æ•°æ•°é‡
- **ä½¿ç”¨**: `albert-base-v2`, `albert-large-v2`

### 4. DeBERTa
- **åˆ›æ–°**: è§£è€¦æ³¨æ„åŠ›æœºåˆ¶
- **æå‡**: æ›´å¥½çš„ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›
- **ä½¿ç”¨**: `microsoft/deberta-base`

## æœ€ä½³å®è·µ

### 1. æ•°æ®é¢„å¤„ç†
```python
# æ‰¹é‡ç¼–ç 
texts = ["text 1", "text 2", "text 3"]
inputs = tokenizer(
    texts,
    padding=True,        # å¡«å……åˆ°ç›¸åŒé•¿åº¦
    truncation=True,     # æˆªæ–­è¶…é•¿åºåˆ—
    max_length=512,      # æœ€å¤§é•¿åº¦
    return_tensors="pt"  # è¿”å›PyTorchå¼ é‡
)
```

### 2. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
```python
# ä¿å­˜æ¨¡å‹
model.save_pretrained("./my-bert-model")
tokenizer.save_pretrained("./my-bert-model")

# åŠ è½½æ¨¡å‹
model = BertForSequenceClassification.from_pretrained("./my-bert-model")
tokenizer = BertTokenizer.from_pretrained("./my-bert-model")
```

### 3. æ¨ç†ä¼˜åŒ–
```python
# æ¨ç†æ¨¡å¼
model.eval()

# ç¦ç”¨æ¢¯åº¦è®¡ç®—
with torch.no_grad():
    outputs = model(**inputs)

# æ‰¹é‡æ¨ç†
def batch_inference(model, texts, batch_size=32):
    results = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
        results.append(outputs)
    return results
```

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„BERTæ¨¡å‹ï¼Ÿ
A: æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š
- **ç²¾åº¦ä¼˜å…ˆ**: `bert-large-uncased`
- **é€Ÿåº¦ä¼˜å…ˆ**: `distilbert-base-uncased`
- **ä¸­æ–‡ä»»åŠ¡**: `bert-base-chinese`
- **ç‰¹å®šä»»åŠ¡**: ä½¿ç”¨å·²ç»å¾®è°ƒå¥½çš„æ¨¡å‹

### Q: å¦‚ä½•å¤„ç†é•¿æ–‡æœ¬ï¼Ÿ
A: å‡ ç§æ–¹æ³•ï¼š
- æ»‘åŠ¨çª—å£ï¼šå°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºé‡å çš„ç‰‡æ®µ
- å±‚çº§æ–¹æ³•ï¼šå…ˆåˆ†æ®µç¼–ç å†èšåˆ
- Longformerï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶

### Q: å¦‚ä½•æé«˜å¾®è°ƒæ•ˆæœï¼Ÿ
A: æŠ€å·§åŒ…æ‹¬ï¼š
- åˆé€‚çš„å­¦ä¹ ç‡ï¼š2e-5åˆ°5e-5
- æ¸è¿›å¼è§£å†»ï¼šé€å±‚è§£å†»å‚æ•°
- æ•°æ®å¢å¼ºï¼šå›è¯‘ã€åŒä¹‰è¯æ›¿æ¢ç­‰
- æ—©åœæœºåˆ¶ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ–‡ä»¶
- `modeling_bert.py`: 758è¡Œï¼ŒåŒ…å«å®Œæ•´çš„BERTå®ç°
- `configuration_bert.py`: BertConfigé…ç½®ç±»
- `tokenization_bert.py`: WordPieceåˆ†è¯å™¨å®ç°
- `tokenization_bert_fast.py`: åŸºäºRustçš„å¿«é€Ÿåˆ†è¯å™¨

### è½¬æ¢è„šæœ¬
- `convert_bert_original_tf_checkpoint_to_pytorch.py`: TensorFlowåˆ°PyTorchè½¬æ¢
- `convert_bert_original_tf2_checkpoint_to_pytorch.py`: TensorFlow 2.xè½¬æ¢

### æµ‹è¯•æ–‡ä»¶
- `tests/test_modeling_bert.py`: BERTæ¨¡å‹æµ‹è¯•
- `tests/test_tokenization_bert.py`: åˆ†è¯å™¨æµ‹è¯•

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - è¯¦ç»†åˆ†æ
- âœ¨ å®ŒæˆBERTæ¨¡å‹æ ¸å¿ƒç»„ä»¶åˆ†æ
- ğŸ” è®°å½•æ‰€æœ‰ä»»åŠ¡ç‰¹å®šæ¨¡å‹ç±»
- ğŸ“Š åˆ†æé…ç½®å‚æ•°å’Œæœ€ä½³å®è·µ
- ğŸ¯ æä¾›å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹å’Œä¼˜åŒ–æŠ€å·§

### ä¸‹ä¸€æ­¥è®¡åˆ’
- [ ] åˆ†æBERTå˜ä½“æ¨¡å‹(RoBERTa, DistilBERTç­‰)
- [ ] åˆ›å»ºBERTå¾®è°ƒæœ€ä½³å®è·µæ–‡æ¡£
- [ ] è®°å½•BERTåœ¨å„ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½
- [ ] åˆ†æBERTçš„é¢„è®­ç»ƒç­–ç•¥

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 95%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 98%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20