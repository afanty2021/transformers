[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > [src](/Users/berton/Github/transformers/src/CLAUDE.md) > [transformers](/Users/berton/Github/transformers/src/transformers/CLAUDE.md) > **utils**

# Utils æ¨¡å—æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `src/transformers/utils/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 85%

## æ¨¡å—èŒè´£

Utilsæ¨¡å—æ˜¯Transformersçš„æ ¸å¿ƒåŸºç¡€è®¾æ–½æ¨¡å—ï¼Œæä¾›ï¼š

1. **é€šç”¨å·¥å…·å‡½æ•°**: è·¨æ¨¡å—å…±äº«çš„å®ç”¨å‡½æ•°
2. **é…ç½®ç®¡ç†**: æ¨¡å‹é…ç½®å’Œå‚æ•°ç®¡ç†
3. **å¯¼å…¥ç®¡ç†**: å»¶è¿ŸåŠ è½½å’Œä¾èµ–æ£€æŸ¥
4. **æ—¥å¿—ç³»ç»Ÿ**: ç»Ÿä¸€çš„æ—¥å¿—è®°å½•æ¥å£
5. **æ–‡ä»¶æ“ä½œ**: æ¨¡å‹ä¸‹è½½ã€ç¼“å­˜å’ŒHubé›†æˆ
6. **æ–‡æ¡£å·¥å…·**: è‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆå’Œä»£ç æ³¨é‡Š

## æ ¸å¿ƒç»„ä»¶

### 1. é€šç”¨å·¥å…· (`generic.py`)
```python
# æ ¸å¿ƒç±»å‹å’Œå·¥å…·ç±»
ModelOutput          # æ¨¡å‹è¾“å‡ºåŸºç±»
TensorType          # å¼ é‡ç±»å‹æšä¸¾
PaddingStrategy     # å¡«å……ç­–ç•¥
ExplicitEnum       # æ˜¾å¼æšä¸¾åŸºç±»
ContextManagers    # ä¸Šä¸‹æ–‡ç®¡ç†å™¨é›†åˆ

# å¼ é‡æ“ä½œå·¥å…·
is_torch_tensor()  # å¼ é‡ç±»å‹æ£€æŸ¥
to_numpy()         # å¼ é‡è½¬æ¢ä¸ºnumpy
flatten_dict()     # å­—å…¸å±•å¹³
```

### 2. å¯¼å…¥ç®¡ç† (`import_utils.py`)
```python
# æ ¸å¿ƒåŠŸèƒ½
OptionalDependencyNotAvailable  # å¯é€‰ä¾èµ–å¼‚å¸¸
_LazyModule                    # å»¶è¿ŸåŠ è½½æ¨¡å—
is_torch_available()          # PyTorchå¯ç”¨æ€§æ£€æŸ¥
is_tokenizers_available()     # Tokenizerså¯ç”¨æ€§æ£€æŸ¥
```

### 3. Hubé›†æˆ (`hub.py`)
```python
# æ ¸å¿ƒåŠŸèƒ½
cached_file()           # ç¼“å­˜æ–‡ä»¶ä¸‹è½½
download_url()          # URLä¸‹è½½
PushToHubMixin          # Hubæ¨é€æ··å…¥ç±»
default_cache_path()    # é»˜è®¤ç¼“å­˜è·¯å¾„
```

### 4. æ—¥å¿—ç³»ç»Ÿ (`logging.py`)
```python
# ç»Ÿä¸€æ—¥å¿—æ¥å£
logging.get_logger()    # è·å–æ—¥å¿—è®°å½•å™¨
logger.warning_advice() # è­¦å‘Šå’Œå»ºè®®
```

### 5. æ–‡æ¡£å·¥å…· (`doc.py`, `auto_docstring.py`)
```python
# æ–‡æ¡£ç”Ÿæˆå·¥å…·
add_start_docstrings()      # æ·»åŠ å¼€å§‹æ–‡æ¡£å­—ç¬¦ä¸²
add_end_docstrings()        # æ·»åŠ ç»“æŸæ–‡æ¡£å­—ç¬¦ä¸²
auto_class_docstring()      # è‡ªåŠ¨ç±»æ–‡æ¡£ç”Ÿæˆ
```

## å…³é”®æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | ä¸»è¦åŠŸèƒ½ | æ ¸å¿ƒç±»/å‡½æ•° |
|------|----------|-------------|
| `__init__.py` | æ¨¡å—å¯¼å‡º | æ‰€æœ‰å…¬å…±APIçš„å¯¼å‡ºå®šä¹‰ |
| `generic.py` | é€šç”¨å·¥å…· | ModelOutput, TensorTypeç­‰ |
| `import_utils.py` | å¯¼å…¥ç®¡ç† | _LazyModule, ä¾èµ–æ£€æŸ¥ |
| `hub.py` | Hubé›†æˆ | cached_file, PushToHubMixin |
| `logging.py` | æ—¥å¿—ç³»ç»Ÿ | get_logger, æ—¥å¿—é…ç½® |
| `constants.py` | å¸¸é‡å®šä¹‰ | IMAGENETå‡å€¼æ ‡å‡†å·®ç­‰ |
| `chat_template_utils.py` | èŠå¤©æ¨¡æ¿ | æ¨¡æ¿è§£æå’Œå¤„ç† |
| `quantization_config.py` | é‡åŒ–é…ç½® | å„ç§é‡åŒ–ç®—æ³•é…ç½®ç±» |
| `versions.py` | ç‰ˆæœ¬ç®¡ç† | ä¾èµ–ç‰ˆæœ¬æ£€æŸ¥ |

## é…ç½®å’Œå¸¸é‡

### å›¾åƒå¤„ç†å¸¸é‡
```python
IMAGENET_DEFAULT_MEAN = [0.485, 0.456, 0.406]
IMAGENET_DEFAULT_STD = [0.229, 0.224, 0.225]
IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]
IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]
```

### ç¼“å­˜è·¯å¾„å¸¸é‡
```python
TRANSFORMERS_CACHE = "~/.cache/huggingface/hub"
PYTORCH_TRANSFORMERS_CACHE = TRANSFORMERS_CACHE
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. æ£€æŸ¥ä¾èµ–å¯ç”¨æ€§
```python
from transformers.utils import is_torch_available, is_tokenizers_available

if is_torch_available():
    import torch
    print("PyTorch is available")

if is_tokenizers_available():
    from tokenizers import Tokenizer
    print("Fast tokenizers are available")
```

### 2. ä½¿ç”¨ModelOutput
```python
from transformers.utils import ModelOutput
from typing import Optional

class MyModelOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
```

### 3. å»¶è¿ŸåŠ è½½æ¨¡å—
```python
from transformers.utils import _LazyModule

# åˆ›å»ºå»¶è¿ŸåŠ è½½æ¨¡å—
lazy_module = _LazyModule(
    "module_name",
    __file__,
    {"Class1": ["module1", "Class1"], "function1": ["module2", "function1"]}
)
```

### 4. Hubæ–‡ä»¶æ“ä½œ
```python
from transformers.utils import cached_file

# ä¸‹è½½å¹¶ç¼“å­˜æ–‡ä»¶
file_path = cached_file(
    "bert-base-uncased",
    "pytorch_model.bin",
    cache_dir="./custom_cache"
)
```

## è®¾è®¡æ¨¡å¼

### 1. å»¶è¿ŸåŠ è½½æ¨¡å¼
- ä½¿ç”¨ `_LazyModule` å®ç°æŒ‰éœ€å¯¼å…¥
- å‡å°‘å¯åŠ¨æ—¶é—´å’Œå†…å­˜å ç”¨
- æ”¯æŒå¯é€‰ä¾èµ–çš„ä¼˜é›…é™çº§

### 2. Mixinæ¨¡å¼
- `PushToHubMixin`: æä¾›Hubæ¨é€åŠŸèƒ½
- `BackboneMixin`: éª¨å¹²ç½‘ç»œé€šç”¨åŠŸèƒ½

### 3. å·¥å‚æ¨¡å¼
- `ModelOutput`: åŠ¨æ€åˆ›å»ºè¾“å‡ºç±»
- é…ç½®ç±»ä½¿ç”¨å·¥å‚æ–¹æ³•åˆ›å»ºå®ä¾‹

## æ€§èƒ½ä¼˜åŒ–

1. **å»¶è¿ŸåŠ è½½**: é¿å…ä¸å¿…è¦çš„æ¨¡å—å¯¼å…¥
2. **ç¼“å­˜æœºåˆ¶**: Hubæ–‡ä»¶æœ¬åœ°ç¼“å­˜
3. **æ‰¹é‡æ“ä½œ**: æ”¯æŒæ‰¹é‡å¼ é‡æ“ä½œ
4. **å†…å­˜ä¼˜åŒ–**: åŠæ—¶é‡Šæ”¾å¤§å‹å¼ é‡

## æµ‹è¯•ç­–ç•¥

- **å•å…ƒæµ‹è¯•**: æ¯ä¸ªå·¥å…·å‡½æ•°çš„ç‹¬ç«‹æµ‹è¯•
- **é›†æˆæµ‹è¯•**: ä¸å…¶ä»–æ¨¡å—çš„äº¤äº’æµ‹è¯•
- **æ€§èƒ½æµ‹è¯•**: å»¶è¿ŸåŠ è½½å’Œç¼“å­˜æ€§èƒ½æµ‹è¯•

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•æ£€æŸ¥ç‰¹å®šä¾èµ–æ˜¯å¦å¯ç”¨ï¼Ÿ
A: ä½¿ç”¨ `is_*_available()` å‡½æ•°ç³»åˆ—ï¼š
```python
from transformers.utils import is_torch_available, is_vision_available

if is_torch_available() and is_vision_available():
    # ä½¿ç”¨PyTorchå’Œè§†è§‰åŠŸèƒ½
    pass
```

### Q: å¦‚ä½•è‡ªå®šä¹‰ç¼“å­˜ç›®å½•ï¼Ÿ
A: è®¾ç½®ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨cache_dirå‚æ•°ï¼š
```python
import os
os.environ["TRANSFORMERS_CACHE"] = "/path/to/cache"

# æˆ–è€…åœ¨å‡½æ•°ä¸­æŒ‡å®š
cached_file(model_id, filename, cache_dir="/path/to/cache")
```

### Q: å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ModelOutputï¼Ÿ
A: ç»§æ‰¿ModelOutputå¹¶å®šä¹‰å­—æ®µï¼š
```python
from transformers.utils import ModelOutput
from typing import Optional, Tuple

class CustomOutput(ModelOutput):
    logits: Optional[torch.FloatTensor] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
```

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒå·¥å…·æ–‡ä»¶
- `__init__.py` - æ¨¡å—å¯¼å‡ºå®šä¹‰
- `generic.py` - é€šç”¨å·¥å…·ç±»å’Œå‡½æ•°
- `constants.py` - é¡¹ç›®å¸¸é‡å®šä¹‰
- `backbone_utils.py` - éª¨å¹²ç½‘ç»œå·¥å…·

### å¯¼å…¥å’Œä¾èµ–æ–‡ä»¶
- `import_utils.py` - å¯¼å…¥ç®¡ç†å’Œå»¶è¿ŸåŠ è½½
- `versions.py` - ç‰ˆæœ¬æ£€æŸ¥å’Œå…¼å®¹æ€§

### Hubé›†æˆæ–‡ä»¶
- `hub.py` - Hugging Face Hubé›†æˆ
- `chat_template_utils.py` - èŠå¤©æ¨¡æ¿å¤„ç†

### æ–‡æ¡£å·¥å…·æ–‡ä»¶
- `doc.py` - æ–‡æ¡£å­—ç¬¦ä¸²å·¥å…·
- `auto_docstring.py` - è‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆ
- `notebook.py` - Jupyter notebookå·¥å…·

### ç‰¹æ®ŠåŠŸèƒ½æ–‡ä»¶
- `quantization_config.py` - é‡åŒ–é…ç½®
- `kernel_config.py` - å†…æ ¸é…ç½®
- `type_validators.py` - ç±»å‹éªŒè¯å™¨
- `attention_visualizer.py` - æ³¨æ„åŠ›å¯è§†åŒ–

### Dummyå¯¹è±¡æ–‡ä»¶ï¼ˆç”¨äºå¯é€‰ä¾èµ–ï¼‰
- `dummy_pt_objects.py` - PyTorch dummyå¯¹è±¡
- `dummy_vision_objects.py` - è§†è§‰åº“dummyå¯¹è±¡
- `dummy_tokenizers_objects.py` - Tokenizers dummyå¯¹è±¡

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - åˆå§‹åˆ†æ
- âœ¨ åˆ›å»ºutilsæ¨¡å—è¯¦ç»†æ–‡æ¡£
- ğŸ” åˆ†ææ ¸å¿ƒç»„ä»¶å’ŒåŠŸèƒ½
- ğŸ“Š è®°å½•å…³é”®APIå’Œä½¿ç”¨æ¨¡å¼
- ğŸ¯ è¯†åˆ«æ€§èƒ½ä¼˜åŒ–ç‚¹

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 85%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 98%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20