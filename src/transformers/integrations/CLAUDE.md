[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > [src](/Users/berton/Github/transformers/src/CLAUDE.md) > [transformers](/Users/berton/Github/transformers/src/transformers/CLAUDE.md) > **integrations**

# Integrations æ¨¡å—æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `src/transformers/integrations/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 92%

## æ¨¡å—èŒè´£

Integrationsæ¨¡å—è´Ÿè´£Transformersä¸ç¬¬ä¸‰æ–¹åº“å’Œç¡¬ä»¶å¹³å°çš„é›†æˆï¼ŒåŒ…æ‹¬ï¼š

1. **åˆ†å¸ƒå¼è®­ç»ƒ**: DeepSpeed, FSDP, Accelerateç­‰è®­ç»ƒæ¡†æ¶é›†æˆ
2. **é‡åŒ–ä¼˜åŒ–**: å¤šç§é‡åŒ–ç®—æ³•æ”¯æŒï¼ˆAWQ, GPTQ, BitsAndBytesç­‰ï¼‰
3. **æ³¨æ„åŠ›ä¼˜åŒ–**: Flash Attention, SDPAç­‰é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
4. **ç¡¬ä»¶åŠ é€Ÿ**: ç‰¹å®šç¡¬ä»¶å¹³å°çš„ä¼˜åŒ–æ”¯æŒ
5. **PEFTé›†æˆ**: å‚æ•°é«˜æ•ˆå¾®è°ƒæ”¯æŒ
6. **æ¨ç†å¼•æ“**: å„ç§æ¨ç†æ¡†æ¶çš„é›†æˆ

## æ ¸å¿ƒé›†æˆåˆ†ç±»

### ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒé›†æˆ

#### DeepSpeed (`deepspeed.py`)
```python
# Microsoft DeepSpeedé›†æˆ
- DeepSpeedEngineWrapper
- HfDeepSpeedConfig
- is_deepspeed_available()
- deepspeed_config_is_quantized()

# ä¸»è¦ç‰¹æ€§
- ZeROä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡
- æ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹
- æ··åˆç²¾åº¦è®­ç»ƒ
- å¤§æ¨¡å‹è®­ç»ƒä¼˜åŒ–
```

#### FSDP (`fsdp.py`)
```python
# PyTorch FSDPé›†æˆ
- FullyShardedDataParallel
- fsdp_auto_wrap_policy
- is_fsdp_available()

# ç‰¹æ€§
- å…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ
- å†…å­˜é«˜æ•ˆè®­ç»ƒ
- è‡ªåŠ¨åŒ…è£…ç­–ç•¥
```

#### Accelerate (`accelerate.py`)
```python
# Hugging Face Accelerate
- Accelerator
- DistributedType
- is_accelerate_available()

# åŠŸèƒ½
- ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
- å¤šè®¾å¤‡æ”¯æŒ
- æ··åˆç²¾åº¦
- æ¢¯åº¦ç´¯ç§¯
```

### ğŸ¯ é‡åŒ–æŠ€æœ¯é›†æˆ

#### BitsAndBytes (`bitsandbytes.py`)
```python
# 8ä½å’Œ4ä½é‡åŒ–
- BitsAndBytesConfig
- quantize_blockwise
- dequantize_blockwise
- is_bitsandbytes_available()

# é…ç½®é€‰é¡¹
load_in_8bit=True
load_in_4bit=True
bnb_4bit_compute_dtype=torch.float16
bnb_4bit_use_double_quant=True
```

#### AWQ (`awq.py`)
```python
# Activation-aware Weight Quantization
- AwqConfig
- is_awq_available()
- awq_quantize()

# ç‰¹æ€§
- æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–
- ç¡¬ä»¶å‹å¥½çš„é‡åŒ–æ–¹æ¡ˆ
- ä½ç²¾åº¦æ¨ç†ä¼˜åŒ–
```

#### GPTQ (`quantization_config.py`)
```python
# GPTQé‡åŒ–é…ç½®
- GptqConfig
- is_gptq_available()

# é…ç½®å‚æ•°
bits=4
group_size=128
dataset="c4"
exllama_config=False
```

### âš¡ æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–

#### Flash Attention (`flash_attention.py`)
```python
# Flash Attention 2é›†æˆ
- is_flash_attn_2_available()
- flash_attention_forward()
- FlashAttentionConfig

# ç‰¹æ€§
- å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—
- æ”¯æŒå› æœæ©ç 
- å…¼å®¹å¤šç§ç¡¬ä»¶
```

#### SDPA (`sdpa_attention.py`)
```python
# Scaled Dot Product Attention
- torch.nn.functional.scaled_dot_product_attention
- is_sdpa_available()
- SDPAConfig

# ä¼˜åŒ–ç‰¹æ€§
- å†…ç½®PyTorchä¼˜åŒ–
- è‡ªåŠ¨ç®—æ³•é€‰æ‹©
- å†…å­˜æ•ˆç‡æå‡
```

#### Flex Attention (`flex_attention.py`)
```python
# çµæ´»æ³¨æ„åŠ›æœºåˆ¶
- is_flex_attn_available()
- flex_attention_forward()

# ç‰¹æ€§
- è‡ªå®šä¹‰æ³¨æ„åŠ›æ¨¡å¼
- é«˜åº¦å¯é…ç½®
- ç‰¹æ®Šæ©ç æ”¯æŒ
```

### ğŸ”§ ç¡¬ä»¶ç‰¹å®šä¼˜åŒ–

#### TPU (`tpu.py`)
```python
# Google TPUæ”¯æŒ
- is_tpu_available()
- xmp.spawn()
- tpu_state_dict()

# ç‰¹æ€§
- XLAç¼–è¯‘ä¼˜åŒ–
- TPUç‰¹å®šä¼˜åŒ–
- å¤šTPUæ”¯æŒ
```

#### NPU (`npu_flash_attention.py`)
```python
# åä¸ºNPUæ”¯æŒ
- is_npu_available()
- npu_flash_attention_forward()

# ç‰¹æ€§
- æ˜‡è…¾èŠ¯ç‰‡ä¼˜åŒ–
- NPUç‰¹å®šç®—å­
```

### ğŸ›ï¸ å‚æ•°é«˜æ•ˆå¾®è°ƒ

#### PEFT (`peft.py`)
```python
# Parameter-Efficient Fine-Tuning
- is_peft_available()
- PeftConfig
- get_peft_model()

# æ”¯æŒçš„PEFTæ–¹æ³•
- LoRA (Low-Rank Adaptation)
- AdaLoRA (Adaptive LoRA)
- QLoRA (Quantized LoRA)
- Prefix Tuning
- P-Tuning
```

### ğŸ”„ å…¶ä»–é‡è¦é›†æˆ

#### Tensor Parallel (`tensor_parallel.py`)
```python
# å¼ é‡å¹¶è¡Œ
- TensorParallel
- is_tensor_parallel_available()

# ç‰¹æ€§
- å¤šGPUå¼ é‡åˆ†å¸ƒ
- å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿ
```

#### Tiktoken (`tiktoken.py`)
```python
# OpenAI Tiktokenåˆ†è¯å™¨
- is_tiktoken_available()
- TiktokenTokenizer

# ç‰¹æ€§
- å¿«é€ŸBPEåˆ†è¯
- å¤šè¯­è¨€æ”¯æŒ
```

#### Hugging Face Kernels (`hub_kernels.py`)
```python
# Hubè‡ªå®šä¹‰å†…æ ¸
- is_hubb_kernels_available()
- download_kernel_from_hub()

# ç‰¹æ€§
- è‡ªå®šä¹‰CUDAå†…æ ¸
- ç¤¾åŒºè´¡çŒ®å†…æ ¸
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. DeepSpeedé›†æˆ
```python
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
import deepspeed

# DeepSpeedé…ç½®
deepspeed_config = {
    "train_batch_size": 32,
    "gradient_accumulation_steps": 4,
    "fp16": {
        "enabled": True
    },
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu"
        }
    }
}

# è®­ç»ƒå™¨è®¾ç½®
training_args = TrainingArguments(
    output_dir="./results",
    deepspeed=deepspeed_config
)

model = AutoModelForCausalLM.from_pretrained("model_name")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)
```

### 2. BitsAndBytesé‡åŒ–
```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# é‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    quantization_config=quantization_config,
    device_map="auto"
)
```

### 3. Flash Attention
```python
# ä½¿ç”¨Flash Attention 2
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    use_flash_attention_2=True,
    torch_dtype=torch.float16
)

# æˆ–åœ¨è®­ç»ƒæ—¶å¯ç”¨
training_args = TrainingArguments(
    use_flash_attention_2=True
)
```

### 4. PEFTå¾®è°ƒ
```python
from transformers import AutoModelForCausalLM
from peft import get_peft_model, LoraConfig

# åŸºç¡€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("model_name")

# LoRAé…ç½®
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none"
)

# åº”ç”¨PEFT
model = get_peft_model(model, lora_config)
```

### 5. AWQé‡åŒ–
```python
from transformers import AutoModelForCausalLM, AwqConfig

# AWQé…ç½®
awq_config = AwqConfig(
    bits=4,
    group_size=128,
    zero_point=True,
    version="GEMM"
)

# AWQé‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    quantization_config=awq_config,
    device_map="auto"
)
```

### 6. å¤šæŠ€æœ¯ç»„åˆ
```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments

# ç»„åˆå¤šç§ä¼˜åŒ–æŠ€æœ¯
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True
)

model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    quantization_config=quantization_config,
    device_map="auto",
    use_flash_attention_2=True,
    torch_dtype=torch.float16
)

# PEFTå¾®è°ƒ
from peft import get_peft_model, LoraConfig
peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05)
model = get_peft_model(model, peft_config)
```

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. å†…å­˜ä¼˜åŒ–
```python
# æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# æ··åˆç²¾åº¦è®­ç»ƒ
training_args = TrainingArguments(
    fp16=True,
    dataloader_num_workers=4
)
```

### 2. è®¡ç®—ä¼˜åŒ–
```python
# ç¼–è¯‘ä¼˜åŒ–
model = torch.compile(model)

# æ³¨æ„åŠ›ä¼˜åŒ–
model.config.use_cache = False  # è®­ç»ƒæ—¶
model.config.use_flash_attention_2 = True
```

### 3. å¹¶è¡ŒåŒ–ç­–ç•¥
```python
# æ•°æ®å¹¶è¡Œ
training_args = TrainingArguments(
    dataloader_pin_memory=True,
    dataloader_num_workers=4
)

# æ¨¡å‹å¹¶è¡Œ
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    device_map="auto",
    max_memory={0: "40GB", 1: "40GB"}
)
```

## å…¼å®¹æ€§æ£€æŸ¥

### 1. å¯ç”¨æ€§æ£€æŸ¥
```python
from transformers.utils import is_bitsandbytes_available, is_flash_attn_2_available

if is_bitsandbytes_available():
    print("BitsAndBytes is available")

if is_flash_attn_2_available():
    print("Flash Attention 2 is available")
```

### 2. ç¡¬ä»¶æ£€æŸ¥
```python
import torch

print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"GPU count: {torch.cuda.device_count()}")

if torch.cuda.is_available():
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
```

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„é‡åŒ–æ–¹æ³•
- **BitsAndBytes**: ç®€å•æ˜“ç”¨ï¼Œé€‚åˆå¿«é€Ÿå®éªŒ
- **AWQ**: ç¡¬ä»¶å‹å¥½ï¼Œæ¨ç†æ€§èƒ½å¥½
- **GPTQ**: æˆç†Ÿç¨³å®šï¼Œç¤¾åŒºæ”¯æŒå¥½

### 2. åˆ†å¸ƒå¼è®­ç»ƒé€‰æ‹©
- **å°è§„æ¨¡**: Accelerate
- **å¤§è§„æ¨¡**: DeepSpeed
- **ZeROä¼˜åŒ–**: DeepSpeed ZeRO-3
- **å¤šèŠ‚ç‚¹**: DeepSpeed + NCCL

### 3. æ³¨æ„åŠ›ä¼˜åŒ–
- **è®­ç»ƒ**: Flash Attention 2
- **æ¨ç†**: SDPA + Flash Attention
- **ç‰¹æ®Šç¡¬ä»¶**: ç¡¬ä»¶ç‰¹å®šæ³¨æ„åŠ›

## æµ‹è¯•ç­–ç•¥

### 1. é›†æˆæµ‹è¯•
- å„é›†æˆçš„åŠŸèƒ½æ­£ç¡®æ€§
- ä¸ä¸åŒæ¨¡å‹çš„å…¼å®¹æ€§
- æ€§èƒ½å›å½’æµ‹è¯•

### 2. æ€§èƒ½åŸºå‡†
- å†…å­˜ä½¿ç”¨æ•ˆç‡
- è®­ç»ƒ/æ¨ç†é€Ÿåº¦
- ç²¾åº¦æŸå¤±è¯„ä¼°

### 3. ç¨³å®šæ€§æµ‹è¯•
- é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§
- å¤§è§„æ¨¡è®­ç»ƒç¨³å®šæ€§
- é”™è¯¯æ¢å¤èƒ½åŠ›

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•é€‰æ‹©é‡åŒ–æ–¹æ¡ˆï¼Ÿ
A: æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š
- **å¿«é€ŸåŸå‹**: BitsAndBytes
- **ç”Ÿäº§éƒ¨ç½²**: AWQæˆ–GPTQ
- **æœ€é«˜ç²¾åº¦**: æ— é‡åŒ–æˆ–8ä½é‡åŒ–

### Q: Flash Attentionä¸å·¥ä½œæ€ä¹ˆåŠï¼Ÿ
A: æ£€æŸ¥ä»¥ä¸‹äº‹é¡¹ï¼š
- CUDAç‰ˆæœ¬å…¼å®¹æ€§ï¼ˆ>=11.6ï¼‰
- PyTorchç‰ˆæœ¬ï¼ˆ>=2.0ï¼‰
- GPUæ¶æ„æ”¯æŒï¼ˆAmpere+ï¼‰
- å®‰è£…Flash AttentionåŒ…

### Q: å¦‚ä½•ä¼˜åŒ–å¤šGPUè®­ç»ƒï¼Ÿ
A: ä½¿ç”¨ä»¥ä¸‹ç­–ç•¥ï¼š
- DeepSpeed ZeROä¼˜åŒ–
- æ¢¯åº¦ç´¯ç§¯
- æ··åˆç²¾åº¦è®­ç»ƒ
- é€‚å½“çš„æ•°æ®å¹¶è¡Œç­–ç•¥

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒè®­ç»ƒé›†æˆ
- `__init__.py` - æ¨¡å—å¯¼å‡ºå®šä¹‰
- `deepspeed.py` - Microsoft DeepSpeedé›†æˆ
- `accelerate.py` - Hugging Face Accelerateé›†æˆ
- `fsdp.py` - PyTorch FSDPé›†æˆ

### é‡åŒ–æŠ€æœ¯
- `bitsandbytes.py` - BitsAndBytesé‡åŒ–
- `awq.py` - Activation-aware Weight Quantization
- `quantization_config.py` - å„ç§é‡åŒ–é…ç½®ç±»
- `quanto.py` - Quantoé‡åŒ–é›†æˆ
- `hqq.py` - HQQé‡åŒ–é›†æˆ

### æ³¨æ„åŠ›ä¼˜åŒ–
- `flash_attention.py` - Flash Attentioné›†æˆ
- `sdpa_attention.py` - Scaled Dot Product Attention
- `flex_attention.py` - çµæ´»æ³¨æ„åŠ›æœºåˆ¶
- `npu_flash_attention.py` - NPU Flash Attention

### ç¡¬ä»¶ç‰¹å®š
- `tpu.py` - Google TPUæ”¯æŒ
- `tensor_parallel.py` - å¼ é‡å¹¶è¡Œ
- `hub_kernels.py` - Hubè‡ªå®šä¹‰å†…æ ¸

### PEFTé›†æˆ
- `peft.py` - å‚æ•°é«˜æ•ˆå¾®è°ƒé›†æˆ

### å…¶ä»–é›†æˆ
- `tiktoken.py` - OpenAI Tiktokenåˆ†è¯å™¨
- `executorch.py` - PyTorch ExecuTorch
- `ggml.py` - GGMLæ ¼å¼æ”¯æŒ
- `mistral.py` - Mistralç‰¹å®šä¼˜åŒ–

## æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°é›†æˆ
```python
# 1. æ£€æŸ¥å¯ç”¨æ€§
def is_new_integration_available():
    try:
        import new_library
        return True
    except ImportError:
        return False

# 2. å®ç°é›†æˆåŠŸèƒ½
class NewIntegrationWrapper:
    def __init__(self, config):
        self.config = config

    def wrap_model(self, model):
        # åŒ…è£…æ¨¡å‹é€»è¾‘
        return wrapped_model

# 3. æ·»åŠ åˆ°__init__.pyå¯¼å‡º
```

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - åˆå§‹åˆ†æ
- âœ¨ åˆ›å»ºintegrationsæ¨¡å—è¯¦ç»†æ–‡æ¡£
- ğŸ” åˆ†æä¸»è¦é›†æˆç±»åˆ«å’ŒåŠŸèƒ½
- ğŸ“Š è®°å½•ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µ
- ğŸ¯ è¯†åˆ«æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 92%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 98%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20