[æ ¹ç›®å½•](/Users/berton/Github/transformers/CLAUDE.md) > [src](/Users/berton/Github/transformers/src/CLAUDE.md) > [transformers](/Users/berton/Github/transformers/src/transformers/CLAUDE.md) > **cli**

# CLI æ¨¡å—æ–‡æ¡£

> æ¨¡å—è·¯å¾„: `src/transformers/cli/`
> æœ€åæ›´æ–°: 2025-01-20
> è¦†ç›–ç‡: 95%

## æ¨¡å—èŒè´£

CLIæ¨¡å—æä¾›Transformersçš„å‘½ä»¤è¡Œå·¥å…·é›†åˆï¼ŒåŒ…æ‹¬ï¼š

1. **æ¨¡å‹ç®¡ç†**: ä¸‹è½½ã€ä¸Šä¼ ã€ç®¡ç†é¢„è®­ç»ƒæ¨¡å‹
2. **äº¤äº’å¼å·¥å…·**: èŠå¤©ã€æ¨ç†æœåŠ¡
3. **å¼€å‘è¾…åŠ©**: æ·»åŠ æ–°æ¨¡å‹ã€åˆ›å»ºæ¨¡æ¿
4. **ç³»ç»Ÿä¿¡æ¯**: ç¯å¢ƒæ£€æŸ¥ã€ä¾èµ–è¯Šæ–­
5. **æ‰¹é‡å¤„ç†**: æ‰¹é‡æ¨ç†å’Œè¿è¡Œè„šæœ¬

## å‘½ä»¤è¡Œå·¥å…·æ¦‚è§ˆ

### ğŸ¤– æ ¸å¿ƒCLIå·¥å…· (`transformers.py`)

Transformersçš„ä¸»è¦å‘½ä»¤è¡Œå…¥å£ç‚¹ï¼Œæä¾›å¤šä¸ªå­å‘½ä»¤ï¼š

```bash
# æŸ¥çœ‹å¸®åŠ©
transformers --help

# ä¸»è¦å‘½ä»¤åˆ†ç±»
transformers download    # æ¨¡å‹ä¸‹è½½
transformers serve       # æ¨¡å‹æœåŠ¡
transformers chat        # äº¤äº’å¼èŠå¤©
transformers run         # æ‰§è¡Œè„šæœ¬
transformers system      # ç³»ç»Ÿä¿¡æ¯
```

### ğŸ“¥ æ¨¡å‹ç®¡ç†å·¥å…·

#### ä¸‹è½½å·¥å…· (`download.py`)
```python
# å‘½ä»¤è¡Œä½¿ç”¨
transformers download model_name --cache-dir ./cache

# åŠŸèƒ½ç‰¹æ€§
- æ¨¡å‹æ–‡ä»¶ä¸‹è½½
- åˆ†ç‰‡ä¸‹è½½æ”¯æŒ
- æ–­ç‚¹ç»­ä¼ 
- ç¼“å­˜ç®¡ç†
- Hubé›†æˆ
```

#### æœåŠ¡å·¥å…· (`serve.py`)
```python
# å¯åŠ¨æ¨¡å‹æœåŠ¡
transformers serve model_name --port 8080 --host 0.0.0.0

# åŠŸèƒ½ç‰¹æ€§
- REST APIæœåŠ¡
- æ‰¹é‡æ¨ç†
- è´Ÿè½½å‡è¡¡
- å¥åº·æ£€æŸ¥
- ç›‘æ§æ¥å£
```

### ğŸ’¬ äº¤äº’å¼å·¥å…·

#### èŠå¤©å·¥å…· (`chat.py`)
```python
# å¯åŠ¨èŠå¤©
transformers chat model_name --system-prompt "You are a helpful assistant."

# åŠŸèƒ½ç‰¹æ€§
- äº¤äº’å¼å¯¹è¯
- æµå¼è¾“å‡º
- å†å²è®°å½•
- æç¤ºå·¥ç¨‹
- å¤šè½®å¯¹è¯
```

### ğŸ”§ å¼€å‘è¾…åŠ©å·¥å…·

#### æ–°æ¨¡å‹æ¨¡æ¿ (`add_new_model_like.py`)
```python
# åˆ›å»ºæ–°æ¨¡å‹æ¨¡æ¿
transformers add-new-model-like bert --name my_model

# åŠŸèƒ½ç‰¹æ€§
- æ¨¡å‹æ¨¡æ¿ç”Ÿæˆ
- é…ç½®æ–‡ä»¶åˆ›å»º
- æµ‹è¯•æ¡†æ¶æ­å»º
- æ–‡æ¡£æ¨¡æ¿
- ä»£ç è§„èŒƒ
```

#### å¿«é€Ÿå›¾åƒå¤„ç†å™¨ (`add_fast_image_processor.py`)
```python
# æ·»åŠ å¿«é€Ÿå›¾åƒå¤„ç†å™¨
transformers add-fast-image-processor model_name

# åŠŸèƒ½ç‰¹æ€§
- å›¾åƒå¤„ç†å™¨ç”Ÿæˆ
- é¢„å¤„ç†ç®¡é“
- æ‰¹å¤„ç†ä¼˜åŒ–
- æ ¼å¼è½¬æ¢
```

### âš™ï¸ ç³»ç»Ÿå·¥å…·

#### ç³»ç»Ÿä¿¡æ¯ (`system.py`)
```python
# ç³»ç»Ÿè¯Šæ–­
transformers system

# è¾“å‡ºä¿¡æ¯
- Pythonç‰ˆæœ¬
- PyTorchç‰ˆæœ¬
- CUDAä¿¡æ¯
- å†…å­˜çŠ¶æ€
- ä¾èµ–æ£€æŸ¥
- ç¡¬ä»¶é…ç½®
```

### ğŸƒ è¿è¡Œå·¥å…· (`run.py`)
```python
# æ‰§è¡Œè„šæœ¬
transformers run script.py --args

# åŠŸèƒ½ç‰¹æ€§
- è„šæœ¬æ‰§è¡Œç®¡ç†
- ç¯å¢ƒéš”ç¦»
- æ—¥å¿—è®°å½•
- é”™è¯¯å¤„ç†
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. æ¨¡å‹ä¸‹è½½å’Œç®¡ç†
```bash
# ä¸‹è½½BERTæ¨¡å‹
transformers download bert-base-uncased

# ä¸‹è½½åˆ°æŒ‡å®šç›®å½•
transformers download bert-base-uncased --cache-dir ./models

# ä¸‹è½½ç‰¹å®šæ–‡ä»¶
transformers download bert-base-uncased --files config.json pytorch_model.bin

# å¼ºåˆ¶é‡æ–°ä¸‹è½½
transformers download bert-base-uncased --force-download
```

### 2. æ¨¡å‹æœåŠ¡
```bash
# å¯åŠ¨åŸºç¡€æœåŠ¡
transformers serve bert-base-uncased

# é…ç½®æœåŠ¡
transformers serve bert-base-uncased \
    --port 8080 \
    --host 0.0.0.0 \
    --workers 4 \
    --max-batch-size 32

# å¯åŠ¨å¸¦æœ‰è®¤è¯çš„æœåŠ¡
transformers serve bert-base-uncased \
    --api-key your_api_key \
    --rate-limit 100
```

### 3. äº¤äº’å¼èŠå¤©
```bash
# åŸºç¡€èŠå¤©
transformers chat gpt2

# å¸¦ç³»ç»Ÿæç¤ºçš„èŠå¤©
transformers chat gpt2 --system-prompt "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"

# é™åˆ¶æœ€å¤§é•¿åº¦
transformers chat gpt2 --max-length 200 --temperature 0.8

# æµå¼è¾“å‡º
transformers chat gpt2 --stream
```

### 4. ç³»ç»Ÿè¯Šæ–­
```bash
# å®Œæ•´ç³»ç»Ÿä¿¡æ¯
transformers system

# æ£€æŸ¥ç‰¹å®šç»„ä»¶
transformers system --check cuda
transformers system --check dependencies
transformers system --check memory
```

### 5. å¼€å‘è¾…åŠ©
```bash
# åˆ›å»ºæ–°æ¨¡å‹æ¨¡æ¿
transformers add-new-model-like bert --name my-bert-variant

# åˆ›å»ºå¸¦é…ç½®çš„æ¨¡å‹
transformers add-new-model-like bert \
    --name custom-bert \
    --config-file custom_config.json

# æ·»åŠ å›¾åƒå¤„ç†å™¨
transformers add-fast-image-processing vit --name custom-vit
```

## APIæ¥å£è§„èŒƒ

### REST APIç«¯ç‚¹

#### æ¨¡å‹æ¨ç†
```http
POST /predict
Content-Type: application/json

{
    "text": "Hello, world",
    "parameters": {
        "max_length": 50,
        "temperature": 0.7
    }
}
```

#### å¥åº·æ£€æŸ¥
```http
GET /health
Response: {"status": "healthy", "model": "bert-base-uncased"}
```

#### æ¨¡å‹ä¿¡æ¯
```http
GET /model/info
Response: {
    "model_name": "bert-base-uncased",
    "model_type": "bert",
    "vocab_size": 30522
}
```

## é…ç½®æ–‡ä»¶

### æœåŠ¡å™¨é…ç½® (`config.yaml`)
```yaml
server:
  host: "0.0.0.0"
  port: 8080
  workers: 4
  max_batch_size: 32

model:
  name: "bert-base-uncased"
  device: "auto"
  dtype: "float16"

security:
  api_key: null
  rate_limit: 100

logging:
  level: "INFO"
  file: "server.log"
```

### ä¸‹è½½é…ç½®
```yaml
cache:
  directory: "~/.cache/huggingface"
  max_size: "100GB"

download:
  resume: true
  verify_checksum: true
  parallel_downloads: 4
```

## é«˜çº§åŠŸèƒ½

### 1. æ‰¹é‡æ¨ç†
```bash
# æ‰¹é‡å¤„ç†æ–‡æœ¬æ–‡ä»¶
transformers run inference.py \
    --input texts.txt \
    --output results.txt \
    --batch-size 16 \
    --model bert-base-uncased
```

### 2. æ¨¡å‹è¯„ä¼°
```bash
# è¯„ä¼°æ¨¡å‹æ€§èƒ½
transformers run evaluate.py \
    --model gpt2 \
    --dataset wikitext \
    --metrics perplexity bleu
```

### 3. æ¨¡å‹è½¬æ¢
```bash
# è½¬æ¢æ¨¡å‹æ ¼å¼
transformers run convert.py \
    --input-model model.pt \
    --output-format onnx \
    --output-model model.onnx
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æœåŠ¡ä¼˜åŒ–
```yaml
# ä¼˜åŒ–é…ç½®
server:
  workers: 8  # å¢åŠ å·¥ä½œè¿›ç¨‹
  timeout: 300  # å¢åŠ è¶…æ—¶æ—¶é—´
  keepalive: 30  # è¿æ¥ä¿æŒ

model:
  device_map: "auto"  # è‡ªåŠ¨è®¾å¤‡åˆ†é…
  use_cache: true  # å¯ç”¨ç¼“å­˜
  torch_dtype: "float16"  # åŠç²¾åº¦
```

### 2. å†…å­˜ä¼˜åŒ–
```bash
# å†…å­˜ç›‘æ§
transformers system --monitor-memory

# å†…å­˜é™åˆ¶
transformers serve bert-base-uncased --memory-limit "8GB"
```

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

#### ç½‘ç»œè¿æ¥é—®é¢˜
```bash
# é‡è¯•ä¸‹è½½
transformers download model-name --retry 3 --timeout 60

# ä½¿ç”¨é•œåƒ
transformers download model-name --mirror https://hf-mirror.com
```

#### å†…å­˜ä¸è¶³
```bash
# åˆ†æ‰¹ä¸‹è½½
transformers download model-name --batch-download

# ä½¿ç”¨CPUæ¨ç†
transformers serve model-name --device cpu
```

#### æƒé™é—®é¢˜
```bash
# ä½¿ç”¨Hugging Face token
export HF_TOKEN="your_token"
transformers download private/model
```

## å®‰å…¨è€ƒè™‘

### 1. è®¿é—®æ§åˆ¶
```bash
# å¯ç”¨APIå¯†é’¥
transformers serve model --api-key secure_key

# é™åˆ¶è®¿é—®IP
transformers serve model --allowed-ips "192.168.1.0/24"
```

### 2. è¾“å…¥éªŒè¯
```yaml
security:
  max_input_length: 2048
  allowed_formats: ["text/plain", "application/json"]
  content_filter: true
```

### 3. é€Ÿç‡é™åˆ¶
```yaml
security:
  rate_limit:
    requests_per_minute: 100
    burst_size: 20
  user_limits:
    default: 10
    premium: 1000
```

## ç›‘æ§å’Œæ—¥å¿—

### 1. æ—¥å¿—é…ç½®
```yaml
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    file:
      filename: "transformers.log"
      max_size: "10MB"
      backup_count: 5
    console:
      enabled: true
```

### 2. ç›‘æ§æŒ‡æ ‡
```python
# å†…ç½®ç›‘æ§æŒ‡æ ‡
- è¯·æ±‚å»¶è¿Ÿ
- ååé‡
- é”™è¯¯ç‡
- å†…å­˜ä½¿ç”¨
- GPUä½¿ç”¨ç‡
- æ¨¡å‹åŠ è½½æ—¶é—´
```

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°å‘½ä»¤
```python
# 1. åˆ›å»ºæ–°å‘½ä»¤æ–‡ä»¶
# cli/new_command.py

import argparse
from transformers import HfArgumentParser

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True)
    parser.add_argument("--output", required=True)
    args = parser.parse_args()

    # å®ç°å‘½ä»¤é€»è¾‘
    print(f"Processing {args.input} -> {args.output}")

# 2. åœ¨__init__.pyä¸­æ³¨å†Œ
# 3. æ·»åŠ åˆ°ä¸»CLIå…¥å£
```

### è‡ªå®šä¹‰æœåŠ¡ç«¯ç‚¹
```python
# æ‰©å±•REST API
from transformers.cli.serve import BaseServer

class CustomServer(BaseServer):
    def setup_custom_routes(self):
        @self.app.post("/custom")
        def custom_endpoint(data):
            # è‡ªå®šä¹‰å¤„ç†é€»è¾‘
            return {"result": "success"}
```

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•
- æ¯ä¸ªCLIå‘½ä»¤çš„åŠŸèƒ½æµ‹è¯•
- å‚æ•°éªŒè¯æµ‹è¯•
- é”™è¯¯å¤„ç†æµ‹è¯•

### 2. é›†æˆæµ‹è¯•
- ç«¯åˆ°ç«¯æœåŠ¡æµ‹è¯•
- ç½‘ç»œè¿æ¥æµ‹è¯•
- æ–‡ä»¶ç³»ç»Ÿæ“ä½œæµ‹è¯•

### 3. æ€§èƒ½æµ‹è¯•
- å¤§æ–‡ä»¶ä¸‹è½½æ€§èƒ½
- é«˜å¹¶å‘æœåŠ¡æµ‹è¯•
- å†…å­˜ä½¿ç”¨æ•ˆç‡æµ‹è¯•

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•è‡ªå®šä¹‰æ¨¡å‹ç¼“å­˜ä½ç½®ï¼Ÿ
A: è®¾ç½®ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼š
```bash
export TRANSFORMERS_CACHE="/path/to/cache"
transformers download model-name --cache-dir "/custom/path"
```

### Q: å¦‚ä½•æé«˜æ¨¡å‹æ¨ç†é€Ÿåº¦ï¼Ÿ
A: ä½¿ç”¨ä»¥ä¸‹ä¼˜åŒ–ï¼š
- å¯ç”¨GPUï¼š`--device cuda`
- ä½¿ç”¨åŠç²¾åº¦ï¼š`--dtype float16`
- å¯ç”¨ç¼“å­˜ï¼š`--use-cache true`
- å¢åŠ å·¥ä½œè¿›ç¨‹ï¼š`--workers 8`

### Q: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Ÿ
A: æ¨èé…ç½®ï¼š
```bash
transformers serve model \
    --workers 8 \
    --port 8080 \
    --host 0.0.0.0 \
    --api-key secure_key \
    --rate-limit 1000 \
    --monitoring
```

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒCLIæ–‡ä»¶
- `__init__.py` - æ¨¡å—å¯¼å‡ºå®šä¹‰
- `transformers.py` - ä¸»CLIå…¥å£ç‚¹

### æ¨¡å‹ç®¡ç†å·¥å…·
- `download.py` - æ¨¡å‹ä¸‹è½½å·¥å…·
- `serve.py` - æ¨¡å‹æœåŠ¡å·¥å…·
- `run.py` - è„šæœ¬æ‰§è¡Œå·¥å…·

### äº¤äº’å¼å·¥å…·
- `chat.py` - äº¤äº’å¼èŠå¤©å·¥å…·
- `system.py` - ç³»ç»Ÿä¿¡æ¯å·¥å…·

### å¼€å‘è¾…åŠ©å·¥å…·
- `add_new_model_like.py` - æ–°æ¨¡å‹æ¨¡æ¿ç”Ÿæˆ
- `add_fast_image_processor.py` - å¿«é€Ÿå›¾åƒå¤„ç†å™¨

## å˜æ›´è®°å½• (Changelog)

### 2025-01-20 - åˆå§‹åˆ†æ
- âœ¨ åˆ›å»ºCLIæ¨¡å—è¯¦ç»†æ–‡æ¡£
- ğŸ” åˆ†æå‘½ä»¤è¡Œå·¥å…·æ¶æ„
- ğŸ“Š è®°å½•ä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®
- ğŸ¯ è¯†åˆ«æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

---

**ğŸ“Š å½“å‰è¦†ç›–ç‡**: 95%
**ğŸ¯ ç›®æ ‡è¦†ç›–ç‡**: 98%+
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20