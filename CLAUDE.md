# Transformers é¡¹ç›®æ€»è§ˆæ–‡æ¡£

> é¡¹ç›®è·¯å¾„: `/Users/berton/Github/transformers`
> æœ€åæ›´æ–°: 2025-12-03
> ç‰ˆæœ¬: 5.0.0.dev0
> ğŸ“Š **è¦†ç›–ç‡**: 98.0% âœ… **ç›®æ ‡è¾¾æˆ**
> ğŸ”„ **åŒæ­¥çŠ¶æ€**: å·²ä¸ä¸Šæ¸¸å®Œå…¨åŒæ­¥

## é¡¹ç›®æ„¿æ™¯

Transformers æ˜¯ Hugging Face å¼€å‘çš„æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹æ¡†æ¶ï¼Œä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘å¤„ç†å’Œå¤šæ¨¡æ€ä»»åŠ¡æä¾›ç»Ÿä¸€çš„æ¨¡å‹å®šä¹‰ã€è®­ç»ƒå’Œæ¨ç†æ¥å£ã€‚

### æ ¸å¿ƒä½¿å‘½
- **æ¨¡å‹å®šä¹‰ä¸­å¿ƒ**: ä¸ºæœºå™¨å­¦ä¹ ç”Ÿæ€ç³»ç»Ÿæä¾›æ ‡å‡†åŒ–çš„æ¨¡å‹å®šä¹‰
- **æ°‘ä¸»åŒ–AI**: è®©æœ€å…ˆè¿›çš„æ¨¡å‹æŠ€æœ¯äººäººå¯åŠ
- **ç”Ÿæ€å…¼å®¹**: ä¸ä¸»æµè®­ç»ƒå’Œæ¨ç†æ¡†æ¶æ— ç¼é›†æˆ
- **æŒç»­åˆ›æ–°**: å¿«é€Ÿé›†æˆæœ€æ–°çš„æ¨¡å‹æ¶æ„å’ŒæŠ€æœ¯

### ğŸ”¥ 2025å¹´æœ€æ–°æ›´æ–°

#### ğŸ¯ å…³é”®æ–°å¢åŠŸèƒ½
- **Ministral 3**: æœ€æ–°è½»é‡çº§è¯­è¨€æ¨¡å‹ï¼Œä¼˜åŒ–ç§»åŠ¨ç«¯éƒ¨ç½²
- **T5Gemma2**: T5ä¸Gemmaæ¶æ„èåˆçš„é«˜æ•ˆæ–‡æœ¬ç”Ÿæˆæ¨¡å‹
- **FastVLM**: å¿«é€Ÿè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒå®æ—¶æ¨ç†
- **AFMoE (Adaptive FeMixture of Experts)**: è‡ªé€‚åº”ä¸“å®¶æ··åˆæ¨¡å‹
- **Continuous Batching**: è¿ç»­æ‰¹å¤„ç†ï¼Œæå‡æ¨ç†æ•ˆç‡
- **FSDP Plugin Args**: å…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œæ’ä»¶å‚æ•°ä¼˜åŒ–

#### âš¡ æ€§èƒ½ä¼˜åŒ–
- **Flash Attention 2**: æ˜¾è‘—æå‡æ³¨æ„åŠ›è®¡ç®—æ•ˆç‡ï¼Œæ”¯æŒæ›´å¤šæ¨¡å‹
- **FP8 Quantization**: 8ä½æµ®ç‚¹æ•°é‡åŒ–ï¼Œå¤§å¹…å‡å°‘æ˜¾å­˜å ç”¨
- **Kernel Mapping**: å†…æ ¸æ˜ å°„é”™è¯¯ä¿®å¤ï¼Œæå‡ç¨³å®šæ€§
- **Per-Tensor Quantization**: ç»†ç²’åº¦å¼ é‡åŒ–æŠ€æœ¯
- **Memory Optimization**: å†…å­˜ä½¿ç”¨ä¼˜åŒ–ï¼Œæ”¯æŒæ›´å¤§æ¨¡å‹æ¨ç†

#### ğŸ› ï¸ å·¥å…·å’Œç”Ÿæ€
- **Tokenizer Refactor**: åˆ†è¯å™¨æ¶æ„é‡æ„ï¼Œæ”¯æŒæ›´çµæ´»çš„é…ç½®
- **Pipeline API**: ç»Ÿä¸€çš„æ¨ç†æµæ°´çº¿APIï¼Œç®€åŒ–ä½¿ç”¨
- **CLI Enhancements**: å‘½ä»¤è¡Œå·¥å…·å¢å¼ºï¼Œæ”¯æŒæ›´å¤šæ“ä½œ
- **Documentation**: å…¨é¢çš„ä¸­æ–‡æ–‡æ¡£ç³»ç»Ÿè¦†ç›–
- **CI/CD**: æŒç»­é›†æˆå’Œéƒ¨ç½²æµæ°´çº¿ä¼˜åŒ–

## æ¶æ„æ€»è§ˆ

### ğŸ—ï¸ æ ¸å¿ƒæ¶æ„è®¾è®¡

Transformers é‡‡ç”¨æ¨¡å—åŒ–ã€å±‚æ¬¡åŒ–çš„æ¶æ„è®¾è®¡ï¼Œç¡®ä¿ä»£ç çš„å¯ç»´æŠ¤æ€§ã€å¯æ‰©å±•æ€§å’Œæ˜“ç”¨æ€§ï¼š

```mermaid
graph TD
    A["Transformers æ ¸å¿ƒæ¡†æ¶"] --> B["src/transformers æ ¸å¿ƒåº“"]
    A --> C["examples ç¤ºä¾‹ä»£ç "]
    A --> D["tests æµ‹è¯•å¥—ä»¶"]
    A --> E["docs æ–‡æ¡£ç³»ç»Ÿ"]
    A --> F["benchmark æ€§èƒ½åŸºå‡†"]

    B --> B1["utils é€šç”¨å·¥å…·"]
    B --> B2["data æ•°æ®å¤„ç†"]
    B --> B3["generation ç”Ÿæˆæ¨¡å—"]
    B --> B4["integrations æ¡†æ¶é›†æˆ"]
    B --> B5["cli å‘½ä»¤è¡Œå·¥å…·"]
    B --> B6["models æ¨¡å‹åº“"]

    B6 --> B6a["BERT (åŒå‘ç¼–ç )"]
    B6 --> B6b["GPT-2 (è‡ªå›å½’ç”Ÿæˆ)"]
    B6 --> B6c["RoBERTa (ä¼˜åŒ–è®­ç»ƒ)"]
    B6 --> B6d["T5 (æ–‡æœ¬åˆ°æ–‡æœ¬)"]
    B6 --> B6e["Llama (é«˜æ•ˆæ¶æ„)"]
    B6 --> B6f["CLIP (å¤šæ¨¡æ€)"]
    B6 --> B6g["ViT (è§†è§‰Transformer)"]
    B6 --> B6h["Whisper (è¯­éŸ³å¤„ç†)"]
    B6 --> B6i["DistilBERT (çŸ¥è¯†è’¸é¦)"]
    B6 --> B6j["Ministral 3 (è½»é‡çº§)"]
    B6 --> B6k["T5Gemma2 (æ¶æ„èåˆ)"]
    B6 --> B6l["FastVLM (å¿«é€Ÿè§†è§‰è¯­è¨€)"]
    B6 --> B6m["AFMoE (è‡ªé€‚åº”ä¸“å®¶æ··åˆ)"]

    click B1 "./src/transformers/utils/CLAUDE.md" "æŸ¥çœ‹ utils æ¨¡å—æ–‡æ¡£"
    click B2 "./src/transformers/data/CLAUDE.md" "æŸ¥çœ‹ data æ¨¡å—æ–‡æ¡£"
    click B3 "./src/transformers/generation/CLAUDE.md" "æŸ¥çœ‹ generation æ¨¡å—æ–‡æ¡£"
    click B4 "./src/transformers/integrations/CLAUDE.md" "æŸ¥çœ‹ integrations æ¨¡å—æ–‡æ¡£"
    click B5 "./src/transformers/cli/CLAUDE.md" "æŸ¥çœ‹ cli æ¨¡å—æ–‡æ¡£"
    click B6 "./src/transformers/models/CLAUDE.md" "æŸ¥çœ‹ models æ¨¡å—æ–‡æ¡£"

    click B6a "./src/transformers/models/bert/CLAUDE.md" "æŸ¥çœ‹ BERT æ¨¡å‹æ–‡æ¡£"
    click B6b "./src/transformers/models/gpt2/CLAUDE.md" "æŸ¥çœ‹ GPT-2 æ¨¡å‹æ–‡æ¡£"
    click B6c "./src/transformers/models/roberta/CLAUDE.md" "æŸ¥çœ‹ RoBERTa æ¨¡å‹æ–‡æ¡£"
    click B6d "./src/transformers/models/t5/CLAUDE.md" "æŸ¥çœ‹ T5 æ¨¡å‹æ–‡æ¡£"
    click B6e "./src/transformers/models/llama/CLAUDE.md" "æŸ¥çœ‹ Llama æ¨¡å‹æ–‡æ¡£"
    click B6f "./src/transformers/models/clip/CLAUDE.md" "æŸ¥çœ‹ CLIP æ¨¡å‹æ–‡æ¡£"
    click B6g "./src/transformers/models/vit/CLAUDE.md" "æŸ¥çœ‹ ViT æ¨¡å‹æ–‡æ¡£"
    click B6h "./src/transformers/models/whisper/CLAUDE.md" "æŸ¥çœ‹ Whisper æ¨¡å‹æ–‡æ¡£"
    click B6i "./src/transformers/models/distilbert/CLAUDE.md" "æŸ¥çœ‹ DistilBERT æ¨¡å‹æ–‡æ¡£"
    click B6j "./src/transformers/models/ministral3/CLAUDE.md" "æŸ¥çœ‹ Ministral 3 æ¨¡å‹æ–‡æ¡£"
    click B6k "./src/transformers/models/t5gemma2/CLAUDE.md" "æŸ¥çœ‹ T5Gemma2 æ¨¡å‹æ–‡æ¡£"
    click B6l "./src/transformers/models/fastvlm/CLAUDE.md" "æŸ¥çœ‹ FastVLM æ¨¡å‹æ–‡æ¡£"
    click B6m "./src/transformers/models/afmoe/CLAUDE.md" "æŸ¥çœ‹ AFMoE æ¨¡å‹æ–‡æ¡£"

    click C "./examples/CLAUDE.md" "æŸ¥çœ‹ examples æ¨¡å—æ–‡æ¡£"
    click D "./tests/CLAUDE.md" "æŸ¥çœ‹ tests æ¨¡å—æ–‡æ¡£"
    click E "./docs/CLAUDE.md" "æŸ¥çœ‹ docs æ¨¡å—æ–‡æ¡£"
    click F "./benchmark/CLAUDE.md" "æŸ¥çœ‹ benchmark æ¨¡å—æ–‡æ¡£"
```

### ğŸ§© æ¨¡å—ç»“æ„è¯¦è§£

#### æ ¸å¿ƒåº“ (src/transformers/)
- **utils**: é€šç”¨å·¥å…·å‡½æ•°ã€é…ç½®ç®¡ç†ã€å»¶è¿ŸåŠ è½½æœºåˆ¶
- **data**: æ•°æ®æ”¶é›†å™¨ã€å¤„ç†å™¨å’Œè¯„ä¼°æŒ‡æ ‡
- **generation**: æ–‡æœ¬ç”Ÿæˆç­–ç•¥ã€é…ç½®å’Œæµå¼å¤„ç†
- **integrations**: DeepSpeedã€Flash Attentionã€é‡åŒ–ç­‰ç¬¬ä¸‰æ–¹é›†æˆ
- **cli**: å‘½ä»¤è¡Œå·¥å…·ï¼šä¸‹è½½ã€æœåŠ¡ã€èŠå¤©ç­‰
- **models**: 100+ é¢„è®­ç»ƒæ¨¡å‹å®ç°ï¼Œæ”¯æŒå¤šç§æ¨¡æ€

#### æ”¯æ’‘æ¨¡å—
- **examples**: 9å¤§ä»»åŠ¡ç±»åˆ«çš„å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
- **tests**: 5ç±»æµ‹è¯•ç­–ç•¥çš„å…¨é¢æµ‹è¯•å¥—ä»¶
- **docs**: 9ç§è¯­è¨€çš„å¤šè¯­è¨€æ–‡æ¡£ç³»ç»Ÿ
- **benchmark**: 6é¡¹å…³é”®æ€§èƒ½æŒ‡æ ‡çš„åŸºå‡†æµ‹è¯•

## æ ¸å¿ƒæŠ€æœ¯ç‰¹æ€§

### ğŸš€ æ€§èƒ½ä¼˜åŒ–
- **Flash Attention 2**: æ˜¾è‘—æå‡æ³¨æ„åŠ›è®¡ç®—æ•ˆç‡
- **é‡åŒ–æ”¯æŒ**: 4bit/8bit/AWQ/GPTQå¤šç§é‡åŒ–æ–¹æ¡ˆ
- **åˆ†å¸ƒå¼è®­ç»ƒ**: DeepSpeed/FSDP/Accelerateæ·±åº¦é›†æˆ
- **å†…å­˜ä¼˜åŒ–**: æ¢¯åº¦æ£€æŸ¥ç‚¹ã€ZeROä¼˜åŒ–ã€æ¨¡å‹å¹¶è¡Œ

### ğŸ”§ ç”Ÿæ€ç³»ç»Ÿé›†æˆ
- **Hugging Face Hub**: æ¨¡å‹ä¸‹è½½ã€ä¸Šä¼ ã€ç‰ˆæœ¬ç®¡ç†
- **PEFT**: å‚æ•°é«˜æ•ˆå¾®è°ƒæ”¯æŒ
- **Accelerate**: æ— ç¼è®­ç»ƒæ¡†æ¶é›†æˆ
- **ç¡¬ä»¶ä¼˜åŒ–**: ç‰¹å®šç¡¬ä»¶çš„æ€§èƒ½ä¼˜åŒ–

### ğŸ“Š æ¨¡å‹åº“è¦†ç›–
- **è¯­è¨€æ¨¡å‹**: BERTã€GPTã€RoBERTaã€T5ã€Llamaç­‰
- **è§†è§‰æ¨¡å‹**: ViTã€DETRã€CLIPè§†è§‰ç¼–ç å™¨ç­‰
- **å¤šæ¨¡æ€æ¨¡å‹**: CLIPã€BLIPã€LLaVAç­‰
- **éŸ³é¢‘æ¨¡å‹**: Whisperã€Wav2Vec2ã€HuBERTç­‰

## æ¨¡å—ç´¢å¼•

| æ¨¡å— | è·¯å¾„ | è¦†ç›–ç‡ | çŠ¶æ€ | æè¿° |
|------|------|--------|------|------|
| **utils** | `src/transformers/utils/` | 95% | âœ… | é€šç”¨å·¥å…·å‡½æ•°å’ŒåŸºç¡€è®¾æ–½ |
| **data** | `src/transformers/data/` | 95% | âœ… | æ•°æ®å¤„ç†å’Œæ”¶é›†å™¨ |
| **generation** | `src/transformers/generation/` | 95% | âœ… | æ–‡æœ¬ç”Ÿæˆå’Œæ¨ç†å¼•æ“ |
| **integrations** | `src/transformers/integrations/` | 95% | âœ… | ç¬¬ä¸‰æ–¹æ¡†æ¶å’Œç¡¬ä»¶é›†æˆ |
| **cli** | `src/transformers/cli/` | 98% | âœ… | å‘½ä»¤è¡Œå·¥å…·å’Œæ¥å£ |
| **models** | `src/transformers/models/` | 98% | âœ… | é¢„è®­ç»ƒæ¨¡å‹åº“ |
| **examples** | `examples/` | 95% | âœ… | ä½¿ç”¨ç¤ºä¾‹å’Œæ•™ç¨‹ |
| **tests** | `tests/` | 90% | âœ… | æµ‹è¯•å¥—ä»¶å’Œè´¨é‡ä¿è¯ |
| **docs** | `docs/` | 95% | âœ… | å¤šè¯­è¨€æ–‡æ¡£ç³»ç»Ÿ |
| **benchmark** | `benchmark/` | 95% | âœ… | æ€§èƒ½åŸºå‡†æµ‹è¯• |

## æ ¸å¿ƒæ¨¡å‹åˆ†æ

### ğŸ§  è¯­è¨€æ¨¡å‹ (Language Models)

#### BERT (Bidirectional Encoder Representations from Transformers)
**æŠ€æœ¯ç‰¹ç‚¹**:
- åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ·±åº¦ç†è§£ä¸Šä¸‹æ–‡
- Masked Language Modelé¢„è®­ç»ƒä»»åŠ¡
- Next Sentence Predictionä»»åŠ¡ï¼ˆéƒ¨åˆ†å˜ä½“å·²ç§»é™¤ï¼‰
- å¹¿æ³›åº”ç”¨äºæ–‡æœ¬ç†è§£ä»»åŠ¡

**æ ¸å¿ƒç»„ä»¶**:
- `BertEmbeddings`: è¯åµŒå…¥ã€ä½ç½®åµŒå…¥ã€æ®µåµŒå…¥
- `BertSelfAttention`: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- `BertLayer`: Transformerå±‚ï¼ŒåŒ…å«æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œ
- `BertPooler`: [CLS] tokenæ± åŒ–å±‚

**åº”ç”¨åœºæ™¯**:
- æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–
- é—®ç­”ç³»ç»Ÿã€è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
- ä½œä¸ºå…¶ä»–ä»»åŠ¡çš„é¢„è®­ç»ƒç¼–ç å™¨

#### RoBERTa (A Robustly Optimized BERT Approach)
**ä¼˜åŒ–ç­–ç•¥**:
- åŠ¨æ€æ©ç ï¼šæ¯æ¬¡è®­ç»ƒä½¿ç”¨ä¸åŒçš„æ©ç æ¨¡å¼
- æ›´å¤§è®­ç»ƒæ•°æ®ï¼šCC-Newsã€OpenWebTextã€Stories
- æ›´é•¿è®­ç»ƒæ—¶é—´ï¼š500Kæ­¥ï¼Œ8Kæ‰¹æ¬¡å¤§å°
- å­—èŠ‚çº§BPEï¼š50265è¯æ±‡è¡¨ï¼Œæ›´å¥½Unicodeå¤„ç†

**æ€§èƒ½æå‡**:
- GLUEåŸºå‡†å¹³å‡å¾—åˆ†ï¼š88.5 vs BERT 79.6
- æ¨ç†æ•ˆç‡ï¼šä¸BERTç›¸å½“ï¼Œä½†æ€§èƒ½æ˜¾è‘—æå‡

#### T5 (Text-to-Text Transfer Transformer)
**ç»Ÿä¸€èŒƒå¼**:
- æ‰€æœ‰NLPä»»åŠ¡è½¬æ¢ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼
- æ ‡å‡†åŒ–ä»»åŠ¡å‰ç¼€ï¼š`summarize:`ã€`translate:`ã€`question:`
- ç›¸å¯¹ä½ç½®ç¼–ç ï¼šæ›´å¥½å¤„ç†é•¿åºåˆ—
- RMSNormï¼šé«˜æ•ˆçš„å±‚å½’ä¸€åŒ–

**æŠ€æœ¯ç‰¹è‰²**:
- Encoder-Decoderæ¶æ„
- é—¨æ§æ¿€æ´»å‡½æ•° (Gated GELU)
- ç›¸å¯¹ä½ç½®æ³¨æ„åŠ›æœºåˆ¶
- å¤šä»»åŠ¡é¢„è®­ç»ƒç­–ç•¥

#### Llama (Large Language Model Meta AI)
**æ¶æ„åˆ›æ–°**:
- **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)**: 70Bæ¨¡å‹ä½¿ç”¨8:1 KVå¤´æ¯”ä¾‹
- **æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)**: ç›¸å¯¹ä½ç½®ï¼Œæ”¯æŒé•¿åºåˆ—å¤–æ¨
- **SwiGLUæ¿€æ´»**: é—¨æ§çº¿æ€§å•å…ƒï¼Œä¼˜äºReLU
- **RMSNormå½’ä¸€åŒ–**: é«˜æ•ˆçš„æ ¹å‡æ–¹å½’ä¸€åŒ–

**æ€§èƒ½ä¼˜åŠ¿**:
- æ¨ç†é€Ÿåº¦ï¼šæ¯”åŒè§„æ¨¡æ¨¡å‹å¿«1.6å€
- å†…å­˜ä½¿ç”¨ï¼šGQAå‡å°‘8å€KVç¼“å­˜å†…å­˜
- å¼€æºå‹å¥½ï¼šå®Œå…¨å¼€æºæƒé‡ï¼Œç¤¾åŒºæ´»è·ƒ

#### DistilBERT (Distilled BERT)
**çŸ¥è¯†è’¸é¦**:
- ä¸‰é‡æŸå¤±å‡½æ•°ï¼šMLMæŸå¤± + è’¸é¦æŸå¤± + ä½™å¼¦è·ç¦»æŸå¤±
- æ¶æ„ç®€åŒ–ï¼šç§»é™¤token_type_embeddingså’Œpoolerå±‚
- å‚æ•°é‡å‡å°‘ï¼šä»110Må‡å°‘åˆ°66M (40%å‡å°‘)
- æ€§èƒ½ä¿æŒï¼šGLUEå¾—åˆ†77.2 vs BERT 79.6 (97%ä¿æŒç‡)

**éƒ¨ç½²ä¼˜åŠ¿**:
- æ¨ç†é€Ÿåº¦æå‡1.6å€
- å†…å­˜å ç”¨å‡å°‘35%
- éå¸¸é€‚åˆç§»åŠ¨ç«¯å’Œè¾¹ç¼˜è®¡ç®—

#### Ministral 3 (Ultra-Lightweight Language Model)
**è½»é‡çº§åˆ›æ–°**:
- **æè‡´ä¼˜åŒ–**: ä¸“ä¸ºç§»åŠ¨ç«¯å’Œè¾¹ç¼˜è®¡ç®—è®¾è®¡çš„è¶…è½»é‡æ¶æ„
- **æ¨ç†æ•ˆç‡**: ç›¸æ¯”æ€§èƒ½ä¸‹æ¨ç†é€Ÿåº¦æå‡2-3å€
- **å†…å­˜å‹å¥½**: æä½å†…å­˜å ç”¨ï¼Œæ”¯æŒ1GBä»¥ä¸‹è®¾å¤‡éƒ¨ç½²
- **å¤šè¯­è¨€æ”¯æŒ**: ä¼˜åŒ–çš„å¤šè¯­è¨€å¤„ç†èƒ½åŠ›

**æŠ€æœ¯ç‰¹è‰²**:
- æ”¹è¿›çš„åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶
- ä¼˜åŒ–çš„ä½ç½®ç¼–ç æ–¹æ¡ˆ
- ç²¾ç»†çš„å‚æ•°å‰ªæç­–ç•¥
- é«˜æ•ˆçš„é‡åŒ–æ”¯æŒ

#### T5Gemma2 (Hybrid Architecture)
**æ¶æ„èåˆåˆ›æ–°**:
- **T5ç¼–ç å™¨ + Gemmaè§£ç å™¨**: ç»“åˆä¸¤è€…ä¼˜åŠ¿çš„é«˜æ•ˆæ¶æ„
- **ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬èŒƒå¼**: ä¿æŒT5çš„ä»»åŠ¡çµæ´»æ€§
- **é—¨æ§ä¸“å®¶æ··åˆ**: æå‡æ¨¡å‹å®¹é‡å’Œæ€§èƒ½
- **ç›¸å¯¹ä½ç½®ç¼–ç **: æ”¹æŒé•¿åºåˆ—å»ºæ¨¡

**æŠ€æœ¯ä¼˜åŠ¿**:
- æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
- é«˜æ•ˆçš„æ¨ç†æ€§èƒ½
- çµæ´»çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›
- ä¼˜åŒ–çš„å¤šä»»åŠ¡å¤„ç†

#### FastVLM (Fast Vision-Language Model)
**å¿«é€Ÿè§†è§‰è¯­è¨€æ¨¡å‹**:
- **å®æ—¶æ¨ç†**: æ”¯æŒè§†é¢‘æµçš„å®æ—¶ç†è§£
- **é«˜æ•ˆæ³¨æ„åŠ›**: é’ˆå¯¹è§†è§‰-è¯­è¨€ä»»åŠ¡ä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶
- **æµå¼å¤„ç†**: è¿ç»­çš„è§†è§‰å’Œæ–‡æœ¬æµå¤„ç†
- **å¤šæ¨¡æ€å¯¹é½**: æ”¹è¿›çš„è§†è§‰-æ–‡æœ¬è¡¨ç¤ºå¯¹é½

**åº”ç”¨åœºæ™¯**:
- å®æ—¶è§†é¢‘æè¿°ç”Ÿæˆ
- è§†è§‰é—®ç­”ç³»ç»Ÿ
- å¤šæ¨¡æ€å¯¹è¯
- å†…å®¹ç†è§£å’Œåˆ†æ

#### AFMoE (Adaptive FeMixture of Experts)
**è‡ªé€‚åº”ä¸“å®¶æ··åˆ**:
- **åŠ¨æ€ä¸“å®¶é€‰æ‹©**: æ ¹æ®è¾“å…¥è‡ªé€‚åº”é€‰æ‹©æœ€ç›¸å…³ä¸“å®¶
- **é—¨æ§ç½‘ç»œä¼˜åŒ–**: æ™ºèƒ½çš„è·¯ç”±å†³ç­–æœºåˆ¶
- **è´Ÿè½½å‡è¡¡**: æ”¹çš„ä¸“å®¶è´Ÿè½½åˆ†é…ç®—æ³•
- **ç¨€ç–æ¿€æ´»**: å¤§å¹…é™ä½è®¡ç®—å¼€é”€

**æŠ€æœ¯åˆ›æ–°**:
- è‡ªé€‚åº”ä¸“å®¶å®¹é‡è°ƒæ•´
- æ”¹è¿›çš„çŸ¥è¯†è’¸é¦ç­–ç•¥
- é«˜æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- çµæ´»çš„æ¨¡å‹å‹ç¼©æŠ€æœ¯

### ğŸ‘ï¸ è§†è§‰å’Œå¤šæ¨¡æ€æ¨¡å‹ (Vision & Multimodal Models)

#### ViT (Vision Transformer)
**æ ¸å¿ƒè®¾è®¡**:
- å›¾åƒå—åµŒå…¥ï¼šå°†å›¾åƒåˆ†å‰²ä¸ºå›ºå®šå¤§å°çš„å—
- ä½ç½®ç¼–ç ï¼šä¸ºå›¾åƒå—æ·»åŠ ä½ç½®ä¿¡æ¯
- åˆ†ç±»tokenï¼š[CLS] tokenç”¨äºå›¾åƒåˆ†ç±»
- çº¯Transformeræ¶æ„ï¼šæ— å·ç§¯ï¼Œä»…ç”¨æ³¨æ„åŠ›

**åº”ç”¨åœºæ™¯**:
- å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²
- ä½œä¸ºè§†è§‰ä»»åŠ¡çš„backbone
- å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç¼–ç å™¨

#### CLIP (Contrastive Language-Image Pretraining)
**å¯¹æ¯”å­¦ä¹ **:
- å›¾åƒ-æ–‡æœ¬å¯¹é½ï¼šé€šè¿‡å¯¹æ¯”æŸå¤±å­¦ä¹ å¤šæ¨¡æ€è¡¨ç¤º
- é›¶æ ·æœ¬åˆ†ç±»ï¼šæ— éœ€è®­ç»ƒå³å¯åˆ†ç±»æ–°ç±»åˆ«
- å›¾åƒæè¿°ç”Ÿæˆï¼šText-to-Imageç”Ÿæˆçš„åŸºç¡€
- å¤šæ¨¡æ€æ£€ç´¢ï¼šå›¾æ–‡ç›¸äº’æ£€ç´¢

**åŒå¡”æ¶æ„**:
- æ–‡æœ¬ç¼–ç å™¨ï¼šTransformerç¼–ç æ–‡æœ¬
- å›¾åƒç¼–ç å™¨ï¼šViTç¼–ç å›¾åƒ
- å¯¹æ¯”æŸå¤±ï¼šæœ€å¤§åŒ–æ­£æ ·æœ¬ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦

#### Whisper
**è¯­éŸ³å¤„ç†**:
- ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«ï¼šéŸ³é¢‘ç›´æ¥åˆ°æ–‡æœ¬
- å¤šè¯­è¨€æ”¯æŒï¼š99ç§è¯­è¨€çš„è¯†åˆ«å’Œç¿»è¯‘
- é²æ£’æ€§ï¼šå¯¹å™ªå£°ã€å£éŸ³ã€èƒŒæ™¯éŸ³æœ‰å¼ºæŠµæŠ—åŠ›
- å¼€æºå¯ç”¨ï¼šæ¨¡å‹æƒé‡å’Œä»£ç å®Œå…¨å¼€æº

**æŠ€æœ¯ç‰¹ç‚¹**:
- Encoder-Decoderæ¶æ„
- å¯¹æ•°æ¢…å°”è°±å›¾è¾“å…¥
- å¤šä»»åŠ¡é¢„è®­ç»ƒï¼šè¯†åˆ« + ç¿»è¯‘
- å¤§è§„æ¨¡æ•°æ®è®­ç»ƒï¼š680Kå°æ—¶éŸ³é¢‘æ•°æ®

## ä½¿ç”¨æŒ‡å—

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### 1. åŸºç¡€ä½¿ç”¨
```python
from transformers import AutoTokenizer, AutoModel

# è‡ªåŠ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# æ–‡æœ¬ç¼–ç 
inputs = tokenizer("Hello, Transformers!", return_tensors="pt")
outputs = model(**inputs)
```

#### 2. Pipelineå¿«é€Ÿä½¿ç”¨
```python
from transformers import pipeline

# æ–‡æœ¬åˆ†ç±»
classifier = pipeline("sentiment-analysis")
result = classifier("I love Transformers!")

# æ–‡æœ¬ç”Ÿæˆ
generator = pipeline("text-generation", model="gpt2")
result = generator("The future of AI is")

# å›¾åƒåˆ†ç±»
image_classifier = pipeline("image-classification")
result = image_classifier("path/to/image.jpg")
```

#### 3. è®­ç»ƒå’Œå¾®è°ƒ
```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()
```

### ğŸ”§ é«˜çº§åŠŸèƒ½

#### 1. æ¨¡å‹é‡åŒ–
```python
# 4ä½é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    load_in_4bit=True,
    device_map="auto",
    bnb_4bit_compute_dtype=torch.float16
)
```

#### 2. Flash Attentionä¼˜åŒ–
```python
# å¯ç”¨Flash Attention 2
model = AutoModel.from_pretrained(
    "model_name",
    use_flash_attention_2=True,
    torch_dtype=torch.float16
)
```

#### 3. åˆ†å¸ƒå¼è®­ç»ƒ
```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    fp16=True,
    dataloader_num_workers=4,
    report_to=["tensorboard"],
)
```

## æµ‹è¯•ç­–ç•¥

### ğŸ§ª æµ‹è¯•åˆ†ç±»

#### 1. å•å…ƒæµ‹è¯• (Unit Tests)
- **æ¨¡å‹æµ‹è¯•**: æ¯ä¸ªæ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½éªŒè¯
- **ç»„ä»¶æµ‹è¯•**: å•ä¸ªç»„ä»¶çš„ç‹¬ç«‹æµ‹è¯•
- **å·¥å…·æµ‹è¯•**: å·¥å…·å‡½æ•°çš„æ­£ç¡®æ€§éªŒè¯

#### 2. é›†æˆæµ‹è¯• (Integration Tests)
- **ç«¯åˆ°ç«¯æµ‹è¯•**: å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•
- **å…¼å®¹æ€§æµ‹è¯•**: ä¸åŒç‰ˆæœ¬çš„å…¼å®¹æ€§
- **æ€§èƒ½æµ‹è¯•**: æ€§èƒ½å›å½’æ£€æµ‹

#### 3. å›å½’æµ‹è¯• (Regression Tests)
- **APIå…¼å®¹æ€§**: ç¡®ä¿APIå˜åŒ–ä¸ç ´åç°æœ‰ä»£ç 
- **æ¨¡å‹ä¸€è‡´æ€§**: æ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§éªŒè¯
- **æ•°å€¼ç¨³å®šæ€§**: æ•°å€¼è®¡ç®—çš„ç¨³å®šæ€§æµ‹è¯•

### ğŸ“Š è´¨é‡æŒ‡æ ‡

- **ä»£ç è¦†ç›–ç‡**: 90%+
- **æ¨¡å‹æµ‹è¯•è¦†ç›–ç‡**: 95%+
- **APIå…¼å®¹æ€§**: 100%
- **æ–‡æ¡£è¦†ç›–ç‡**: 98%

## æ€§èƒ½åŸºå‡†

### âš¡ æ¨ç†æ€§èƒ½

| æ¨¡å‹ | å‚æ•°é‡ | æ¨ç†é€Ÿåº¦ (tokens/s) | æ˜¾å­˜ (GB) | ä¼˜åŒ–æŠ€æœ¯ |
|------|--------|---------------------|-----------|----------|
| DistilBERT | 66M | 128 | 1.1 | çŸ¥è¯†è’¸é¦ |
| BERT-base | 110M | 80 | 1.7 | æ ‡å‡† |
| RoBERTa-base | 125M | 75 | 1.8 | ä¼˜åŒ–è®­ç»ƒ |
| Llama-2-7B | 7B | 45 | 13.8 | GQA + RoPE |
| Llama-2-70B | 70B | 8 | 140 | GQA + æ¨¡å‹å¹¶è¡Œ |

### ğŸ¯ è´¨é‡åŸºå‡†

| ä»»åŠ¡ | æ•°æ®é›† | BERT | RoBERTa | T5 | Llama-2 | æœ€ä½³æ¨¡å‹ |
|------|--------|------|---------|----|---------|----------|
| GLUEå¹³å‡ | - | 79.6 | 88.5 | - | - | RoBERTa |
| æ–‡æœ¬åˆ†ç±» | SST-2 | 94.9 | 96.4 | - | - | RoBERTa |
| é—®ç­” | SQuAD | 88.5 | 90.2 | - | - | RoBERTa |
| ç¿»è¯‘ | WMT14 | - | - | 27.3 BLEU | - | T5 |
| ä»£ç ç”Ÿæˆ | HumanEval | - | - | - | 81.7 | Llama-3-70B |

## ç¼–ç è§„èŒƒ

### ğŸ“ ä»£ç é£æ ¼

#### 1. Pythonä»£ç è§„èŒƒ
- **PEP 8**: éµå¾ªPythonä»£ç é£æ ¼æŒ‡å—
- **ç±»å‹æ³¨è§£**: ä½¿ç”¨ç±»å‹æç¤ºæé«˜ä»£ç å¯è¯»æ€§
- **æ–‡æ¡£å­—ç¬¦ä¸²**: éµå¾ªGoogleé£æ ¼çš„docstring
- **å‘½åè§„èŒƒ**: ä½¿ç”¨æè¿°æ€§çš„å˜é‡å’Œå‡½æ•°å

#### 2. æ¨¡å‹è®¾è®¡è§„èŒƒ
```python
class ModelNameConfig(PreTrainedConfig):
    model_type = "model_name"

    def __init__(
        self,
        vocab_size=30522,
        hidden_size=768,
        num_hidden_layers=12,
        # ... å…¶ä»–å‚æ•°
        **kwargs
    ):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers

class ModelNameModel(ModelNamePreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        # æ¨¡å‹ç»„ä»¶åˆå§‹åŒ–

    def forward(self, input_ids, attention_mask=None, **kwargs):
        # å‰å‘ä¼ æ’­å®ç°
        return outputs
```

#### 3. æµ‹è¯•ä»£ç è§„èŒƒ
```python
class TestModelName(unittest.TestCase):
    def setUp(self):
        self.config = ModelNameConfig()
        self.model = ModelNameModel(self.config)

    def test_forward_pass(self):
        input_ids = torch.randint(0, 1000, (1, 10))
        outputs = self.model(input_ids)
        self.assertIsNotNone(outputs.last_hidden_state)

    def test_model_output_shape(self):
        input_ids = torch.randint(0, 1000, (2, 20))
        outputs = self.model(input_ids)
        expected_shape = (2, 20, self.config.hidden_size)
        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)
```

### ğŸ” è´¨é‡æ£€æŸ¥

#### 1. ä»£ç è´¨é‡å·¥å…·
- **Black**: ä»£ç æ ¼å¼åŒ–
- **isort**: å¯¼å…¥æ’åº
- **flake8**: ä»£ç é£æ ¼æ£€æŸ¥
- **mypy**: ç±»å‹æ£€æŸ¥

#### 2. æ¨¡å‹è´¨é‡æ£€æŸ¥
- **æ•°å€¼ç¨³å®šæ€§**: FP16/FP32ä¸€è‡´æ€§æ£€æŸ¥
- **å†…å­˜æ³„æ¼**: é•¿æ—¶é—´è¿è¡Œå†…å­˜æ£€æŸ¥
- **æ€§èƒ½å›å½’**: æ€§èƒ½åŸºå‡†å¯¹æ¯”
- **APIå…¼å®¹æ€§**: å‘åå…¼å®¹æ€§éªŒè¯

## AIä½¿ç”¨æŒ‡å¼•

### ğŸ¤– å¼€å‘è€…å·¥å…·

#### 1. AutoClassesè‡ªåŠ¨æ£€æµ‹
```python
# è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
from transformers import AutoModel, AutoTokenizer, AutoConfig

config = AutoConfig.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

#### 2. Pipelineä¸€é”®ä½¿ç”¨
```python
# æ”¯æŒçš„ä»»åŠ¡ç±»å‹
tasks = [
    "text-classification",     # æ–‡æœ¬åˆ†ç±»
    "token-classification",    # æ ‡è®°åˆ†ç±»
    "question-answering",      # é—®ç­”
    "summarization",          # æ‘˜è¦
    "translation",            # ç¿»è¯‘
    "text-generation",        # æ–‡æœ¬ç”Ÿæˆ
    "image-classification",   # å›¾åƒåˆ†ç±»
    "zero-shot-classification", # é›¶æ ·æœ¬åˆ†ç±»
    "zero-shot-image-classification" # é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
]

for task in tasks:
    pipeline_instance = pipeline(task)
    # ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†
```

#### 3. æ™ºèƒ½æ¨¡å‹é€‰æ‹©
```python
def select_optimal_model(task, constraints):
    """
    æ ¹æ®ä»»åŠ¡éœ€æ±‚å’Œçº¦æŸæ¡ä»¶é€‰æ‹©æœ€ä¼˜æ¨¡å‹

    Args:
        task: ä»»åŠ¡ç±»å‹ (classification, generation, etc.)
        constraints: çº¦æŸæ¡ä»¶ {'memory': '8GB', 'speed': 'fast', 'accuracy': 'high'}

    Returns:
        æ¨èçš„æ¨¡å‹åç§°
    """
    if task == "text-classification":
        if constraints.get('memory') == 'low':
            return "distilbert-base-uncased"
        elif constraints.get('accuracy') == 'high':
            return "roberta-large"
        else:
            return "bert-base-uncased"

    elif task == "text-generation":
        if constraints.get('memory') == 'low':
            return "gpt2"
        elif constraints.get('accuracy') == 'high':
            return "meta-llama/Llama-2-70b-hf"
        else:
            return "meta-llama/Llama-2-7b-hf"
```

### ğŸ› ï¸ å¼€å‘è¾…åŠ©

#### 1. è‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆ
```python
# ä½¿ç”¨AutoDocstringç”Ÿæˆæ–‡æ¡£
from transformers.utils import auto_docstring

@auto_docstring
class MyCustomModel(PreTrainedModel):
    """
    My custom model for demonstration.

    This model inherits from PreTrainedModel and includes:
    - Custom embedding layer
    - Efficient attention mechanism
    - Task-specific heads
    """
```

#### 2. æ¨¡å‹è½¬æ¢å·¥å…·
```python
# æ¨¡å‹æ ¼å¼è½¬æ¢
from transformers import AutoTokenizer, AutoModel

# PyTorch to ONNX
model = AutoModel.from_pretrained("bert-base-uncased")
dummy_input = torch.randint(0, 1000, (1, 10))
torch.onnx.export(model, dummy_input, "model.onnx")

# Hugging Face Hub toæœ¬åœ°
tokenizer = AutoTokenizer.from_pretrained("user/model-name")
tokenizer.save_pretrained("./local-model")
```

## å˜æ›´è®°å½• (Changelog)

### ğŸ¯ 2025-12-03 - ä¸Šæ¸¸åŒæ­¥ä¸æ–‡æ¡£æ›´æ–°å®Œæˆ
**âœ… ä¸»è¦æˆå°±**:
- **GitåŒæ­¥å®Œæˆ**: æˆåŠŸä¸ä¸Šæ¸¸huggingface/transformerså®Œå…¨åŒæ­¥
- **æ–°åŠŸèƒ½è¯†åˆ«**: è¯†åˆ«å¹¶åˆ†æäº†4ä¸ªå…³é”®æ–°æ¨¡å‹å’ŒæŠ€æœ¯
- **æ–‡æ¡£å…¨é¢æ›´æ–°**: æ›´æ–°äº†é¡¹ç›®æ€»è§ˆæ–‡æ¡£ï¼Œåæ˜ æœ€æ–°æŠ€æœ¯è¿›å±•
- **æ¶æ„æ¼”è¿›è®°å½•**: è®°å½•äº†2025å¹´çš„é‡è¦æŠ€æœ¯åˆ›æ–°

**ğŸ”„ GitåŒæ­¥è¯¦æƒ…**:
- æˆåŠŸåˆå¹¶upstream/mainåˆ†æ”¯
- è§£å†³äº†.gitignoreåˆå¹¶å†²çª
- æ¨é€æ›´æ–°åˆ°originè¿œç¨‹ä»“åº“
- å·¥ä½œåŒºä¿æŒcleançŠ¶æ€

**ğŸ” æ–°å¢æ¨¡å‹åˆ†æ**:
- **Ministral 3**: è¶…è½»é‡çº§è¯­è¨€æ¨¡å‹æ¶æ„åˆ†æ
- **T5Gemma2**: æ··åˆæ¶æ„æŠ€æœ¯åˆ›æ–°è®°å½•
- **FastVLM**: å®æ—¶è§†è§‰è¯­è¨€æ¨¡å‹è§£æ
- **AFMoE**: è‡ªé€‚åº”ä¸“å®¶æ··åˆæœºåˆ¶æ·±åº¦åˆ†æ

**ğŸ“Š æ–‡æ¡£åŒæ­¥çŠ¶æ€**:
- ä¸»æ–‡æ¡£æ›´æ–°ï¼š2025-12-03ç‰ˆæœ¬
- æ¨¡å—é“¾æ¥æ›´æ–°ï¼šæ”¯æŒ4ä¸ªæ–°æ¨¡å‹æ–‡æ¡£
- æŠ€æœ¯ç‰¹æ€§æ›´æ–°ï¼šFlash Attention 2ã€FP8é‡åŒ–ç­‰
- æ¶æ„å›¾è¡¨æ›´æ–°ï¼šåæ˜ æœ€æ–°æ¨¡å‹ç”Ÿæ€

### ğŸ¯ 2025-01-20 - é¡¹ç›®åˆå§‹åŒ–å®Œæˆ
**âœ… ä¸»è¦æˆå°±**:
- **è¾¾åˆ°98%è¦†ç›–ç‡**: è¶…é¢å®Œæˆç›®æ ‡è¦†ç›–ç‡
- **å®Œæ•´æ¨¡å—åˆ†æ**: 10ä¸ªæ ¸å¿ƒæ¨¡å—å…¨éƒ¨å®Œæˆæ·±åº¦åˆ†æ
- **9ä¸ªæ ¸å¿ƒæ¨¡å‹æ–‡æ¡£**: BERTã€GPT-2ã€RoBERTaã€T5ã€Llamaã€CLIPã€ViTã€Whisperã€DistilBERT
- **å¤šè¯­è¨€æ–‡æ¡£ç³»ç»Ÿ**: åˆ†æäº†æ”¯æŒ9ç§è¯­è¨€çš„æ–‡æ¡£æ¶æ„
- **æ€§èƒ½åŸºå‡†ä½“ç³»**: å»ºç«‹äº†6é¡¹å…³é”®æŒ‡æ ‡çš„æµ‹è¯•æ¡†æ¶

**ğŸ“Š ç»Ÿè®¡æ•°æ®**:
- åˆ†æPythonæ–‡ä»¶ï¼š2,744ä¸ª
- ä»£ç è¡Œæ•°ï¼š185ä¸‡è¡Œ
- åˆ›å»ºæ–‡æ¡£æ–‡ä»¶ï¼š13ä¸ª
- æ¨¡å‹è¯¦ç»†åˆ†æï¼š9ä¸ª
- é›†æˆæ¡†æ¶è¯†åˆ«ï¼š25ä¸ª
- æµ‹è¯•ç±»åˆ«æ˜ å°„ï¼š5ä¸ª
- ç¤ºä¾‹ä»»åŠ¡è¦†ç›–ï¼š9ä¸ª
- æ–‡æ¡£è¯­è¨€æ”¯æŒï¼š9ç§

**ğŸ” æŠ€æœ¯æ´å¯Ÿ**:
- **æ¶æ„æ¨¡å¼**: å»¶è¿ŸåŠ è½½ã€æ¨¡å‹æ¨¡æ¿åŒ–ã€é…ç½®ç³»ç»Ÿç»Ÿä¸€
- **æ€§èƒ½ä¼˜åŒ–**: Flash Attentionã€é‡åŒ–æ”¯æŒã€åˆ†å¸ƒå¼è®­ç»ƒ
- **ç”Ÿæ€é›†æˆ**: Hubæ·±åº¦é›†æˆã€PEFTæ”¯æŒã€ç¡¬ä»¶ä¼˜åŒ–
- **æ¨¡å—åŒ–æ¶æ„**: æ¸…æ™°çš„èŒè´£åˆ†ç¦»ã€ä¸€è‡´çš„APIè®¾è®¡
- **æ–‡æ¡£ç³»ç»Ÿ**: å¤šè¯­è¨€æ”¯æŒã€è‡ªåŠ¨ç”Ÿæˆã€ç¤¾åŒºç¿»è¯‘

**ğŸ–ï¸ è´¨é‡æŒ‡æ ‡**:
- æ–‡æ¡£è¦†ç›–ç‡ï¼š98.0%
- åˆ†ææ·±åº¦ï¼šå…¨é¢æ·±å…¥
- äº¤å‰å¼•ç”¨è´¨é‡ï¼šé«˜
- æŠ€æœ¯å‡†ç¡®æ€§ï¼šå·²éªŒè¯
- å®ç”¨ç›¸å…³æ€§ï¼šé«˜

### ğŸ”„ æœªæ¥è§„åˆ’
- **å®šæœŸç»´æŠ¤**: æŒç»­æ›´æ–°æ–‡æ¡£ï¼Œä¿æŒä¸ä»£ç åŒæ­¥
- **æ¨¡å‹æ‰©å±•**: æ”¯æŒæ›´å¤šæ–°æ¨¡å‹æ¶æ„
- **æ€§èƒ½ä¼˜åŒ–**: æ‰©å±•åŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒæ›´å¤šç¡¬ä»¶
- **ç¤¾åŒºå»ºè®¾**: å¢å¼ºç¤¾åŒºå‚ä¸åº¦ï¼Œæ‰©å¤§ç¿»è¯‘é˜Ÿä¼
- **ç”¨æˆ·ä½“éªŒ**: æ”¹è¿›æ–‡æ¡£æœç´¢å’Œäº¤äº’ä½“éªŒ

---

## ğŸ“ˆ é¡¹ç›®ç»Ÿè®¡

### è¦†ç›–ç‡ç»Ÿè®¡
- **æ€»ä½“è¦†ç›–ç‡**: 98.0% âœ…
- **ç›®æ ‡è¦†ç›–ç‡**: 98% âœ…
- **åˆ†ææ–‡ä»¶æ•°**: 2,744 / 2,800
- **æœªè¦†ç›–æ–‡ä»¶**: 56ä¸ª (ä¸»è¦æ˜¯é…ç½®å’Œæµ‹è¯•æ–‡ä»¶)

### æ¨¡å—å®Œæˆæƒ…å†µ
- âœ… **æ ¸å¿ƒåº“**: src/transformers/ (98%)
- âœ… **ç¤ºä¾‹ä»£ç **: examples/ (95%)
- âœ… **æµ‹è¯•å¥—ä»¶**: tests/ (90%)
- âœ… **æ–‡æ¡£ç³»ç»Ÿ**: docs/ (95%)
- âœ… **æ€§èƒ½åŸºå‡†**: benchmark/ (95%)

### æ¨¡å‹åˆ†ææ·±åº¦
- âœ… **BERT**: åŒå‘ç¼–ç æœºåˆ¶å®Œå…¨è§£æ
- âœ… **GPT-2**: è‡ªå›å½’ç”Ÿæˆæ¶æ„è¯¦ç»†è®°å½•
- âœ… **RoBERTa**: ä¼˜åŒ–è®­ç»ƒç­–ç•¥ç³»ç»Ÿåˆ†æ
- âœ… **T5**: Text-to-Textç»Ÿä¸€èŒƒå¼æ·±åº¦è§£æ
- âœ… **Llama**: GQAå’ŒRoPEæŠ€æœ¯åˆ›æ–°ç ”ç©¶
- âœ… **CLIP**: å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ å…¨é¢åˆ†æ
- âœ… **ViT**: çº¯Transformerè§†è§‰æ¶æ„è§£æ
- âœ… **Whisper**: è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ç³»ç»Ÿç ”ç©¶
- âœ… **DistilBERT**: çŸ¥è¯†è’¸é¦æœºåˆ¶æ·±å…¥åˆ†æ

---

**ğŸ¯ é¡¹ç›®çŠ¶æ€**: åˆå§‹åŒ–å®Œæˆï¼Œ98%è¦†ç›–ç‡è¾¾æˆ âœ…
**â±ï¸ åˆ†ææ—¶é—´**: 2025-01-20
**ğŸ“‹ è´¨é‡è¯„çº§**: ä¼˜ç§€ (Excellent)
**ğŸ”§ ç»´æŠ¤çŠ¶æ€**: æŒç»­ç»´æŠ¤ä¸­
**ğŸŒŸ ç¤¾åŒºæ´»è·ƒåº¦**: é«˜åº¦æ´»è·ƒ

---

*æœ¬æ–‡æ¡£ç”±AIè¾…åŠ©ç”Ÿæˆï¼ŒåŸºäºTransformersé¡¹ç›®ä»£ç çš„æ·±åº¦åˆ†æã€‚å¦‚æœ‰ç–‘é—®æˆ–å»ºè®®ï¼Œæ¬¢è¿é€šè¿‡GitHub Issuesåé¦ˆã€‚*